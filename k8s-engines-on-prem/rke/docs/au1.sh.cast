{"version": 2, "width": 191, "height": 48, "timestamp": 1607188026, "env": {"SHELL": "/run/current-system/sw/bin/bash", "TERM": "xterm-256color"}}
[0.50265, "o", "cd /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ && tee /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/au1.yml </home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/au1-v1.18.12-rancher1-1.yml \\\r\n            && ./files/rke up --config=/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/au1.yml\r\n"]
[0.591674, "o", "---\r\ncluster_name: au1\r\nkubernetes_version: v1.18.12-rancher1-1\r\n\r\nssh_agent_auth: true\r\n\r\nnodes:\r\n  - &m1\r\n    user: ubuntu\r\n    role: [etcd, controlplane]\r\n    \"address\": \"104.45.86.4\"\r\n    \"internal_address\": \"10.0.240.8\"\r\n  - <<: *m1\r\n    \"address\": \"23.102.48.17\"\r\n    \"internal_address\": \"10.0.240.6\"\r\n  - <<: *m1\r\n    \"address\": \"104.45.84.172\"\r\n    \"internal_address\": \"10.0.240.4\"\r\n\r\n  - &n1\r\n    user: ubuntu\r\n    role: [worker]\r\n    \"address\": \"23.102.22.29\"\r\n    \"internal_address\": \"10.0.240.7\"\r\n  - <<: *n1\r\n    \"address\": \"137.135.244.150\"\r\n    \"internal_address\": \"10.0.240.5\"\r\n  - <<: *n1\r\n    \"address\": \"23.102.23.168\"\r\n    \"internal_address\": \"10.0.240.9\"\r\n\r\nnetwork:\r\n  plugin: calico\r\n"]
[0.614268, "o", "\u001b[36mINFO\u001b[0m[0000] Running RKE version: v1.2.3                  \r\n"]
[0.616572, "o", "\u001b[36mINFO\u001b[0m[0000] Initiating Kubernetes cluster                \r\n"]
[0.638401, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [23.102.48.17] \r\n"]
[0.638536, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [104.45.84.172] \r\n"]
[0.638651, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [23.102.22.29] \r\n"]
[5.33816, "o", "\u001b[36mINFO\u001b[0m[0004] Checking if container [cluster-state-deployer] is running on host [23.102.23.168], try #1 \r\n"]
[5.510277, "o", "\u001b[36mINFO\u001b[0m[0004] Pulling image [rancher/rke-tools:v0.1.66] on host [23.102.23.168], try #1 \r\n"]
[23.99732, "o", "\u001b[36mINFO\u001b[0m[0023] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[27.67696, "o", "\u001b[36mINFO\u001b[0m[0027] Starting container [cluster-state-deployer] on host [23.102.23.168], try #1 \r\n"]
[29.317982, "o", "\u001b[36mINFO\u001b[0m[0028] [state] Successfully started [cluster-state-deployer] container on host [23.102.23.168] \r\n"]
[30.134921, "o", "\u001b[36mINFO\u001b[0m[0029] Checking if container [cluster-state-deployer] is running on host [104.45.86.4], try #1 \r\n"]
[30.336239, "o", "\u001b[36mINFO\u001b[0m[0029] Pulling image [rancher/rke-tools:v0.1.66] on host [104.45.86.4], try #1 \r\n"]
[46.170089, "o", "\u001b[36mINFO\u001b[0m[0045] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[50.410464, "o", "\u001b[36mINFO\u001b[0m[0049] Starting container [cluster-state-deployer] on host [104.45.86.4], try #1 \r\n"]
[51.68686, "o", "\u001b[36mINFO\u001b[0m[0051] [state] Successfully started [cluster-state-deployer] container on host [104.45.86.4] \r\n"]
[52.357052, "o", "\u001b[36mINFO\u001b[0m[0051] Checking if container [cluster-state-deployer] is running on host [23.102.48.17], try #1 \r\n"]
[52.553247, "o", "\u001b[36mINFO\u001b[0m[0051] Pulling image [rancher/rke-tools:v0.1.66] on host [23.102.48.17], try #1 \r\n"]
[68.698243, "o", "\u001b[36mINFO\u001b[0m[0068] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[71.403129, "o", "\u001b[36mINFO\u001b[0m[0070] Starting container [cluster-state-deployer] on host [23.102.48.17], try #1 \r\n"]
[72.734134, "o", "\u001b[36mINFO\u001b[0m[0072] [state] Successfully started [cluster-state-deployer] container on host [23.102.48.17] \r\n"]
[73.252988, "o", "\u001b[36mINFO\u001b[0m[0072] Checking if container [cluster-state-deployer] is running on host [104.45.84.172], try #1 \r\n"]
[75.797853, "o", "\u001b[36mINFO\u001b[0m[0075] Pulling image [rancher/rke-tools:v0.1.66] on host [104.45.84.172], try #1 \r\n"]
[90.748577, "o", "\u001b[36mINFO\u001b[0m[0090] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[92.701703, "o", "\u001b[36mINFO\u001b[0m[0092] Starting container [cluster-state-deployer] on host [104.45.84.172], try #1 \r\n"]
[94.237755, "o", "\u001b[36mINFO\u001b[0m[0093] [state] Successfully started [cluster-state-deployer] container on host [104.45.84.172] \r\n"]
[94.938009, "o", "\u001b[36mINFO\u001b[0m[0094] Checking if container [cluster-state-deployer] is running on host [23.102.22.29], try #1 \r\n"]
[97.508845, "o", "\u001b[36mINFO\u001b[0m[0096] Pulling image [rancher/rke-tools:v0.1.66] on host [23.102.22.29], try #1 \r\n"]
[113.913269, "o", "\u001b[36mINFO\u001b[0m[0113] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[116.255492, "o", "\u001b[36mINFO\u001b[0m[0115] Starting container [cluster-state-deployer] on host [23.102.22.29], try #1 \r\n"]
[117.562709, "o", "\u001b[36mINFO\u001b[0m[0116] [state] Successfully started [cluster-state-deployer] container on host [23.102.22.29] \r\n"]
[118.233214, "o", "\u001b[36mINFO\u001b[0m[0117] Checking if container [cluster-state-deployer] is running on host [137.135.244.150], try #1 \r\n"]
[120.651784, "o", "\u001b[36mINFO\u001b[0m[0120] Pulling image [rancher/rke-tools:v0.1.66] on host [137.135.244.150], try #1 \r\n"]
[132.846325, "o", "\u001b[36mINFO\u001b[0m[0132] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[148.818282, "o", "\u001b[36mINFO\u001b[0m[0148] Starting container [cluster-state-deployer] on host [137.135.244.150], try #1 \r\n"]
[149.842474, "o", "\u001b[36mINFO\u001b[0m[0149] [state] Successfully started [cluster-state-deployer] container on host [137.135.244.150] \r\n"]
[150.35446, "o", "\u001b[36mINFO\u001b[0m[0149] [certificates] Generating CA kubernetes certificates \r\n"]
[150.521106, "o", "\u001b[36mINFO\u001b[0m[0149] [certificates] Generating Kubernetes API server aggregation layer requestheader client CA certificates \r\n"]
[150.884537, "o", "\u001b[36mINFO\u001b[0m[0150] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates \r\n\u001b[36mINFO\u001b[0m[0150] [certificates] Generating Kubernetes API server certificates \r\n"]
[151.109914, "o", "\u001b[36mINFO\u001b[0m[0150] [certificates] Generating Service account token key \r\n\u001b[36mINFO\u001b[0m[0150] [certificates] Generating Kube Controller certificates \r\n"]
[151.291205, "o", "\u001b[36mINFO\u001b[0m[0150] [certificates] Generating Kube Scheduler certificates \r\n"]
[151.482652, "o", "\u001b[36mINFO\u001b[0m[0150] [certificates] Generating Kube Proxy certificates \r\n"]
[151.581034, "o", "\u001b[36mINFO\u001b[0m[0150] [certificates] Generating Node certificate   \r\n"]
[151.582831, "o", "\u001b[36mINFO\u001b[0m[0150] [certificates] Generating admin certificates and kubeconfig \r\n"]
[151.676518, "o", "\u001b[36mINFO\u001b[0m[0151] [certificates] Generating Kubernetes API server proxy client certificates \r\n"]
[151.808325, "o", "\u001b[36mINFO\u001b[0m[0151] [certificates] Generating kube-etcd-10-0-240-8 certificate and key \r\n"]
[151.89411, "o", "\u001b[36mINFO\u001b[0m[0151] [certificates] Generating kube-etcd-10-0-240-6 certificate and key \r\n"]
[152.274327, "o", "\u001b[36mINFO\u001b[0m[0151] [certificates] Generating kube-etcd-10-0-240-4 certificate and key \r\n"]
[152.551936, "o", "\u001b[36mINFO\u001b[0m[0151] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/au1.rkestate] \r\n"]
[152.553931, "o", "\u001b[36mINFO\u001b[0m[0151] Building Kubernetes cluster                  \r\n\u001b[36mINFO\u001b[0m[0151] [dialer] Setup tunnel for host [137.135.244.150] \r\n"]
[152.554172, "o", "\u001b[36mINFO\u001b[0m[0151] [dialer] Setup tunnel for host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0151] [dialer] Setup tunnel for host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0151] [dialer] Setup tunnel for host [104.45.86.4] \r\n"]
[152.554246, "o", "\u001b[36mINFO\u001b[0m[0151] [dialer] Setup tunnel for host [23.102.22.29] \r\n"]
[152.554298, "o", "\u001b[36mINFO\u001b[0m[0151] [dialer] Setup tunnel for host [104.45.84.172] \r\n"]
[155.108253, "o", "\u001b[36mINFO\u001b[0m[0154] [network] Deploying port listener containers \r\n"]
[155.243898, "o", "\u001b[36mINFO\u001b[0m[0154] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[155.309411, "o", "\u001b[36mINFO\u001b[0m[0154] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[155.310734, "o", "\u001b[36mINFO\u001b[0m[0154] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[156.088252, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [rke-etcd-port-listener] on host [104.45.86.4], try #1 \r\n"]
[156.146573, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [rke-etcd-port-listener] on host [23.102.48.17], try #1 \r\n"]
[156.280302, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [rke-etcd-port-listener] on host [104.45.84.172], try #1 \r\n"]
[157.419204, "o", "\u001b[36mINFO\u001b[0m[0156] [network] Successfully started [rke-etcd-port-listener] container on host [104.45.86.4] \r\n"]
[157.426495, "o", "\u001b[36mINFO\u001b[0m[0156] [network] Successfully started [rke-etcd-port-listener] container on host [23.102.48.17] \r\n"]
[157.577311, "o", "\u001b[36mINFO\u001b[0m[0156] [network] Successfully started [rke-etcd-port-listener] container on host [104.45.84.172] \r\n"]
[157.768026, "o", "\u001b[36mINFO\u001b[0m[0157] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[157.769509, "o", "\u001b[36mINFO\u001b[0m[0157] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[157.779649, "o", "\u001b[36mINFO\u001b[0m[0157] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[158.545818, "o", "\u001b[36mINFO\u001b[0m[0157] Starting container [rke-cp-port-listener] on host [23.102.48.17], try #1 \r\n"]
[158.622818, "o", "\u001b[36mINFO\u001b[0m[0158] Starting container [rke-cp-port-listener] on host [104.45.86.4], try #1 \r\n"]
[158.735591, "o", "\u001b[36mINFO\u001b[0m[0158] Starting container [rke-cp-port-listener] on host [104.45.84.172], try #1 \r\n"]
[159.774505, "o", "\u001b[36mINFO\u001b[0m[0159] [network] Successfully started [rke-cp-port-listener] container on host [23.102.48.17] \r\n"]
[159.827343, "o", "\u001b[36mINFO\u001b[0m[0159] [network] Successfully started [rke-cp-port-listener] container on host [104.45.86.4] \r\n"]
[159.951163, "o", "\u001b[36mINFO\u001b[0m[0159] [network] Successfully started [rke-cp-port-listener] container on host [104.45.84.172] \r\n"]
[160.149603, "o", "\u001b[36mINFO\u001b[0m[0159] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[160.150008, "o", "\u001b[36mINFO\u001b[0m[0159] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[160.15073, "o", "\u001b[36mINFO\u001b[0m[0159] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[161.198489, "o", "\u001b[36mINFO\u001b[0m[0160] Starting container [rke-worker-port-listener] on host [23.102.23.168], try #1 \r\n"]
[161.279096, "o", "\u001b[36mINFO\u001b[0m[0160] Starting container [rke-worker-port-listener] on host [23.102.22.29], try #1 \r\n"]
[162.539342, "o", "\u001b[36mINFO\u001b[0m[0161] [network] Successfully started [rke-worker-port-listener] container on host [23.102.23.168] \r\n"]
[162.562763, "o", "\u001b[36mINFO\u001b[0m[0161] [network] Successfully started [rke-worker-port-listener] container on host [23.102.22.29] \r\n"]
[163.460831, "o", "\u001b[36mINFO\u001b[0m[0162] Starting container [rke-worker-port-listener] on host [137.135.244.150], try #1 \r\n"]
[164.588187, "o", "\u001b[36mINFO\u001b[0m[0163] [network] Successfully started [rke-worker-port-listener] container on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0163] [network] Port listener containers deployed successfully \r\n\u001b[36mINFO\u001b[0m[0163] [network] Running etcd <-> etcd port checks  \r\n"]
[164.850121, "o", "\u001b[36mINFO\u001b[0m[0164] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[164.85574, "o", "\u001b[36mINFO\u001b[0m[0164] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[164.858212, "o", "\u001b[36mINFO\u001b[0m[0164] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[165.816616, "o", "\u001b[36mINFO\u001b[0m[0165] Starting container [rke-port-checker] on host [104.45.84.172], try #1 \r\n\u001b[36mINFO\u001b[0m[0165] Starting container [rke-port-checker] on host [23.102.48.17], try #1 \r\n"]
[165.826276, "o", "\u001b[36mINFO\u001b[0m[0165] Starting container [rke-port-checker] on host [104.45.86.4], try #1 \r\n"]
[166.840597, "o", "\u001b[36mINFO\u001b[0m[0166] [network] Successfully started [rke-port-checker] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0166] [network] Successfully started [rke-port-checker] container on host [104.45.86.4] \r\n"]
[166.841018, "o", "\u001b[36mINFO\u001b[0m[0166] [network] Successfully started [rke-port-checker] container on host [104.45.84.172] \r\n"]
[167.214268, "o", "\u001b[36mINFO\u001b[0m[0166] Removing container [rke-port-checker] on host [104.45.86.4], try #1 \r\n\u001b[36mINFO\u001b[0m[0166] Removing container [rke-port-checker] on host [23.102.48.17], try #1 \r\n"]
[167.256042, "o", "\u001b[36mINFO\u001b[0m[0166] Removing container [rke-port-checker] on host [104.45.84.172], try #1 \r\n"]
[167.52595, "o", "\u001b[36mINFO\u001b[0m[0166] [network] Running control plane -> etcd port checks \r\n"]
[167.787477, "o", "\u001b[36mINFO\u001b[0m[0167] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0167] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[167.796159, "o", "\u001b[36mINFO\u001b[0m[0167] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[168.684167, "o", "\u001b[36mINFO\u001b[0m[0168] Starting container [rke-port-checker] on host [23.102.48.17], try #1 \r\n"]
[168.684678, "o", "\u001b[36mINFO\u001b[0m[0168] Starting container [rke-port-checker] on host [104.45.86.4], try #1 \r\n"]
[168.718074, "o", "\u001b[36mINFO\u001b[0m[0168] Starting container [rke-port-checker] on host [104.45.84.172], try #1 \r\n"]
[169.502286, "o", "\u001b[36mINFO\u001b[0m[0168] [network] Successfully started [rke-port-checker] container on host [104.45.86.4] \r\n"]
[169.502545, "o", "\u001b[36mINFO\u001b[0m[0168] [network] Successfully started [rke-port-checker] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0168] [network] Successfully started [rke-port-checker] container on host [104.45.84.172] \r\n"]
[169.87361, "o", "\u001b[36mINFO\u001b[0m[0169] Removing container [rke-port-checker] on host [23.102.48.17], try #1 \r\n"]
[169.927459, "o", "\u001b[36mINFO\u001b[0m[0169] Removing container [rke-port-checker] on host [104.45.86.4], try #1 \r\n"]
[169.977154, "o", "\u001b[36mINFO\u001b[0m[0169] Removing container [rke-port-checker] on host [104.45.84.172], try #1 \r\n"]
[170.160625, "o", "\u001b[36mINFO\u001b[0m[0169] [network] Running control plane -> worker port checks \r\n"]
[170.435697, "o", "\u001b[36mINFO\u001b[0m[0169] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0169] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[170.440371, "o", "\u001b[36mINFO\u001b[0m[0169] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[171.140951, "o", "\u001b[36mINFO\u001b[0m[0170] Starting container [rke-port-checker] on host [23.102.48.17], try #1 \r\n"]
[171.156914, "o", "\u001b[36mINFO\u001b[0m[0170] Starting container [rke-port-checker] on host [104.45.86.4], try #1 \r\n"]
[171.435527, "o", "\u001b[36mINFO\u001b[0m[0170] Starting container [rke-port-checker] on host [104.45.84.172], try #1 \r\n"]
[171.914675, "o", "\u001b[36mINFO\u001b[0m[0171] [network] Successfully started [rke-port-checker] container on host [23.102.48.17] \r\n"]
[171.914833, "o", "\u001b[36mINFO\u001b[0m[0171] [network] Successfully started [rke-port-checker] container on host [104.45.86.4] \r\n"]
[172.267634, "o", "\u001b[36mINFO\u001b[0m[0171] [network] Successfully started [rke-port-checker] container on host [104.45.84.172] \r\n"]
[172.329398, "o", "\u001b[36mINFO\u001b[0m[0171] Removing container [rke-port-checker] on host [104.45.86.4], try #1 \r\n"]
[172.330706, "o", "\u001b[36mINFO\u001b[0m[0171] Removing container [rke-port-checker] on host [23.102.48.17], try #1 \r\n"]
[172.623708, "o", "\u001b[36mINFO\u001b[0m[0172] Removing container [rke-port-checker] on host [104.45.84.172], try #1 \r\n"]
[172.791378, "o", "\u001b[36mINFO\u001b[0m[0172] [network] Running workers -> control plane port checks \r\n"]
[173.058209, "o", "\u001b[36mINFO\u001b[0m[0172] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[173.058856, "o", "\u001b[36mINFO\u001b[0m[0172] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[173.060219, "o", "\u001b[36mINFO\u001b[0m[0172] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[173.808299, "o", "\u001b[36mINFO\u001b[0m[0173] Starting container [rke-port-checker] on host [23.102.23.168], try #1 \r\n"]
[174.122921, "o", "\u001b[36mINFO\u001b[0m[0173] Starting container [rke-port-checker] on host [23.102.22.29], try #1 \r\n"]
[174.821693, "o", "\u001b[36mINFO\u001b[0m[0174] [network] Successfully started [rke-port-checker] container on host [23.102.23.168] \r\n"]
[175.032246, "o", "\u001b[36mINFO\u001b[0m[0174] [network] Successfully started [rke-port-checker] container on host [23.102.22.29] \r\n"]
[175.400874, "o", "\u001b[36mINFO\u001b[0m[0174] Removing container [rke-port-checker] on host [23.102.23.168], try #1 \r\n"]
[175.502123, "o", "\u001b[36mINFO\u001b[0m[0174] Removing container [rke-port-checker] on host [23.102.22.29], try #1 \r\n"]
[176.370287, "o", "\u001b[36mINFO\u001b[0m[0175] Starting container [rke-port-checker] on host [137.135.244.150], try #1 \r\n"]
[177.081403, "o", "\u001b[36mINFO\u001b[0m[0176] [network] Successfully started [rke-port-checker] container on host [137.135.244.150] \r\n"]
[177.451856, "o", "\u001b[36mINFO\u001b[0m[0176] Removing container [rke-port-checker] on host [137.135.244.150], try #1 \r\n"]
[177.557705, "o", "\u001b[36mINFO\u001b[0m[0176] [network] Checking KubeAPI port Control Plane hosts \r\n"]
[177.763456, "o", "\u001b[36mINFO\u001b[0m[0177] [network] Removing port listener containers  \r\n"]
[177.898593, "o", "\u001b[36mINFO\u001b[0m[0177] Removing container [rke-etcd-port-listener] on host [104.45.84.172], try #1 \r\n"]
[177.898988, "o", "\u001b[36mINFO\u001b[0m[0177] Removing container [rke-etcd-port-listener] on host [104.45.86.4], try #1 \r\n\u001b[36mINFO\u001b[0m[0177] Removing container [rke-etcd-port-listener] on host [23.102.48.17], try #1 \r\n"]
[178.82074, "o", "\u001b[36mINFO\u001b[0m[0178] [remove/rke-etcd-port-listener] Successfully removed container on host [104.45.86.4] \r\n"]
[178.88485, "o", "\u001b[36mINFO\u001b[0m[0178] [remove/rke-etcd-port-listener] Successfully removed container on host [104.45.84.172] \r\n"]
[178.890996, "o", "\u001b[36mINFO\u001b[0m[0178] [remove/rke-etcd-port-listener] Successfully removed container on host [23.102.48.17] \r\n"]
[179.017677, "o", "\u001b[36mINFO\u001b[0m[0178] Removing container [rke-cp-port-listener] on host [104.45.84.172], try #1 \r\n"]
[179.025562, "o", "\u001b[36mINFO\u001b[0m[0178] Removing container [rke-cp-port-listener] on host [104.45.86.4], try #1 \r\n"]
[179.02728, "o", "\u001b[36mINFO\u001b[0m[0178] Removing container [rke-cp-port-listener] on host [23.102.48.17], try #1 \r\n"]
[179.948174, "o", "\u001b[36mINFO\u001b[0m[0179] [remove/rke-cp-port-listener] Successfully removed container on host [104.45.84.172] \r\n"]
[179.94846, "o", "\u001b[36mINFO\u001b[0m[0179] [remove/rke-cp-port-listener] Successfully removed container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0179] [remove/rke-cp-port-listener] Successfully removed container on host [104.45.86.4] \r\n"]
[180.087086, "o", "\u001b[36mINFO\u001b[0m[0179] Removing container [rke-worker-port-listener] on host [23.102.22.29], try #1 \r\n"]
[180.087131, "o", "\u001b[36mINFO\u001b[0m[0179] Removing container [rke-worker-port-listener] on host [23.102.23.168], try #1 \r\n"]
[180.091531, "o", "\u001b[36mINFO\u001b[0m[0179] Removing container [rke-worker-port-listener] on host [137.135.244.150], try #1 \r\n"]
[180.868459, "o", "\u001b[36mINFO\u001b[0m[0180] [remove/rke-worker-port-listener] Successfully removed container on host [137.135.244.150] \r\n"]
[181.073159, "o", "\u001b[36mINFO\u001b[0m[0180] [remove/rke-worker-port-listener] Successfully removed container on host [23.102.23.168] \r\n"]
[181.38142, "o", "\u001b[36mINFO\u001b[0m[0180] [remove/rke-worker-port-listener] Successfully removed container on host [23.102.22.29] \r\n\u001b[36mINFO\u001b[0m[0180] [network] Port listener containers removed successfully \r\n"]
[181.381825, "o", "\u001b[36mINFO\u001b[0m[0180] [certificates] Deploying kubernetes certificates to Cluster nodes \r\n"]
[181.382208, "o", "\u001b[36mINFO\u001b[0m[0180] Checking if container [cert-deployer] is running on host [137.135.244.150], try #1 \r\n"]
[181.382306, "o", "\u001b[36mINFO\u001b[0m[0180] Checking if container [cert-deployer] is running on host [23.102.23.168], try #1 \r\n\u001b[36mINFO\u001b[0m[0180] Checking if container [cert-deployer] is running on host [23.102.22.29], try #1 \r\n"]
[181.383302, "o", "\u001b[36mINFO\u001b[0m[0180] Checking if container [cert-deployer] is running on host [23.102.48.17], try #1 \r\n\u001b[36mINFO\u001b[0m[0180] Checking if container [cert-deployer] is running on host [104.45.84.172], try #1 \r\n"]
[181.383643, "o", "\u001b[36mINFO\u001b[0m[0180] Checking if container [cert-deployer] is running on host [104.45.86.4], try #1 \r\n"]
[181.578661, "o", "\u001b[36mINFO\u001b[0m[0180] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[181.587883, "o", "\u001b[36mINFO\u001b[0m[0180] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[181.587972, "o", "\u001b[36mINFO\u001b[0m[0180] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[181.588452, "o", "\u001b[36mINFO\u001b[0m[0180] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[181.595155, "o", "\u001b[36mINFO\u001b[0m[0180] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[181.596068, "o", "\u001b[36mINFO\u001b[0m[0181] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[182.418409, "o", "\u001b[36mINFO\u001b[0m[0181] Starting container [cert-deployer] on host [23.102.23.168], try #1 \r\n"]
[183.10729, "o", "\u001b[36mINFO\u001b[0m[0182] Starting container [cert-deployer] on host [23.102.22.29], try #1 \r\n"]
[183.786298, "o", "\u001b[36mINFO\u001b[0m[0183] Starting container [cert-deployer] on host [23.102.48.17], try #1 \r\n"]
[183.908074, "o", "\u001b[36mINFO\u001b[0m[0183] Starting container [cert-deployer] on host [104.45.86.4], try #1 \r\n"]
[183.977205, "o", "\u001b[36mINFO\u001b[0m[0183] Checking if container [cert-deployer] is running on host [23.102.23.168], try #1 \r\n"]
[184.327718, "o", "\u001b[36mINFO\u001b[0m[0183] Checking if container [cert-deployer] is running on host [23.102.22.29], try #1 \r\n"]
[184.499613, "o", "\u001b[36mINFO\u001b[0m[0183] Starting container [cert-deployer] on host [137.135.244.150], try #1 \r\n"]
[185.067845, "o", "\u001b[36mINFO\u001b[0m[0184] Checking if container [cert-deployer] is running on host [23.102.48.17], try #1 \r\n"]
[185.088101, "o", "\u001b[36mINFO\u001b[0m[0184] Checking if container [cert-deployer] is running on host [104.45.86.4], try #1 \r\n"]
[185.367925, "o", "\u001b[36mINFO\u001b[0m[0184] Checking if container [cert-deployer] is running on host [137.135.244.150], try #1 \r\n"]
[185.452858, "o", "\u001b[36mINFO\u001b[0m[0184] Starting container [cert-deployer] on host [104.45.84.172], try #1 \r\n"]
[186.706311, "o", "\u001b[36mINFO\u001b[0m[0186] Checking if container [cert-deployer] is running on host [104.45.84.172], try #1 \r\n"]
[189.045628, "o", "\u001b[36mINFO\u001b[0m[0188] Checking if container [cert-deployer] is running on host [23.102.23.168], try #1 \r\n"]
[189.111707, "o", "\u001b[36mINFO\u001b[0m[0188] Removing container [cert-deployer] on host [23.102.23.168], try #1 \r\n"]
[189.398369, "o", "\u001b[36mINFO\u001b[0m[0188] Checking if container [cert-deployer] is running on host [23.102.22.29], try #1 \r\n"]
[189.46264, "o", "\u001b[36mINFO\u001b[0m[0188] Removing container [cert-deployer] on host [23.102.22.29], try #1 \r\n"]
[190.143668, "o", "\u001b[36mINFO\u001b[0m[0189] Checking if container [cert-deployer] is running on host [23.102.48.17], try #1 \r\n"]
[190.163416, "o", "\u001b[36mINFO\u001b[0m[0189] Checking if container [cert-deployer] is running on host [104.45.86.4], try #1 \r\n"]
[190.215775, "o", "\u001b[36mINFO\u001b[0m[0189] Removing container [cert-deployer] on host [23.102.48.17], try #1 \r\n"]
[190.228939, "o", "\u001b[36mINFO\u001b[0m[0189] Removing container [cert-deployer] on host [104.45.86.4], try #1 \r\n"]
[190.435567, "o", "\u001b[36mINFO\u001b[0m[0189] Checking if container [cert-deployer] is running on host [137.135.244.150], try #1 \r\n"]
[190.499988, "o", "\u001b[36mINFO\u001b[0m[0189] Removing container [cert-deployer] on host [137.135.244.150], try #1 \r\n"]
[191.779659, "o", "\u001b[36mINFO\u001b[0m[0191] Checking if container [cert-deployer] is running on host [104.45.84.172], try #1 \r\n"]
[191.843903, "o", "\u001b[36mINFO\u001b[0m[0191] Removing container [cert-deployer] on host [104.45.84.172], try #1 \r\n"]
[192.031487, "o", "\u001b[36mINFO\u001b[0m[0191] [reconcile] Rebuilding and updating local kube config \r\n"]
[192.032042, "o", "\u001b[36mINFO\u001b[0m[0191] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_au1.yml] \r\n"]
[192.113964, "o", "\u001b[36mINFO\u001b[0m[0191] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_au1.yml] \r\n"]
[192.180549, "o", "\u001b[36mINFO\u001b[0m[0191] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_au1.yml] \r\n"]
[192.239038, "o", "\u001b[36mINFO\u001b[0m[0191] [certificates] Successfully deployed kubernetes certificates to Cluster nodes \r\n"]
[192.239495, "o", "\u001b[36mINFO\u001b[0m[0191] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [104.45.84.172] \r\n"]
[192.497252, "o", "\u001b[36mINFO\u001b[0m[0191] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[193.259643, "o", "\u001b[36mINFO\u001b[0m[0192] Starting container [file-deployer] on host [104.45.84.172], try #1 \r\n"]
[194.591871, "o", "\u001b[36mINFO\u001b[0m[0193] Successfully started [file-deployer] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0193] Waiting for [file-deployer] container to exit on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0193] Waiting for [file-deployer] container to exit on host [104.45.84.172] \r\n"]
[195.292187, "o", "\u001b[36mINFO\u001b[0m[0194] Removing container [file-deployer] on host [104.45.84.172], try #1 \r\n"]
[195.447372, "o", "\u001b[36mINFO\u001b[0m[0194] [remove/file-deployer] Successfully removed container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0194] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [104.45.86.4] \r\n"]
[195.716411, "o", "\u001b[36mINFO\u001b[0m[0195] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[196.638422, "o", "\u001b[36mINFO\u001b[0m[0196] Starting container [file-deployer] on host [104.45.86.4], try #1 \r\n"]
[197.765316, "o", "\u001b[36mINFO\u001b[0m[0197] Successfully started [file-deployer] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0197] Waiting for [file-deployer] container to exit on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0197] Waiting for [file-deployer] container to exit on host [104.45.86.4] \r\n"]
[198.575236, "o", "\u001b[36mINFO\u001b[0m[0197] Removing container [file-deployer] on host [104.45.86.4], try #1 \r\n"]
[198.789206, "o", "\u001b[36mINFO\u001b[0m[0198] [remove/file-deployer] Successfully removed container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0198] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [23.102.48.17] \r\n"]
[199.052592, "o", "\u001b[36mINFO\u001b[0m[0198] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[200.122211, "o", "\u001b[36mINFO\u001b[0m[0199] Starting container [file-deployer] on host [23.102.48.17], try #1 \r\n"]
[201.350012, "o", "\u001b[36mINFO\u001b[0m[0200] Successfully started [file-deployer] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0200] Waiting for [file-deployer] container to exit on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0200] Waiting for [file-deployer] container to exit on host [23.102.48.17] \r\n"]
[202.158802, "o", "\u001b[36mINFO\u001b[0m[0201] Removing container [file-deployer] on host [23.102.48.17], try #1 \r\n"]
[202.476619, "o", "\u001b[36mINFO\u001b[0m[0201] [remove/file-deployer] Successfully removed container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0201] [/etc/kubernetes/audit-policy.yaml] Successfully deployed audit policy file to Cluster control nodes \r\n\u001b[36mINFO\u001b[0m[0201] [reconcile] Reconciling cluster state        \r\n\u001b[36mINFO\u001b[0m[0201] [reconcile] This is newly generated cluster  \r\n\u001b[36mINFO\u001b[0m[0201] Pre-pulling kubernetes images                \r\n"]
[202.541269, "o", "\u001b[36mINFO\u001b[0m[0201] Pulling image [rancher/hyperkube:v1.18.12-rancher1] on host [23.102.22.29], try #1 \r\n\u001b[36mINFO\u001b[0m[0201] Pulling image [rancher/hyperkube:v1.18.12-rancher1] on host [104.45.86.4], try #1 \r\n"]
[202.541695, "o", "\u001b[36mINFO\u001b[0m[0201] Pulling image [rancher/hyperkube:v1.18.12-rancher1] on host [104.45.84.172], try #1 \r\n"]
[202.542296, "o", "\u001b[36mINFO\u001b[0m[0201] Pulling image [rancher/hyperkube:v1.18.12-rancher1] on host [23.102.48.17], try #1 \r\n\u001b[36mINFO\u001b[0m[0201] Pulling image [rancher/hyperkube:v1.18.12-rancher1] on host [23.102.23.168], try #1 \r\n"]
[202.544955, "o", "\u001b[36mINFO\u001b[0m[0201] Pulling image [rancher/hyperkube:v1.18.12-rancher1] on host [137.135.244.150], try #1 \r\n"]
[373.084025, "o", "\u001b[36mINFO\u001b[0m[0372] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.84.172] \r\n"]
[380.618872, "o", "\u001b[36mINFO\u001b[0m[0380] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.48.17] \r\n"]
[389.754051, "o", "\u001b[36mINFO\u001b[0m[0389] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.86.4] \r\n"]
[396.220628, "o", "\u001b[36mINFO\u001b[0m[0395] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.23.168] \r\n"]
[408.762371, "o", "\u001b[36mINFO\u001b[0m[0408] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.22.29] \r\n"]
[415.5911, "o", "\u001b[36mINFO\u001b[0m[0414] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0414] Kubernetes images pulled successfully        \r\n"]
[415.594259, "o", "\u001b[36mINFO\u001b[0m[0414] [etcd] Building up etcd plane..              \r\n"]
[415.80598, "o", "\u001b[36mINFO\u001b[0m[0415] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[419.048256, "o", "\u001b[36mINFO\u001b[0m[0418] Starting container [etcd-fix-perm] on host [104.45.86.4], try #1 \r\n"]
[421.101887, "o", "\u001b[36mINFO\u001b[0m[0420] Successfully started [etcd-fix-perm] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0420] Waiting for [etcd-fix-perm] container to exit on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0420] Waiting for [etcd-fix-perm] container to exit on host [104.45.86.4] \r\n"]
[421.510936, "o", "\u001b[36mINFO\u001b[0m[0420] Container [etcd-fix-perm] is still running on host [104.45.86.4]: stderr: [], stdout: [] \r\n"]
[422.511179, "o", "\u001b[36mINFO\u001b[0m[0421] Waiting for [etcd-fix-perm] container to exit on host [104.45.86.4] \r\n"]
[422.776018, "o", "\u001b[36mINFO\u001b[0m[0422] Container [etcd-fix-perm] is still running on host [104.45.86.4]: stderr: [], stdout: [] \r\n"]
[423.776142, "o", "\u001b[36mINFO\u001b[0m[0423] Waiting for [etcd-fix-perm] container to exit on host [104.45.86.4] \r\n"]
[424.472122, "o", "\u001b[36mINFO\u001b[0m[0423] Removing container [etcd-fix-perm] on host [104.45.86.4], try #1 \r\n"]
[424.693499, "o", "\u001b[36mINFO\u001b[0m[0424] [remove/etcd-fix-perm] Successfully removed container on host [104.45.86.4] \r\n"]
[424.859593, "o", "\u001b[36mINFO\u001b[0m[0424] Pulling image [rancher/coreos-etcd:v3.4.3-rancher1] on host [104.45.86.4], try #1 \r\n"]
[435.835988, "o", "\u001b[36mINFO\u001b[0m[0435] Image [rancher/coreos-etcd:v3.4.3-rancher1] exists on host [104.45.86.4] \r\n"]
[436.256637, "o", "\u001b[36mINFO\u001b[0m[0435] Starting container [etcd] on host [104.45.86.4], try #1 \r\n"]
[436.973254, "o", "\u001b[36mINFO\u001b[0m[0436] [etcd] Successfully started [etcd] container on host [104.45.86.4] \r\n"]
[436.973644, "o", "\u001b[36mINFO\u001b[0m[0436] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [104.45.86.4] \r\n"]
[437.236779, "o", "\u001b[36mINFO\u001b[0m[0436] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[438.222019, "o", "\u001b[36mINFO\u001b[0m[0437] Starting container [etcd-rolling-snapshots] on host [104.45.86.4], try #1 \r\n"]
[439.021692, "o", "\u001b[36mINFO\u001b[0m[0438] [etcd] Successfully started [etcd-rolling-snapshots] container on host [104.45.86.4] \r\n"]
[444.28787, "o", "\u001b[36mINFO\u001b[0m[0443] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[445.06371, "o", "\u001b[36mINFO\u001b[0m[0444] Starting container [rke-bundle-cert] on host [104.45.86.4], try #1 \r\n"]
[446.224364, "o", "\u001b[36mINFO\u001b[0m[0445] [certificates] Successfully started [rke-bundle-cert] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0445] Waiting for [rke-bundle-cert] container to exit on host [104.45.86.4] \r\n"]
[446.872772, "o", "\u001b[36mINFO\u001b[0m[0446] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0446] Removing container [rke-bundle-cert] on host [104.45.86.4], try #1 \r\n"]
[447.53855, "o", "\u001b[36mINFO\u001b[0m[0446] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[448.442644, "o", "\u001b[36mINFO\u001b[0m[0447] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[449.568629, "o", "\u001b[36mINFO\u001b[0m[0448] [etcd] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[450.099284, "o", "\u001b[36mINFO\u001b[0m[0449] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[450.263689, "o", "\u001b[36mINFO\u001b[0m[0449] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n"]
[450.648945, "o", "\u001b[36mINFO\u001b[0m[0450] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[451.515323, "o", "\u001b[36mINFO\u001b[0m[0450] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[452.64053, "o", "\u001b[36mINFO\u001b[0m[0452] [etcd] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[453.228093, "o", "\u001b[36mINFO\u001b[0m[0452] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[453.373657, "o", "\u001b[36mINFO\u001b[0m[0452] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n"]
[455.968214, "o", "\u001b[36mINFO\u001b[0m[0455] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[458.272175, "o", "\u001b[36mINFO\u001b[0m[0457] Starting container [etcd-fix-perm] on host [23.102.48.17], try #1 \r\n"]
[459.808769, "o", "\u001b[36mINFO\u001b[0m[0459] Successfully started [etcd-fix-perm] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0459] Waiting for [etcd-fix-perm] container to exit on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0459] Waiting for [etcd-fix-perm] container to exit on host [23.102.48.17] \r\n"]
[460.066247, "o", "\u001b[36mINFO\u001b[0m[0459] Container [etcd-fix-perm] is still running on host [23.102.48.17]: stderr: [], stdout: [] \r\n"]
[461.066512, "o", "\u001b[36mINFO\u001b[0m[0460] Waiting for [etcd-fix-perm] container to exit on host [23.102.48.17] \r\n"]
[461.815449, "o", "\u001b[36mINFO\u001b[0m[0461] Removing container [etcd-fix-perm] on host [23.102.48.17], try #1 \r\n"]
[461.961546, "o", "\u001b[36mINFO\u001b[0m[0461] [remove/etcd-fix-perm] Successfully removed container on host [23.102.48.17] \r\n"]
[462.094464, "o", "\u001b[36mINFO\u001b[0m[0461] Pulling image [rancher/coreos-etcd:v3.4.3-rancher1] on host [23.102.48.17], try #1 \r\n"]
[472.420652, "o", "\u001b[36mINFO\u001b[0m[0471] Image [rancher/coreos-etcd:v3.4.3-rancher1] exists on host [23.102.48.17] \r\n"]
[474.452075, "o", "\u001b[36mINFO\u001b[0m[0473] Starting container [etcd] on host [23.102.48.17], try #1 \r\n"]
[475.272023, "o", "\u001b[36mINFO\u001b[0m[0474] [etcd] Successfully started [etcd] container on host [23.102.48.17] \r\n"]
[475.272286, "o", "\u001b[36mINFO\u001b[0m[0474] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [23.102.48.17] \r\n"]
[475.542585, "o", "\u001b[36mINFO\u001b[0m[0474] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[476.295334, "o", "\u001b[36mINFO\u001b[0m[0475] Starting container [etcd-rolling-snapshots] on host [23.102.48.17], try #1 \r\n"]
[477.069107, "o", "\u001b[36mINFO\u001b[0m[0476] [etcd] Successfully started [etcd-rolling-snapshots] container on host [23.102.48.17] \r\n"]
[482.343249, "o", "\u001b[36mINFO\u001b[0m[0481] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[483.263218, "o", "\u001b[36mINFO\u001b[0m[0482] Starting container [rke-bundle-cert] on host [23.102.48.17], try #1 \r\n"]
[484.590644, "o", "\u001b[36mINFO\u001b[0m[0483] [certificates] Successfully started [rke-bundle-cert] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0483] Waiting for [rke-bundle-cert] container to exit on host [23.102.48.17] \r\n"]
[485.165373, "o", "\u001b[36mINFO\u001b[0m[0484] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0484] Removing container [rke-bundle-cert] on host [23.102.48.17], try #1 \r\n"]
[485.77356, "o", "\u001b[36mINFO\u001b[0m[0485] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[486.740983, "o", "\u001b[36mINFO\u001b[0m[0486] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[487.97061, "o", "\u001b[36mINFO\u001b[0m[0487] [etcd] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[488.485414, "o", "\u001b[36mINFO\u001b[0m[0487] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[488.649368, "o", "\u001b[36mINFO\u001b[0m[0488] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[489.055519, "o", "\u001b[36mINFO\u001b[0m[0488] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[489.916801, "o", "\u001b[36mINFO\u001b[0m[0489] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[491.041957, "o", "\u001b[36mINFO\u001b[0m[0490] [etcd] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[491.721331, "o", "\u001b[36mINFO\u001b[0m[0491] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[491.868755, "o", "\u001b[36mINFO\u001b[0m[0491] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[494.484132, "o", "\u001b[36mINFO\u001b[0m[0493] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[497.288401, "o", "\u001b[36mINFO\u001b[0m[0496] Starting container [etcd-fix-perm] on host [104.45.84.172], try #1 \r\n"]
[498.925834, "o", "\u001b[36mINFO\u001b[0m[0498] Successfully started [etcd-fix-perm] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0498] Waiting for [etcd-fix-perm] container to exit on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0498] Waiting for [etcd-fix-perm] container to exit on host [104.45.84.172] \r\n"]
[499.161829, "o", "\u001b[36mINFO\u001b[0m[0498] Container [etcd-fix-perm] is still running on host [104.45.84.172]: stderr: [], stdout: [] \r\n"]
[500.162146, "o", "\u001b[36mINFO\u001b[0m[0499] Waiting for [etcd-fix-perm] container to exit on host [104.45.84.172] \r\n"]
[500.636961, "o", "\u001b[36mINFO\u001b[0m[0500] Removing container [etcd-fix-perm] on host [104.45.84.172], try #1 \r\n"]
[500.788887, "o", "\u001b[36mINFO\u001b[0m[0500] [remove/etcd-fix-perm] Successfully removed container on host [104.45.84.172] \r\n"]
[500.903048, "o", "\u001b[36mINFO\u001b[0m[0500] Pulling image [rancher/coreos-etcd:v3.4.3-rancher1] on host [104.45.84.172], try #1 \r\n"]
[510.045135, "o", "\u001b[36mINFO\u001b[0m[0509] Image [rancher/coreos-etcd:v3.4.3-rancher1] exists on host [104.45.84.172] \r\n"]
[512.033441, "o", "\u001b[36mINFO\u001b[0m[0511] Starting container [etcd] on host [104.45.84.172], try #1 \r\n"]
[512.852621, "o", "\u001b[36mINFO\u001b[0m[0512] [etcd] Successfully started [etcd] container on host [104.45.84.172] \r\n"]
[512.852962, "o", "\u001b[36mINFO\u001b[0m[0512] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [104.45.84.172] \r\n"]
[513.039342, "o", "\u001b[36mINFO\u001b[0m[0512] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[513.876458, "o", "\u001b[36mINFO\u001b[0m[0513] Starting container [etcd-rolling-snapshots] on host [104.45.84.172], try #1 \r\n"]
[514.695307, "o", "\u001b[36mINFO\u001b[0m[0514] [etcd] Successfully started [etcd-rolling-snapshots] container on host [104.45.84.172] \r\n"]
[519.986569, "o", "\u001b[36mINFO\u001b[0m[0519] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[520.943834, "o", "\u001b[36mINFO\u001b[0m[0520] Starting container [rke-bundle-cert] on host [104.45.84.172], try #1 \r\n"]
[522.122954, "o", "\u001b[36mINFO\u001b[0m[0521] [certificates] Successfully started [rke-bundle-cert] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0521] Waiting for [rke-bundle-cert] container to exit on host [104.45.84.172] \r\n"]
[522.723672, "o", "\u001b[36mINFO\u001b[0m[0522] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0522] Removing container [rke-bundle-cert] on host [104.45.84.172], try #1 \r\n"]
[523.33974, "o", "\u001b[36mINFO\u001b[0m[0522] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[524.19493, "o", "\u001b[36mINFO\u001b[0m[0523] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[525.345762, "o", "\u001b[36mINFO\u001b[0m[0524] [etcd] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[525.463051, "o", "\u001b[36mINFO\u001b[0m[0524] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[526.062434, "o", "\u001b[36mINFO\u001b[0m[0525] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n"]
[526.420898, "o", "\u001b[36mINFO\u001b[0m[0525] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[527.393754, "o", "\u001b[36mINFO\u001b[0m[0526] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[528.622115, "o", "\u001b[36mINFO\u001b[0m[0528] [etcd] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[529.291915, "o", "\u001b[36mINFO\u001b[0m[0528] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[529.425912, "o", "\u001b[36mINFO\u001b[0m[0528] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0528] [etcd] Successfully started etcd plane.. Checking etcd cluster health \r\n"]
[532.264658, "o", "\u001b[36mINFO\u001b[0m[0531] [etcd] etcd host [104.45.86.4] reported healthy=true \r\n"]
[532.265175, "o", "\u001b[36mINFO\u001b[0m[0531] [controlplane] Building up Controller Plane.. \r\n"]
[532.265533, "o", "\u001b[36mINFO\u001b[0m[0531] Checking if container [service-sidekick] is running on host [23.102.48.17], try #1 \r\n\u001b[36mINFO\u001b[0m[0531] Checking if container [service-sidekick] is running on host [104.45.84.172], try #1 \r\n\u001b[36mINFO\u001b[0m[0531] Checking if container [service-sidekick] is running on host [104.45.86.4], try #1 \r\n"]
[532.500282, "o", "\u001b[36mINFO\u001b[0m[0531] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[532.547703, "o", "\u001b[36mINFO\u001b[0m[0531] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[533.535485, "o", "\u001b[36mINFO\u001b[0m[0532] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.48.17] \r\n"]
[533.577454, "o", "\u001b[36mINFO\u001b[0m[0532] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.84.172] \r\n"]
[533.84523, "o", "\u001b[36mINFO\u001b[0m[0533] Starting container [kube-apiserver] on host [23.102.48.17], try #1 \r\n"]
[533.845383, "o", "\u001b[36mINFO\u001b[0m[0533] Starting container [kube-apiserver] on host [104.45.84.172], try #1 \r\n"]
[534.639725, "o", "\u001b[36mINFO\u001b[0m[0534] [controlplane] Successfully started [kube-apiserver] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0534] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [23.102.48.17] \r\n"]
[534.663164, "o", "\u001b[36mINFO\u001b[0m[0534] [controlplane] Successfully started [kube-apiserver] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0534] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [104.45.84.172] \r\n"]
[534.880253, "o", "\u001b[36mINFO\u001b[0m[0534] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[535.771576, "o", "\u001b[36mINFO\u001b[0m[0535] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.86.4] \r\n"]
[536.200872, "o", "\u001b[36mINFO\u001b[0m[0535] Starting container [kube-apiserver] on host [104.45.86.4], try #1 \r\n"]
[537.019967, "o", "\u001b[36mINFO\u001b[0m[0536] [controlplane] Successfully started [kube-apiserver] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0536] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [104.45.86.4] \r\n"]
[563.920001, "o", "\u001b[36mINFO\u001b[0m[0563] [healthcheck] service [kube-apiserver] on host [104.45.84.172] is healthy \r\n"]
[564.208981, "o", "\u001b[36mINFO\u001b[0m[0563] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[564.307008, "o", "\u001b[36mINFO\u001b[0m[0563] [healthcheck] service [kube-apiserver] on host [104.45.86.4] is healthy \r\n"]
[564.634971, "o", "\u001b[36mINFO\u001b[0m[0564] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[564.838147, "o", "\u001b[36mINFO\u001b[0m[0564] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[565.487062, "o", "\u001b[36mINFO\u001b[0m[0564] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[565.939195, "o", "\u001b[36mINFO\u001b[0m[0565] [healthcheck] service [kube-apiserver] on host [23.102.48.17] is healthy \r\n"]
[566.277513, "o", "\u001b[36mINFO\u001b[0m[0565] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[567.022407, "o", "\u001b[36mINFO\u001b[0m[0566] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[567.522928, "o", "\u001b[36mINFO\u001b[0m[0566] [controlplane] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[567.638693, "o", "\u001b[36mINFO\u001b[0m[0567] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[568.251436, "o", "\u001b[36mINFO\u001b[0m[0567] [controlplane] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0567] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n"]
[568.425617, "o", "\u001b[36mINFO\u001b[0m[0567] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.84.172] \r\n"]
[568.763629, "o", "\u001b[36mINFO\u001b[0m[0568] Starting container [kube-controller-manager] on host [104.45.84.172], try #1 \r\n"]
[568.829041, "o", "\u001b[36mINFO\u001b[0m[0568] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[568.984301, "o", "\u001b[36mINFO\u001b[0m[0568] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n"]
[569.178873, "o", "\u001b[36mINFO\u001b[0m[0568] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.86.4] \r\n"]
[569.582571, "o", "\u001b[36mINFO\u001b[0m[0568] Starting container [kube-controller-manager] on host [104.45.86.4], try #1 \r\n"]
[569.58831, "o", "\u001b[36mINFO\u001b[0m[0568] [controlplane] Successfully started [kube-controller-manager] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0568] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [104.45.84.172] \r\n"]
[569.829997, "o", "\u001b[36mINFO\u001b[0m[0569] [controlplane] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[570.301204, "o", "\u001b[36mINFO\u001b[0m[0569] [controlplane] Successfully started [kube-controller-manager] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0569] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [104.45.86.4] \r\n"]
[570.460786, "o", "\u001b[36mINFO\u001b[0m[0569] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[570.621288, "o", "\u001b[36mINFO\u001b[0m[0570] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[570.816767, "o", "\u001b[36mINFO\u001b[0m[0570] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.48.17] \r\n"]
[571.110064, "o", "\u001b[36mINFO\u001b[0m[0570] Starting container [kube-controller-manager] on host [23.102.48.17], try #1 \r\n"]
[571.937949, "o", "\u001b[36mINFO\u001b[0m[0571] [controlplane] Successfully started [kube-controller-manager] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0571] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [23.102.48.17] \r\n"]
[585.825003, "o", "\u001b[36mINFO\u001b[0m[0585] [healthcheck] service [kube-controller-manager] on host [104.45.84.172] is healthy \r\n"]
[586.122225, "o", "\u001b[36mINFO\u001b[0m[0585] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[587.044838, "o", "\u001b[36mINFO\u001b[0m[0586] [healthcheck] service [kube-controller-manager] on host [23.102.48.17] is healthy \r\n"]
[587.057311, "o", "\u001b[36mINFO\u001b[0m[0586] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[587.38392, "o", "\u001b[36mINFO\u001b[0m[0586] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[587.549994, "o", "\u001b[36mINFO\u001b[0m[0586] [healthcheck] service [kube-controller-manager] on host [104.45.86.4] is healthy \r\n"]
[587.872155, "o", "\u001b[36mINFO\u001b[0m[0587] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[588.323114, "o", "\u001b[36mINFO\u001b[0m[0587] [controlplane] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0587] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[588.936609, "o", "\u001b[36mINFO\u001b[0m[0588] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[588.96312, "o", "\u001b[36mINFO\u001b[0m[0588] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[589.097425, "o", "\u001b[36mINFO\u001b[0m[0588] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n"]
[589.212448, "o", "\u001b[36mINFO\u001b[0m[0588] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.84.172] \r\n"]
[589.550405, "o", "\u001b[36mINFO\u001b[0m[0588] Starting container [kube-scheduler] on host [104.45.84.172], try #1 \r\n"]
[589.63084, "o", "\u001b[36mINFO\u001b[0m[0589] [controlplane] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[590.165681, "o", "\u001b[36mINFO\u001b[0m[0589] [controlplane] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[590.273931, "o", "\u001b[36mINFO\u001b[0m[0589] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[590.303608, "o", "\u001b[36mINFO\u001b[0m[0589] [controlplane] Successfully started [kube-scheduler] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0589] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [104.45.84.172] \r\n"]
[590.433973, "o", "\u001b[36mINFO\u001b[0m[0589] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[590.629685, "o", "\u001b[36mINFO\u001b[0m[0590] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.48.17] \r\n"]
[590.741419, "o", "\u001b[36mINFO\u001b[0m[0590] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[590.908416, "o", "\u001b[36mINFO\u001b[0m[0590] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n"]
[590.923446, "o", "\u001b[36mINFO\u001b[0m[0590] Starting container [kube-scheduler] on host [23.102.48.17], try #1 \r\n"]
[591.098075, "o", "\u001b[36mINFO\u001b[0m[0590] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.86.4] \r\n"]
[591.495901, "o", "\u001b[36mINFO\u001b[0m[0590] Starting container [kube-scheduler] on host [104.45.86.4], try #1 \r\n"]
[591.804611, "o", "\u001b[36mINFO\u001b[0m[0591] [controlplane] Successfully started [kube-scheduler] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0591] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [23.102.48.17] \r\n"]
[592.28877, "o", "\u001b[36mINFO\u001b[0m[0591] [controlplane] Successfully started [kube-scheduler] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0591] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [104.45.86.4] \r\n"]
[598.94671, "o", "\u001b[36mINFO\u001b[0m[0598] [healthcheck] service [kube-scheduler] on host [104.45.84.172] is healthy \r\n"]
[599.244474, "o", "\u001b[36mINFO\u001b[0m[0598] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[599.805558, "o", "\u001b[36mINFO\u001b[0m[0599] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[599.991716, "o", "\u001b[36mINFO\u001b[0m[0599] [healthcheck] service [kube-scheduler] on host [23.102.48.17] is healthy \r\n"]
[600.317244, "o", "\u001b[36mINFO\u001b[0m[0599] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[600.346651, "o", "\u001b[36mINFO\u001b[0m[0599] [healthcheck] service [kube-scheduler] on host [104.45.86.4] is healthy \r\n"]
[600.692807, "o", "\u001b[36mINFO\u001b[0m[0600] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[601.123726, "o", "\u001b[36mINFO\u001b[0m[0600] [controlplane] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[601.220373, "o", "\u001b[36mINFO\u001b[0m[0600] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[601.242834, "o", "\u001b[36mINFO\u001b[0m[0600] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[601.530994, "o", "\u001b[36mINFO\u001b[0m[0600] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[601.838582, "o", "\u001b[36mINFO\u001b[0m[0601] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n"]
[602.455086, "o", "\u001b[36mINFO\u001b[0m[0601] [controlplane] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[602.740921, "o", "\u001b[36mINFO\u001b[0m[0602] [controlplane] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[603.16775, "o", "\u001b[36mINFO\u001b[0m[0602] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[603.374676, "o", "\u001b[36mINFO\u001b[0m[0602] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[603.383434, "o", "\u001b[36mINFO\u001b[0m[0602] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[603.562659, "o", "\u001b[36mINFO\u001b[0m[0602] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0602] [controlplane] Successfully started Controller Plane.. \r\n\u001b[36mINFO\u001b[0m[0602] [authz] Creating rke-job-deployer ServiceAccount \r\n"]
[603.864335, "o", "\u001b[36mINFO\u001b[0m[0603] [authz] rke-job-deployer ServiceAccount created successfully \r\n\u001b[36mINFO\u001b[0m[0603] [authz] Creating system:node ClusterRoleBinding \r\n"]
[604.073976, "o", "\u001b[36mINFO\u001b[0m[0603] [authz] system:node ClusterRoleBinding created successfully \r\n\u001b[36mINFO\u001b[0m[0603] [authz] Creating kube-apiserver proxy ClusterRole and ClusterRoleBinding \r\n"]
[604.250388, "o", "\u001b[36mINFO\u001b[0m[0603] [authz] kube-apiserver proxy ClusterRole and ClusterRoleBinding created successfully \r\n"]
[604.25192, "o", "\u001b[36mINFO\u001b[0m[0603] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/au1.rkestate] \r\n"]
[604.252547, "o", "\u001b[36mINFO\u001b[0m[0603] [state] Saving full cluster state to Kubernetes \r\n"]
[605.845311, "o", "\u001b[36mINFO\u001b[0m[0605] [state] Successfully Saved full cluster state to Kubernetes ConfigMap: full-cluster-state \r\n"]
[605.851848, "o", "\u001b[36mINFO\u001b[0m[0605] [worker] Building up Worker Plane..          \r\n"]
[605.851927, "o", "\u001b[36mINFO\u001b[0m[0605] Checking if container [service-sidekick] is running on host [104.45.84.172], try #1 \r\n"]
[605.852128, "o", "\u001b[36mINFO\u001b[0m[0605] Checking if container [service-sidekick] is running on host [104.45.86.4], try #1 \r\n\u001b[36mINFO\u001b[0m[0605] Checking if container [service-sidekick] is running on host [23.102.48.17], try #1 \r\n"]
[606.14933, "o", "\u001b[36mINFO\u001b[0m[0605] [sidekick] Sidekick container already created on host [104.45.84.172] \r\n"]
[606.201199, "o", "\u001b[36mINFO\u001b[0m[0605] [sidekick] Sidekick container already created on host [23.102.48.17] \r\n"]
[606.26624, "o", "\u001b[36mINFO\u001b[0m[0605] [sidekick] Sidekick container already created on host [104.45.86.4] \r\n"]
[606.333435, "o", "\u001b[36mINFO\u001b[0m[0605] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.84.172] \r\n"]
[606.4048, "o", "\u001b[36mINFO\u001b[0m[0605] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.48.17] \r\n"]
[606.466495, "o", "\u001b[36mINFO\u001b[0m[0605] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.86.4] \r\n"]
[606.603525, "o", "\u001b[36mINFO\u001b[0m[0606] Starting container [kubelet] on host [104.45.84.172], try #1 \r\n"]
[606.73509, "o", "\u001b[36mINFO\u001b[0m[0606] Starting container [kubelet] on host [23.102.48.17], try #1 \r\n"]
[606.797051, "o", "\u001b[36mINFO\u001b[0m[0606] Starting container [kubelet] on host [104.45.86.4], try #1 \r\n"]
[607.37022, "o", "\u001b[36mINFO\u001b[0m[0606] [worker] Successfully started [kubelet] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0606] [healthcheck] Start Healthcheck on service [kubelet] on host [104.45.84.172] \r\n"]
[607.479215, "o", "\u001b[36mINFO\u001b[0m[0606] [worker] Successfully started [kubelet] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0606] [healthcheck] Start Healthcheck on service [kubelet] on host [23.102.48.17] \r\n"]
[607.507229, "o", "\u001b[36mINFO\u001b[0m[0606] [worker] Successfully started [kubelet] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0606] [healthcheck] Start Healthcheck on service [kubelet] on host [104.45.86.4] \r\n"]
[608.405179, "o", "\u001b[36mINFO\u001b[0m[0607] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[608.685492, "o", "\u001b[36mINFO\u001b[0m[0608] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[609.074402, "o", "\u001b[36mINFO\u001b[0m[0608] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[612.080844, "o", "\u001b[36mINFO\u001b[0m[0611] Starting container [nginx-proxy] on host [23.102.22.29], try #1 \r\n\u001b[36mINFO\u001b[0m[0611] Starting container [nginx-proxy] on host [23.102.23.168], try #1 \r\n"]
[612.591553, "o", "\u001b[36mINFO\u001b[0m[0611] Starting container [nginx-proxy] on host [137.135.244.150], try #1 \r\n"]
[613.357818, "o", "\u001b[36mINFO\u001b[0m[0612] [worker] Successfully started [nginx-proxy] container on host [137.135.244.150] \r\n"]
[613.418733, "o", "\u001b[36mINFO\u001b[0m[0612] [worker] Successfully started [nginx-proxy] container on host [23.102.22.29] \r\n"]
[613.45686, "o", "\u001b[36mINFO\u001b[0m[0612] [worker] Successfully started [nginx-proxy] container on host [23.102.23.168] \r\n"]
[613.641101, "o", "\u001b[36mINFO\u001b[0m[0613] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[613.812649, "o", "\u001b[36mINFO\u001b[0m[0613] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[613.85911, "o", "\u001b[36mINFO\u001b[0m[0613] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[614.702685, "o", "\u001b[36mINFO\u001b[0m[0614] Starting container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[615.312663, "o", "\u001b[36mINFO\u001b[0m[0614] Starting container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[615.944939, "o", "\u001b[36mINFO\u001b[0m[0615] [healthcheck] service [kubelet] on host [104.45.84.172] is healthy \r\n"]
[616.317263, "o", "\u001b[36mINFO\u001b[0m[0615] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[616.534025, "o", "\u001b[36mINFO\u001b[0m[0615] [worker] Successfully started [rke-log-linker] container on host [23.102.22.29] \r\n"]
[616.666541, "o", "\u001b[36mINFO\u001b[0m[0616] Removing container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[616.891998, "o", "\u001b[36mINFO\u001b[0m[0616] Starting container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[617.356316, "o", "\u001b[36mINFO\u001b[0m[0616] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[617.81272, "o", "\u001b[36mINFO\u001b[0m[0617] [worker] Successfully started [rke-log-linker] container on host [23.102.23.168] \r\n"]
[617.946424, "o", "\u001b[36mINFO\u001b[0m[0617] Removing container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[618.044117, "o", "\u001b[36mINFO\u001b[0m[0617] [worker] Successfully started [rke-log-linker] container on host [137.135.244.150] \r\n"]
[618.18181, "o", "\u001b[36mINFO\u001b[0m[0617] Removing container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[618.445039, "o", "\u001b[36mINFO\u001b[0m[0617] [remove/rke-log-linker] Successfully removed container on host [23.102.22.29] \r\n\u001b[36mINFO\u001b[0m[0617] Checking if container [service-sidekick] is running on host [23.102.22.29], try #1 \r\n"]
[618.65081, "o", "\u001b[36mINFO\u001b[0m[0618] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[618.836896, "o", "\u001b[36mINFO\u001b[0m[0618] [worker] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[618.955847, "o", "\u001b[36mINFO\u001b[0m[0618] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[618.992123, "o", "\u001b[36mINFO\u001b[0m[0618] [remove/rke-log-linker] Successfully removed container on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0618] Checking if container [service-sidekick] is running on host [137.135.244.150], try #1 \r\n"]
[619.19517, "o", "\u001b[36mINFO\u001b[0m[0618] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[619.677644, "o", "\u001b[36mINFO\u001b[0m[0619] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n"]
[619.758201, "o", "\u001b[36mINFO\u001b[0m[0619] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.22.29] \r\n"]
[619.850584, "o", "\u001b[36mINFO\u001b[0m[0619] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.84.172] \r\n"]
[620.169868, "o", "\u001b[36mINFO\u001b[0m[0619] Starting container [kube-proxy] on host [104.45.84.172], try #1 \r\n"]
[620.170243, "o", "\u001b[36mINFO\u001b[0m[0619] Starting container [kubelet] on host [23.102.22.29], try #1 \r\n"]
[620.891252, "o", "\u001b[36mINFO\u001b[0m[0620] [remove/rke-log-linker] Successfully removed container on host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0620] Checking if container [service-sidekick] is running on host [23.102.23.168], try #1 \r\n"]
[620.938909, "o", "\u001b[36mINFO\u001b[0m[0620] [worker] Successfully started [kubelet] container on host [23.102.22.29] \r\n\u001b[36mINFO\u001b[0m[0620] [healthcheck] Start Healthcheck on service [kubelet] on host [23.102.22.29] \r\n"]
[621.002059, "o", "\u001b[36mINFO\u001b[0m[0620] [worker] Successfully started [kube-proxy] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0620] [healthcheck] Start Healthcheck on service [kube-proxy] on host [104.45.84.172] \r\n"]
[621.097219, "o", "\u001b[36mINFO\u001b[0m[0620] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[622.032678, "o", "\u001b[36mINFO\u001b[0m[0621] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.23.168] \r\n"]
[622.438715, "o", "\u001b[36mINFO\u001b[0m[0621] Starting container [kubelet] on host [23.102.23.168], try #1 \r\n"]
[622.441529, "o", "\u001b[36mINFO\u001b[0m[0621] [healthcheck] service [kubelet] on host [104.45.86.4] is healthy \r\n"]
[622.605797, "o", "\u001b[36mINFO\u001b[0m[0622] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [137.135.244.150] \r\n"]
[622.774018, "o", "\u001b[36mINFO\u001b[0m[0622] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[622.854111, "o", "\u001b[36mINFO\u001b[0m[0622] Starting container [kubelet] on host [137.135.244.150], try #1 \r\n"]
[622.879031, "o", "\u001b[36mINFO\u001b[0m[0622] [healthcheck] service [kube-proxy] on host [104.45.84.172] is healthy \r\n"]
[623.161695, "o", "\u001b[36mINFO\u001b[0m[0622] [healthcheck] service [kubelet] on host [23.102.48.17] is healthy \r\n"]
[623.174659, "o", "\u001b[36mINFO\u001b[0m[0622] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[623.309057, "o", "\u001b[36mINFO\u001b[0m[0622] [worker] Successfully started [kubelet] container on host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0622] [healthcheck] Start Healthcheck on service [kubelet] on host [23.102.23.168] \r\n"]
[623.410736, "o", "\u001b[36mINFO\u001b[0m[0622] [worker] Successfully started [kubelet] container on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0622] [healthcheck] Start Healthcheck on service [kubelet] on host [137.135.244.150] \r\n"]
[623.503146, "o", "\u001b[36mINFO\u001b[0m[0622] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[623.569496, "o", "\u001b[36mINFO\u001b[0m[0622] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[623.960664, "o", "\u001b[36mINFO\u001b[0m[0623] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[624.408875, "o", "\u001b[36mINFO\u001b[0m[0623] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[625.033964, "o", "\u001b[36mINFO\u001b[0m[0624] [worker] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[625.493559, "o", "\u001b[36mINFO\u001b[0m[0624] [worker] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[625.703959, "o", "\u001b[36mINFO\u001b[0m[0625] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[625.860587, "o", "\u001b[36mINFO\u001b[0m[0625] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n"]
[626.061594, "o", "\u001b[36mINFO\u001b[0m[0625] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [104.45.86.4] \r\n"]
[626.181437, "o", "\u001b[36mINFO\u001b[0m[0625] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[626.26819, "o", "\u001b[36mINFO\u001b[0m[0625] [worker] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[626.315556, "o", "\u001b[36mINFO\u001b[0m[0625] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n"]
[626.398161, "o", "\u001b[36mINFO\u001b[0m[0625] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[626.459482, "o", "\u001b[36mINFO\u001b[0m[0625] Starting container [kube-proxy] on host [104.45.86.4], try #1 \r\n"]
[627.029393, "o", "\u001b[36mINFO\u001b[0m[0626] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[627.223338, "o", "\u001b[36mINFO\u001b[0m[0626] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.48.17] \r\n"]
[627.255725, "o", "\u001b[36mINFO\u001b[0m[0626] [worker] Successfully started [kube-proxy] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0626] [healthcheck] Start Healthcheck on service [kube-proxy] on host [104.45.86.4] \r\n"]
[627.320192, "o", "\u001b[36mINFO\u001b[0m[0626] [healthcheck] service [kubelet] on host [137.135.244.150] is healthy \r\n"]
[627.611939, "o", "\u001b[36mINFO\u001b[0m[0627] Starting container [kube-proxy] on host [23.102.48.17], try #1 \r\n"]
[627.731804, "o", "\u001b[36mINFO\u001b[0m[0627] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[628.552516, "o", "\u001b[36mINFO\u001b[0m[0627] [worker] Successfully started [kube-proxy] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0627] [healthcheck] Start Healthcheck on service [kube-proxy] on host [23.102.48.17] \r\n"]
[629.564409, "o", "\u001b[36mINFO\u001b[0m[0628] [healthcheck] service [kubelet] on host [23.102.22.29] is healthy \r\n"]
[629.900091, "o", "\u001b[36mINFO\u001b[0m[0629] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[630.7182, "o", "\u001b[36mINFO\u001b[0m[0630] Starting container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[630.997644, "o", "\u001b[36mINFO\u001b[0m[0630] Starting container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[632.046779, "o", "\u001b[36mINFO\u001b[0m[0631] [worker] Successfully started [rke-log-linker] container on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0631] [worker] Successfully started [rke-log-linker] container on host [23.102.22.29] \r\n"]
[632.621623, "o", "\u001b[36mINFO\u001b[0m[0632] Removing container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[632.623654, "o", "\u001b[36mINFO\u001b[0m[0632] Removing container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[632.742166, "o", "\u001b[36mINFO\u001b[0m[0632] [remove/rke-log-linker] Successfully removed container on host [137.135.244.150] \r\n"]
[632.790376, "o", "\u001b[36mINFO\u001b[0m[0632] [remove/rke-log-linker] Successfully removed container on host [23.102.22.29] \r\n"]
[632.934528, "o", "\u001b[36mINFO\u001b[0m[0632] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [137.135.244.150] \r\n"]
[632.984338, "o", "\u001b[36mINFO\u001b[0m[0632] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.22.29] \r\n"]
[633.181789, "o", "\u001b[36mINFO\u001b[0m[0632] Starting container [kube-proxy] on host [137.135.244.150], try #1 \r\n"]
[633.374281, "o", "\u001b[36mINFO\u001b[0m[0632] Starting container [kube-proxy] on host [23.102.22.29], try #1 \r\n"]
[633.79943, "o", "\u001b[36mINFO\u001b[0m[0633] [worker] Successfully started [kube-proxy] container on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0633] [healthcheck] Start Healthcheck on service [kube-proxy] on host [137.135.244.150] \r\n"]
[634.140014, "o", "\u001b[36mINFO\u001b[0m[0633] [worker] Successfully started [kube-proxy] container on host [23.102.22.29] \r\n\u001b[36mINFO\u001b[0m[0633] [healthcheck] Start Healthcheck on service [kube-proxy] on host [23.102.22.29] \r\n"]
[635.451445, "o", "\u001b[36mINFO\u001b[0m[0634] [healthcheck] service [kubelet] on host [23.102.23.168] is healthy \r\n"]
[635.586472, "o", "\u001b[36mINFO\u001b[0m[0634] [healthcheck] service [kube-proxy] on host [137.135.244.150] is healthy \r\n"]
[635.606159, "o", "\u001b[36mINFO\u001b[0m[0635] [healthcheck] service [kube-proxy] on host [104.45.86.4] is healthy \r\n"]
[635.792165, "o", "\u001b[36mINFO\u001b[0m[0635] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[635.930736, "o", "\u001b[36mINFO\u001b[0m[0635] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0635] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[636.757169, "o", "\u001b[36mINFO\u001b[0m[0636] Starting container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[636.757329, "o", "\u001b[36mINFO\u001b[0m[0636] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[636.927104, "o", "\u001b[36mINFO\u001b[0m[0636] [healthcheck] service [kube-proxy] on host [23.102.48.17] is healthy \r\n"]
[637.28483, "o", "\u001b[36mINFO\u001b[0m[0636] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[638.070902, "o", "\u001b[36mINFO\u001b[0m[0637] [worker] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[638.200988, "o", "\u001b[36mINFO\u001b[0m[0637] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[638.269911, "o", "\u001b[36mINFO\u001b[0m[0637] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[638.602996, "o", "\u001b[36mINFO\u001b[0m[0638] [worker] Successfully started [rke-log-linker] container on host [23.102.23.168] \r\n"]
[638.859814, "o", "\u001b[36mINFO\u001b[0m[0638] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n"]
[639.321142, "o", "\u001b[36mINFO\u001b[0m[0638] Starting container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[639.336336, "o", "\u001b[36mINFO\u001b[0m[0638] Removing container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[639.518629, "o", "\u001b[36mINFO\u001b[0m[0638] [remove/rke-log-linker] Successfully removed container on host [23.102.23.168] \r\n"]
[639.673102, "o", "\u001b[36mINFO\u001b[0m[0639] [worker] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[639.716386, "o", "\u001b[36mINFO\u001b[0m[0639] Image [rancher/hyperkube:v1.18.12-rancher1] exists on host [23.102.23.168] \r\n"]
[639.809266, "o", "\u001b[36mINFO\u001b[0m[0639] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[640.053331, "o", "\u001b[36mINFO\u001b[0m[0639] Starting container [kube-proxy] on host [23.102.23.168], try #1 \r\n"]
[640.395477, "o", "\u001b[36mINFO\u001b[0m[0639] [worker] Successfully started [rke-log-linker] container on host [137.135.244.150] \r\n"]
[640.471127, "o", "\u001b[36mINFO\u001b[0m[0639] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[640.902846, "o", "\u001b[36mINFO\u001b[0m[0640] [worker] Successfully started [kube-proxy] container on host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0640] [healthcheck] Start Healthcheck on service [kube-proxy] on host [23.102.23.168] \r\n"]
[640.991354, "o", "\u001b[36mINFO\u001b[0m[0640] Removing container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[641.098137, "o", "\u001b[36mINFO\u001b[0m[0640] [remove/rke-log-linker] Successfully removed container on host [137.135.244.150] \r\n"]
[642.459047, "o", "\u001b[36mINFO\u001b[0m[0641] [healthcheck] service [kube-proxy] on host [23.102.22.29] is healthy \r\n"]
[642.799257, "o", "\u001b[36mINFO\u001b[0m[0642] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[643.720574, "o", "\u001b[36mINFO\u001b[0m[0643] Starting container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[645.063333, "o", "\u001b[36mINFO\u001b[0m[0644] [worker] Successfully started [rke-log-linker] container on host [23.102.22.29] \r\n"]
[645.831619, "o", "\u001b[36mINFO\u001b[0m[0645] Removing container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[646.009438, "o", "\u001b[36mINFO\u001b[0m[0645] [remove/rke-log-linker] Successfully removed container on host [23.102.22.29] \r\n"]
[649.214384, "o", "\u001b[36mINFO\u001b[0m[0648] [healthcheck] service [kube-proxy] on host [23.102.23.168] is healthy \r\n"]
[649.561311, "o", "\u001b[36mINFO\u001b[0m[0648] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[650.162364, "o", "\u001b[36mINFO\u001b[0m[0649] Starting container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[651.70828, "o", "\u001b[36mINFO\u001b[0m[0651] [worker] Successfully started [rke-log-linker] container on host [23.102.23.168] \r\n"]
[652.706149, "o", "\u001b[36mINFO\u001b[0m[0652] Removing container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[652.89351, "o", "\u001b[36mINFO\u001b[0m[0652] [remove/rke-log-linker] Successfully removed container on host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0652] [worker] Successfully started Worker Plane.. \r\n"]
[653.135565, "o", "\u001b[36mINFO\u001b[0m[0652] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[653.17063, "o", "\u001b[36mINFO\u001b[0m[0652] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[653.170813, "o", "\u001b[36mINFO\u001b[0m[0652] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[653.171762, "o", "\u001b[36mINFO\u001b[0m[0652] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[653.172065, "o", "\u001b[36mINFO\u001b[0m[0652] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0652] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[653.796724, "o", "\u001b[36mINFO\u001b[0m[0653] Starting container [rke-log-cleaner] on host [23.102.22.29], try #1 \r\n"]
[653.867838, "o", "\u001b[36mINFO\u001b[0m[0653] Starting container [rke-log-cleaner] on host [23.102.23.168], try #1 \r\n"]
[653.884899, "o", "\u001b[36mINFO\u001b[0m[0653] Starting container [rke-log-cleaner] on host [104.45.84.172], try #1 \r\n"]
[654.036686, "o", "\u001b[36mINFO\u001b[0m[0653] Starting container [rke-log-cleaner] on host [23.102.48.17], try #1 \r\n"]
[654.201365, "o", "\u001b[36mINFO\u001b[0m[0653] Starting container [rke-log-cleaner] on host [104.45.86.4], try #1 \r\n"]
[655.00643, "o", "\u001b[36mINFO\u001b[0m[0654] [cleanup] Successfully started [rke-log-cleaner] container on host [23.102.22.29] \r\n"]
[655.121034, "o", "\u001b[36mINFO\u001b[0m[0654] [cleanup] Successfully started [rke-log-cleaner] container on host [104.45.84.172] \r\n"]
[655.258173, "o", "\u001b[36mINFO\u001b[0m[0654] Removing container [rke-log-cleaner] on host [104.45.84.172], try #1 \r\n"]
[655.263665, "o", "\u001b[36mINFO\u001b[0m[0654] [cleanup] Successfully started [rke-log-cleaner] container on host [23.102.48.17] \r\n"]
[655.279964, "o", "\u001b[36mINFO\u001b[0m[0654] [cleanup] Successfully started [rke-log-cleaner] container on host [23.102.23.168] \r\n"]
[655.366151, "o", "\u001b[36mINFO\u001b[0m[0654] [cleanup] Successfully started [rke-log-cleaner] container on host [104.45.86.4] \r\n"]
[655.40128, "o", "\u001b[36mINFO\u001b[0m[0654] Removing container [rke-log-cleaner] on host [23.102.48.17], try #1 \r\n"]
[655.41259, "o", "\u001b[36mINFO\u001b[0m[0654] Removing container [rke-log-cleaner] on host [23.102.23.168], try #1 \r\n"]
[655.497819, "o", "\u001b[36mINFO\u001b[0m[0654] Removing container [rke-log-cleaner] on host [104.45.86.4], try #1 \r\n"]
[655.611528, "o", "\u001b[36mINFO\u001b[0m[0655] Removing container [rke-log-cleaner] on host [23.102.22.29], try #1 \r\n"]
[655.778224, "o", "\u001b[36mINFO\u001b[0m[0655] [remove/rke-log-cleaner] Successfully removed container on host [23.102.22.29] \r\n"]
[655.925908, "o", "\u001b[36mINFO\u001b[0m[0655] [remove/rke-log-cleaner] Successfully removed container on host [104.45.84.172] \r\n"]
[656.079834, "o", "\u001b[36mINFO\u001b[0m[0655] [remove/rke-log-cleaner] Successfully removed container on host [23.102.23.168] \r\n"]
[656.155508, "o", "\u001b[36mINFO\u001b[0m[0655] [remove/rke-log-cleaner] Successfully removed container on host [23.102.48.17] \r\n"]
[656.205144, "o", "\u001b[36mINFO\u001b[0m[0655] [remove/rke-log-cleaner] Successfully removed container on host [104.45.86.4] \r\n"]
[656.42493, "o", "\u001b[36mINFO\u001b[0m[0655] Starting container [rke-log-cleaner] on host [137.135.244.150], try #1 \r\n"]
[657.449804, "o", "\u001b[36mINFO\u001b[0m[0656] [cleanup] Successfully started [rke-log-cleaner] container on host [137.135.244.150] \r\n"]
[657.59303, "o", "\u001b[36mINFO\u001b[0m[0656] Removing container [rke-log-cleaner] on host [137.135.244.150], try #1 \r\n"]
[658.160207, "o", "\u001b[36mINFO\u001b[0m[0657] [remove/rke-log-cleaner] Successfully removed container on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0657] [sync] Syncing nodes Labels and Taints       \r\n"]
[658.659457, "o", "\u001b[36mINFO\u001b[0m[0658] [sync] Successfully synced nodes Labels and Taints \r\n"]
[658.659761, "o", "\u001b[36mINFO\u001b[0m[0658] [network] Setting up network plugin: calico  \r\n"]
[658.664489, "o", "\u001b[36mINFO\u001b[0m[0658] [addons] Saving ConfigMap for addon rke-network-plugin to Kubernetes \r\n"]
[658.973429, "o", "\u001b[36mINFO\u001b[0m[0658] [addons] Successfully saved ConfigMap for addon rke-network-plugin to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0658] [addons] Executing deploy job rke-network-plugin \r\n"]
[680.019273, "o", "\u001b[36mINFO\u001b[0m[0679] [addons] Setting up coredns                  \r\n"]
[680.020933, "o", "\u001b[36mINFO\u001b[0m[0679] [addons] Saving ConfigMap for addon rke-coredns-addon to Kubernetes \r\n"]
[680.230172, "o", "\u001b[36mINFO\u001b[0m[0679] [addons] Successfully saved ConfigMap for addon rke-coredns-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0679] [addons] Executing deploy job rke-coredns-addon \r\n"]
[701.729613, "o", "\u001b[36mINFO\u001b[0m[0701] [addons] CoreDNS deployed successfully       \r\n\u001b[36mINFO\u001b[0m[0701] [dns] DNS provider coredns deployed successfully \r\n"]
[701.800012, "o", "\u001b[36mINFO\u001b[0m[0701] [addons] Setting up Metrics Server           \r\n"]
[701.800781, "o", "\u001b[36mINFO\u001b[0m[0701] [addons] Saving ConfigMap for addon rke-metrics-addon to Kubernetes \r\n"]
[702.005438, "o", "\u001b[36mINFO\u001b[0m[0701] [addons] Successfully saved ConfigMap for addon rke-metrics-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0701] [addons] Executing deploy job rke-metrics-addon \r\n"]
[718.881919, "o", "\u001b[36mINFO\u001b[0m[0718] [addons] Metrics Server deployed successfully \r\n\u001b[36mINFO\u001b[0m[0718] [ingress] Setting up nginx ingress controller \r\n"]
[718.883181, "o", "\u001b[36mINFO\u001b[0m[0718] [addons] Saving ConfigMap for addon rke-ingress-controller to Kubernetes \r\n"]
[719.320885, "o", "\u001b[36mINFO\u001b[0m[0718] [addons] Successfully saved ConfigMap for addon rke-ingress-controller to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0718] [addons] Executing deploy job rke-ingress-controller \r\n"]
[742.737156, "o", "\u001b[36mINFO\u001b[0m[0742] [ingress] ingress controller nginx deployed successfully \r\n\u001b[36mINFO\u001b[0m[0742] [addons] Setting up user addons              \r\n"]
[752.983653, "o", "\u001b[36mINFO\u001b[0m[0752] [addons] no user addons defined              \r\n\u001b[36mINFO\u001b[0m[0752] Finished building Kubernetes cluster successfully \r\n"]
[752.986201, "o", "\r\nreal\t12m32.977s\r\nuser\t0m5.006s\r\nsys\t0m1.546s\r\n"]
[784.210492, "o", "NAME                   STATUS   ROLES               AGE     VERSION\r\nnode/104.45.84.172     Ready    controlplane,etcd   2m49s"]
[784.210548, "o", "   v1.18.12\r\nnode/104.45.86.4       Ready    controlplane,etcd   2m44s   v1.18.12\r\nnode/137.135.244.150   Ready    worker              2m36s   v1.18.12\r\nnode/23.102.22.29      Ready    worker              2m38s   v1.18.12\r\nnode/23.102.23.168"]
[784.210568, "o", "     Ready    worker              2m35s   v1.18.12\r\nnode/23.102.48.17"]
[784.210711, "o", "      Ready    controlplane,etcd   2m46s   v1.18.12\r\n"]
[784.220674, "o", "\r\nNAMESPACE       NAME                                           READY   STATUS              RESTARTS   AGE\r\ningress-nginx   pod/default-http-backend-598b7d7dbd-zhmc8      0/1     ContainerCreating   0          48s\r\ningress-nginx   pod/nginx-ingress-controller-2fh76             0/1     ContainerCreating   0          48s\r\ningress-nginx   pod/nginx-ingress-controller-4jrjv             0/1     ContainerCreating   0          48s\r\ningress-nginx   "]
[784.220817, "o", "pod/nginx-ingress-controller-p9kd5             0/1     ContainerCreating   0          48s\r\nkube-system     pod/calico-kube-controllers-7fbc6b86cc-8cc2z   0/1     ContainerCreating   0          107s\r\nkube-system     pod/calico-node-2mgph                          1/1     Running             0          106s\r\nkube-system     pod/calico-node-7n2r7                          1/1     Running             0          106s\r\nkube-system     pod/calico-node-dttns                          1/1     Running             0          106s\r\nkube-system     pod/calico-node-lnbbc                          1/1     Running             1          106s\r\nkube-system     pod/calico-node-nvcxn                          "]
[784.22092, "o", "1/1     Running             0          106s\r\nkube-system     pod/calico-node-wb5z7                          0/1     PodInitializing     0          106s\r\nkube-system     pod/coredns-849545576b-xw5qt                   0/1     ContainerCreating   0          88s\r\nkube-system     pod/coredns-autoscaler-5dcd676cbd-kbqh9        0/1     ContainerCreating   0          85s\r\nkube-system     "]
[784.220996, "o", "pod/metrics-server-697746ff48-jqzqt            0/1     ContainerCreating   0          69s\r\nkube-system     pod/rke-coredns-addon-deploy-job-r9w55         0/1     Completed           0          103s\r\nkube-system     pod/rke-ingress-controller-deploy-job-wsf8d    0/1     Completed           0          63s\r\nkube-system     pod/rke-metrics-addon-deploy-job-7xbdd         0/1     Completed           0          "]
[784.221151, "o", "81s\r\nkube-system     pod/rke-network-plugin-deploy-job-wb2s4        0/1     Completed           0          2m4s\r\n"]
[784.401572, "o", "cd /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ && tee /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/au1.yml </home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/au1-v1.19.4-rancher1-1.yml \\\r\n            && ./files/rke up --config=/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/au1.yml\r\n"]
[784.491026, "o", "---\r\ncluster_name: au1\r\nkubernetes_version: v1.19.4-rancher1-1\r\n\r\nssh_agent_auth: true\r\n\r\nnodes:\r\n  - &m1\r\n    user: ubuntu\r\n    role: [etcd, controlplane]\r\n    \"address\": \"104.45.86.4\"\r\n    \"internal_address\": \"10.0.240.8\"\r\n  - <<: *m1\r\n    \"address\": \"23.102.48.17\"\r\n    \"internal_address\": \"10.0.240.6\"\r\n  - <<: *m1\r\n    \"address\": \"104.45.84.172\"\r\n    \"internal_address\": \"10.0.240.4\"\r\n\r\n  - &n1\r\n    user: ubuntu\r\n    role: [worker]\r\n    \"address\": \"23.102.22.29\"\r\n    \"internal_address\": \"10.0.240.7\"\r\n  - <<: *n1\r\n    \"address\": \"137.135.244.150\"\r\n    \"internal_address\": \"10.0.240.5\"\r\n  - <<: *n1\r\n    \"address\": \"23.102.23.168\"\r\n    \"internal_address\": \"10.0.240.9\"\r\n\r\nnetwork:\r\n  plugin: calico\r\n"]
[784.511148, "o", "\u001b[36mINFO\u001b[0m[0000] Running RKE version: v1.2.3                  \r\n"]
[784.511977, "o", "\u001b[36mINFO\u001b[0m[0000] Initiating Kubernetes cluster                \r\n"]
[784.535459, "o", "\u001b[36mINFO\u001b[0m[0000] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates \r\n\u001b[36mINFO\u001b[0m[0000] [certificates] Generating admin certificates and kubeconfig \r\n"]
[784.538427, "o", "\u001b[36mINFO\u001b[0m[0000] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/au1.rkestate] \r\n"]
[784.541314, "o", "\u001b[36mINFO\u001b[0m[0000] Building Kubernetes cluster                  \r\n"]
[784.541338, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [23.102.23.168] \r\n"]
[784.541445, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [104.45.84.172] \r\n"]
[784.541462, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [137.135.244.150] \r\n"]
[784.541554, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [23.102.22.29] \r\n"]
[784.541614, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [23.102.48.17] \r\n"]
[784.541686, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [104.45.86.4] \r\n"]
[790.553764, "o", "\u001b[36mINFO\u001b[0m[0006] [network] No hosts added existing cluster, skipping port check \r\n"]
[790.553916, "o", "\u001b[36mINFO\u001b[0m[0006] [certificates] Deploying kubernetes certificates to Cluster nodes \r\n"]
[790.554, "o", "\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [23.102.22.29], try #1 \r\n\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [23.102.23.168], try #1 \r\n\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [137.135.244.150], try #1 \r\n"]
[790.554285, "o", "\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [104.45.86.4], try #1 \r\n"]
[790.554371, "o", "\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [23.102.48.17], try #1 \r\n"]
[790.554483, "o", "\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [104.45.84.172], try #1 \r\n"]
[790.913562, "o", "\u001b[36mINFO\u001b[0m[0006] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[790.914719, "o", "\u001b[36mINFO\u001b[0m[0006] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[790.922598, "o", "\u001b[36mINFO\u001b[0m[0006] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[790.926091, "o", "\u001b[36mINFO\u001b[0m[0006] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[790.930171, "o", "\u001b[36mINFO\u001b[0m[0006] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[790.97206, "o", "\u001b[36mINFO\u001b[0m[0006] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[793.226268, "o", "\u001b[36mINFO\u001b[0m[0008] Starting container [cert-deployer] on host [104.45.84.172], try #1 \r\n"]
[793.245696, "o", "\u001b[36mINFO\u001b[0m[0008] Starting container [cert-deployer] on host [23.102.48.17], try #1 \r\n"]
[793.485272, "o", "\u001b[36mINFO\u001b[0m[0008] Starting container [cert-deployer] on host [104.45.86.4], try #1 \r\n"]
[793.485589, "o", "\u001b[36mINFO\u001b[0m[0008] Starting container [cert-deployer] on host [23.102.22.29], try #1 \r\n"]
[794.86148, "o", "\u001b[36mINFO\u001b[0m[0010] Checking if container [cert-deployer] is running on host [104.45.84.172], try #1 \r\n"]
[794.906938, "o", "\u001b[36mINFO\u001b[0m[0010] Checking if container [cert-deployer] is running on host [23.102.48.17], try #1 \r\n"]
[795.089728, "o", "\u001b[36mINFO\u001b[0m[0010] Checking if container [cert-deployer] is running on host [104.45.86.4], try #1 \r\n"]
[795.153134, "o", "\u001b[36mINFO\u001b[0m[0010] Starting container [cert-deployer] on host [23.102.23.168], try #1 \r\n"]
[796.297371, "o", "\u001b[36mINFO\u001b[0m[0011] Starting container [cert-deployer] on host [137.135.244.150], try #1 \r\n"]
[797.32201, "o", "\u001b[36mINFO\u001b[0m[0012] Checking if container [cert-deployer] is running on host [23.102.22.29], try #1 \r\n"]
[798.348454, "o", "\u001b[36mINFO\u001b[0m[0013] Checking if container [cert-deployer] is running on host [23.102.23.168], try #1 \r\n"]
[799.984268, "o", "\u001b[36mINFO\u001b[0m[0015] Checking if container [cert-deployer] is running on host [137.135.244.150], try #1 \r\n"]
[800.041622, "o", "\u001b[36mINFO\u001b[0m[0015] Checking if container [cert-deployer] is running on host [104.45.84.172], try #1 \r\n"]
[800.129216, "o", "\u001b[36mINFO\u001b[0m[0015] Checking if container [cert-deployer] is running on host [23.102.48.17], try #1 \r\n"]
[800.181854, "o", "\u001b[36mINFO\u001b[0m[0015] Removing container [cert-deployer] on host [104.45.84.172], try #1 \r\n"]
[800.276824, "o", "\u001b[36mINFO\u001b[0m[0015] Removing container [cert-deployer] on host [23.102.48.17], try #1 \r\n"]
[800.301334, "o", "\u001b[36mINFO\u001b[0m[0015] Checking if container [cert-deployer] is running on host [104.45.86.4], try #1 \r\n"]
[800.436906, "o", "\u001b[36mINFO\u001b[0m[0015] Removing container [cert-deployer] on host [104.45.86.4], try #1 \r\n"]
[803.004444, "o", "\u001b[36mINFO\u001b[0m[0018] Checking if container [cert-deployer] is running on host [23.102.22.29], try #1 \r\n"]
[803.154551, "o", "\u001b[36mINFO\u001b[0m[0018] Removing container [cert-deployer] on host [23.102.22.29], try #1 \r\n"]
[803.546642, "o", "\u001b[36mINFO\u001b[0m[0019] Checking if container [cert-deployer] is running on host [23.102.23.168], try #1 \r\n"]
[803.692406, "o", "\u001b[36mINFO\u001b[0m[0019] Removing container [cert-deployer] on host [23.102.23.168], try #1 \r\n"]
[805.148096, "o", "\u001b[36mINFO\u001b[0m[0020] Checking if container [cert-deployer] is running on host [137.135.244.150], try #1 \r\n"]
[810.285454, "o", "\u001b[36mINFO\u001b[0m[0025] Checking if container [cert-deployer] is running on host [137.135.244.150], try #1 \r\n"]
[810.437322, "o", "\u001b[36mINFO\u001b[0m[0025] Removing container [cert-deployer] on host [137.135.244.150], try #1 \r\n"]
[811.556829, "o", "\u001b[36mINFO\u001b[0m[0027] [reconcile] Rebuilding and updating local kube config \r\n"]
[811.557227, "o", "\u001b[36mINFO\u001b[0m[0027] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_au1.yml] \r\n"]
[811.7639, "o", "\u001b[36mINFO\u001b[0m[0027] [reconcile] host [104.45.86.4] is active master on the cluster \r\n\u001b[36mINFO\u001b[0m[0027] [certificates] Successfully deployed kubernetes certificates to Cluster nodes \r\n"]
[811.765791, "o", "\u001b[36mINFO\u001b[0m[0027] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [104.45.86.4] \r\n"]
[812.030786, "o", "\u001b[36mINFO\u001b[0m[0027] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[812.988597, "o", "\u001b[36mINFO\u001b[0m[0028] Starting container [file-deployer] on host [104.45.86.4], try #1 \r\n"]
[814.731669, "o", "\u001b[36mINFO\u001b[0m[0030] Successfully started [file-deployer] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0030] Waiting for [file-deployer] container to exit on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0030] Waiting for [file-deployer] container to exit on host [104.45.86.4] \r\n"]
[815.635246, "o", "\u001b[36mINFO\u001b[0m[0031] Removing container [file-deployer] on host [104.45.86.4], try #1 \r\n"]
[815.856493, "o", "\u001b[36mINFO\u001b[0m[0031] [remove/file-deployer] Successfully removed container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0031] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [23.102.48.17] \r\n"]
[816.12202, "o", "\u001b[36mINFO\u001b[0m[0031] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[816.879968, "o", "\u001b[36mINFO\u001b[0m[0032] Starting container [file-deployer] on host [23.102.48.17], try #1 \r\n"]
[818.335421, "o", "\u001b[36mINFO\u001b[0m[0033] Successfully started [file-deployer] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0033] Waiting for [file-deployer] container to exit on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0033] Waiting for [file-deployer] container to exit on host [23.102.48.17] \r\n"]
[819.089326, "o", "\u001b[36mINFO\u001b[0m[0034] Container [file-deployer] is still running on host [23.102.48.17]: stderr: [], stdout: [] \r\n"]
[820.089573, "o", "\u001b[36mINFO\u001b[0m[0035] Waiting for [file-deployer] container to exit on host [23.102.48.17] \r\n"]
[820.418727, "o", "\u001b[36mINFO\u001b[0m[0035] Removing container [file-deployer] on host [23.102.48.17], try #1 \r\n"]
[820.596986, "o", "\u001b[36mINFO\u001b[0m[0036] [remove/file-deployer] Successfully removed container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0036] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [104.45.84.172] \r\n"]
[820.861003, "o", "\u001b[36mINFO\u001b[0m[0036] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[821.796041, "o", "\u001b[36mINFO\u001b[0m[0037] Starting container [file-deployer] on host [104.45.84.172], try #1 \r\n"]
[823.331966, "o", "\u001b[36mINFO\u001b[0m[0038] Successfully started [file-deployer] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0038] Waiting for [file-deployer] container to exit on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0038] Waiting for [file-deployer] container to exit on host [104.45.84.172] \r\n"]
[824.254164, "o", "\u001b[36mINFO\u001b[0m[0039] Container [file-deployer] is still running on host [104.45.84.172]: stderr: [], stdout: [] \r\n"]
[825.254363, "o", "\u001b[36mINFO\u001b[0m[0040] Waiting for [file-deployer] container to exit on host [104.45.84.172] \r\n"]
[825.390368, "o", "\u001b[36mINFO\u001b[0m[0040] Removing container [file-deployer] on host [104.45.84.172], try #1 \r\n"]
[825.558308, "o", "\u001b[36mINFO\u001b[0m[0041] [remove/file-deployer] Successfully removed container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0041] [/etc/kubernetes/audit-policy.yaml] Successfully deployed audit policy file to Cluster control nodes \r\n\u001b[36mINFO\u001b[0m[0041] [reconcile] Reconciling cluster state        \r\n"]
[825.561258, "o", "\u001b[36mINFO\u001b[0m[0041] [reconcile] Check etcd hosts to be deleted   \r\n"]
[825.563249, "o", "\u001b[36mINFO\u001b[0m[0041] [reconcile] Check etcd hosts to be added     \r\n"]
[825.565142, "o", "\u001b[36mINFO\u001b[0m[0041] [reconcile] Rebuilding and updating local kube config \r\n"]
[825.565723, "o", "\u001b[36mINFO\u001b[0m[0041] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_au1.yml] \r\n"]
[825.623318, "o", "\u001b[36mINFO\u001b[0m[0041] [reconcile] host [104.45.86.4] is active master on the cluster \r\n\u001b[36mINFO\u001b[0m[0041] [reconcile] Reconciled cluster state successfully \r\n\u001b[36mINFO\u001b[0m[0041] max_unavailable_worker got rounded down to 0, resetting to 1 \r\n\u001b[36mINFO\u001b[0m[0041] Setting maxUnavailable for worker nodes to: 1 \r\n\u001b[36mINFO\u001b[0m[0041] Setting maxUnavailable for controlplane nodes to: 1 \r\n\u001b[36mINFO\u001b[0m[0041] Pre-pulling kubernetes images                \r\n"]
[825.692856, "o", "\u001b[36mINFO\u001b[0m[0041] Pulling image [rancher/hyperkube:v1.19.4-rancher1] on host [104.45.86.4], try #1 \r\n"]
[825.693071, "o", "\u001b[36mINFO\u001b[0m[0041] Pulling image [rancher/hyperkube:v1.19.4-rancher1] on host [23.102.23.168], try #1 \r\n"]
[825.694231, "o", "\u001b[36mINFO\u001b[0m[0041] Pulling image [rancher/hyperkube:v1.19.4-rancher1] on host [104.45.84.172], try #1 \r\n\u001b[36mINFO\u001b[0m[0041] Pulling image [rancher/hyperkube:v1.19.4-rancher1] on host [23.102.48.17], try #1 \r\n"]
[825.694461, "o", "\u001b[36mINFO\u001b[0m[0041] Pulling image [rancher/hyperkube:v1.19.4-rancher1] on host [137.135.244.150], try #1 \r\n"]
[825.707029, "o", "\u001b[36mINFO\u001b[0m[0041] Pulling image [rancher/hyperkube:v1.19.4-rancher1] on host [23.102.22.29], try #1 \r\n"]
[1003.634315, "o", "\u001b[36mINFO\u001b[0m[0219] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.84.172] \r\n"]
[1032.471208, "o", "\u001b[36mINFO\u001b[0m[0247] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.22.29] \r\n"]
[1038.95247, "o", "\u001b[36mINFO\u001b[0m[0254] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.48.17] \r\n"]
[1057.52827, "o", "\u001b[36mINFO\u001b[0m[0273] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.23.168] \r\n"]
[1070.041511, "o", "\u001b[36mINFO\u001b[0m[0285] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.86.4] \r\n"]
[1124.424067, "o", "\u001b[36mINFO\u001b[0m[0339] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0339] Kubernetes images pulled successfully        \r\n"]
[1124.429388, "o", "\u001b[36mINFO\u001b[0m[0339] [etcd] Building up etcd plane..              \r\n"]
[1126.340433, "o", "\u001b[36mINFO\u001b[0m[0341] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[1127.420809, "o", "\u001b[36mINFO\u001b[0m[0342] Starting container [etcd-fix-perm] on host [104.45.86.4], try #1 \r\n"]
[1129.411892, "o", "\u001b[36mINFO\u001b[0m[0344] Successfully started [etcd-fix-perm] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0344] Waiting for [etcd-fix-perm] container to exit on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0344] Waiting for [etcd-fix-perm] container to exit on host [104.45.86.4] \r\n"]
[1130.294068, "o", "\u001b[36mINFO\u001b[0m[0345] Removing container [etcd-fix-perm] on host [104.45.86.4], try #1 \r\n"]
[1130.476042, "o", "\u001b[36mINFO\u001b[0m[0345] [remove/etcd-fix-perm] Successfully removed container on host [104.45.86.4] \r\n"]
[1130.74437, "o", "\u001b[36mINFO\u001b[0m[0346] Checking if container [etcd] is running on host [104.45.86.4], try #1 \r\n"]
[1130.945916, "o", "\u001b[36mINFO\u001b[0m[0346] Pulling image [rancher/coreos-etcd:v3.4.13-rancher1] on host [104.45.86.4], try #1 \r\n"]
[1138.618695, "o", "\u001b[36mINFO\u001b[0m[0354] Image [rancher/coreos-etcd:v3.4.13-rancher1] exists on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0354] Checking if container [old-etcd] is running on host [104.45.86.4], try #1 \r\n"]
[1138.834218, "o", "\u001b[36mINFO\u001b[0m[0354] Stopping container [etcd] on host [104.45.86.4] with stopTimeoutDuration [5s], try #1 \r\n"]
[1142.723343, "o", "\u001b[36mINFO\u001b[0m[0358] Waiting for [etcd] container to exit on host [104.45.86.4] \r\n"]
[1142.797958, "o", "\u001b[36mINFO\u001b[0m[0358] Renaming container [etcd] to [old-etcd] on host [104.45.86.4], try #1 \r\n"]
[1143.334175, "o", "\u001b[36mINFO\u001b[0m[0358] Starting container [etcd] on host [104.45.86.4], try #1 \r\n"]
[1144.168947, "o", "\u001b[36mINFO\u001b[0m[0359] [etcd] Successfully updated [etcd] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0359] Removing container [old-etcd] on host [104.45.86.4], try #1 \r\n"]
[1144.3278, "o", "\u001b[36mINFO\u001b[0m[0359] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [104.45.86.4] \r\n"]
[1144.461145, "o", "\u001b[36mINFO\u001b[0m[0359] Removing container [etcd-rolling-snapshots] on host [104.45.86.4], try #1 \r\n"]
[1145.893591, "o", "\u001b[36mINFO\u001b[0m[0361] [remove/etcd-rolling-snapshots] Successfully removed container on host [104.45.86.4] \r\n"]
[1146.096299, "o", "\u001b[36mINFO\u001b[0m[0361] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[1147.277912, "o", "\u001b[36mINFO\u001b[0m[0362] Starting container [etcd-rolling-snapshots] on host [104.45.86.4], try #1 \r\n"]
[1148.760902, "o", "\u001b[36mINFO\u001b[0m[0364] [etcd] Successfully started [etcd-rolling-snapshots] container on host [104.45.86.4] \r\n"]
[1154.016164, "o", "\u001b[36mINFO\u001b[0m[0369] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[1154.9061, "o", "\u001b[36mINFO\u001b[0m[0370] Starting container [rke-bundle-cert] on host [104.45.86.4], try #1 \r\n"]
[1156.645776, "o", "\u001b[36mINFO\u001b[0m[0372] [certificates] Successfully started [rke-bundle-cert] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0372] Waiting for [rke-bundle-cert] container to exit on host [104.45.86.4] \r\n"]
[1157.428949, "o", "\u001b[36mINFO\u001b[0m[0372] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0372] Removing container [rke-bundle-cert] on host [104.45.86.4], try #1 \r\n"]
[1158.024729, "o", "\u001b[36mINFO\u001b[0m[0373] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[1158.797521, "o", "\u001b[36mINFO\u001b[0m[0374] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1160.376933, "o", "\u001b[36mINFO\u001b[0m[0375] [etcd] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[1160.541193, "o", "\u001b[36mINFO\u001b[0m[0376] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1161.458924, "o", "\u001b[36mINFO\u001b[0m[0376] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n"]
[1161.777558, "o", "\u001b[36mINFO\u001b[0m[0377] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[1162.532155, "o", "\u001b[36mINFO\u001b[0m[0378] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1164.018669, "o", "\u001b[36mINFO\u001b[0m[0379] [etcd] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[1164.173099, "o", "\u001b[36mINFO\u001b[0m[0379] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1165.146726, "o", "\u001b[36mINFO\u001b[0m[0380] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n"]
[1166.952154, "o", "\u001b[36mINFO\u001b[0m[0382] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[1168.114929, "o", "\u001b[36mINFO\u001b[0m[0383] Starting container [etcd-fix-perm] on host [23.102.48.17], try #1 \r\n"]
[1169.957667, "o", "\u001b[36mINFO\u001b[0m[0385] Successfully started [etcd-fix-perm] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0385] Waiting for [etcd-fix-perm] container to exit on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0385] Waiting for [etcd-fix-perm] container to exit on host [23.102.48.17] \r\n"]
[1170.852657, "o", "\u001b[36mINFO\u001b[0m[0386] Removing container [etcd-fix-perm] on host [23.102.48.17], try #1 \r\n"]
[1171.084999, "o", "\u001b[36mINFO\u001b[0m[0386] [remove/etcd-fix-perm] Successfully removed container on host [23.102.48.17] \r\n"]
[1171.320743, "o", "\u001b[36mINFO\u001b[0m[0386] Checking if container [etcd] is running on host [23.102.48.17], try #1 \r\n"]
[1171.501495, "o", "\u001b[36mINFO\u001b[0m[0387] Pulling image [rancher/coreos-etcd:v3.4.13-rancher1] on host [23.102.48.17], try #1 \r\n"]
[1178.468315, "o", "\u001b[36mINFO\u001b[0m[0393] Image [rancher/coreos-etcd:v3.4.13-rancher1] exists on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0393] Checking if container [old-etcd] is running on host [23.102.48.17], try #1 \r\n"]
[1178.60367, "o", "\u001b[36mINFO\u001b[0m[0394] Stopping container [etcd] on host [23.102.48.17] with stopTimeoutDuration [5s], try #1 \r\n"]
[1181.697601, "o", "\u001b[36mINFO\u001b[0m[0397] Waiting for [etcd] container to exit on host [23.102.48.17] \r\n"]
[1181.81723, "o", "\u001b[36mINFO\u001b[0m[0397] Renaming container [etcd] to [old-etcd] on host [23.102.48.17], try #1 \r\n"]
[1182.349489, "o", "\u001b[36mINFO\u001b[0m[0397] Starting container [etcd] on host [23.102.48.17], try #1 \r\n"]
[1183.255166, "o", "\u001b[36mINFO\u001b[0m[0398] [etcd] Successfully updated [etcd] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0398] Removing container [old-etcd] on host [23.102.48.17], try #1 \r\n"]
[1183.390114, "o", "\u001b[36mINFO\u001b[0m[0398] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [23.102.48.17] \r\n"]
[1183.513735, "o", "\u001b[36mINFO\u001b[0m[0399] Removing container [etcd-rolling-snapshots] on host [23.102.48.17], try #1 \r\n"]
[1184.601948, "o", "\u001b[36mINFO\u001b[0m[0400] [remove/etcd-rolling-snapshots] Successfully removed container on host [23.102.48.17] \r\n"]
[1184.77789, "o", "\u001b[36mINFO\u001b[0m[0400] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[1185.892303, "o", "\u001b[36mINFO\u001b[0m[0401] Starting container [etcd-rolling-snapshots] on host [23.102.48.17], try #1 \r\n"]
[1187.161248, "o", "\u001b[36mINFO\u001b[0m[0402] [etcd] Successfully started [etcd-rolling-snapshots] container on host [23.102.48.17] \r\n"]
[1192.40144, "o", "\u001b[36mINFO\u001b[0m[0407] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[1193.306287, "o", "\u001b[36mINFO\u001b[0m[0408] Starting container [rke-bundle-cert] on host [23.102.48.17], try #1 \r\n"]
[1194.948537, "o", "\u001b[36mINFO\u001b[0m[0410] [certificates] Successfully started [rke-bundle-cert] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0410] Waiting for [rke-bundle-cert] container to exit on host [23.102.48.17] \r\n"]
[1195.923788, "o", "\u001b[36mINFO\u001b[0m[0411] Container [rke-bundle-cert] is still running on host [23.102.48.17]: stderr: [], stdout: [] \r\n"]
[1196.923972, "o", "\u001b[36mINFO\u001b[0m[0412] Waiting for [rke-bundle-cert] container to exit on host [23.102.48.17] \r\n"]
[1196.981087, "o", "\u001b[36mINFO\u001b[0m[0412] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0412] Removing container [rke-bundle-cert] on host [23.102.48.17], try #1 \r\n"]
[1197.526664, "o", "\u001b[36mINFO\u001b[0m[0413] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[1198.425423, "o", "\u001b[36mINFO\u001b[0m[0413] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1199.858878, "o", "\u001b[36mINFO\u001b[0m[0415] [etcd] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[1200.635562, "o", "\u001b[36mINFO\u001b[0m[0416] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1200.885103, "o", "\u001b[36mINFO\u001b[0m[0416] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[1201.175097, "o", "\u001b[36mINFO\u001b[0m[0416] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[1202.112312, "o", "\u001b[36mINFO\u001b[0m[0417] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1203.895519, "o", "\u001b[36mINFO\u001b[0m[0419] [etcd] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[1204.624565, "o", "\u001b[36mINFO\u001b[0m[0420] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1204.755247, "o", "\u001b[36mINFO\u001b[0m[0420] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[1208.255967, "o", "\u001b[36mINFO\u001b[0m[0423] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[1209.177294, "o", "\u001b[36mINFO\u001b[0m[0424] Starting container [etcd-fix-perm] on host [104.45.84.172], try #1 \r\n"]
[1211.123729, "o", "\u001b[36mINFO\u001b[0m[0426] Successfully started [etcd-fix-perm] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0426] Waiting for [etcd-fix-perm] container to exit on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0426] Waiting for [etcd-fix-perm] container to exit on host [104.45.84.172] \r\n"]
[1211.413783, "o", "\u001b[36mINFO\u001b[0m[0426] Container [etcd-fix-perm] is still running on host [104.45.84.172]: stderr: [], stdout: [] \r\n"]
[1212.414161, "o", "\u001b[36mINFO\u001b[0m[0427] Waiting for [etcd-fix-perm] container to exit on host [104.45.84.172] \r\n"]
[1212.558055, "o", "\u001b[36mINFO\u001b[0m[0428] Removing container [etcd-fix-perm] on host [104.45.84.172], try #1 \r\n"]
[1212.729614, "o", "\u001b[36mINFO\u001b[0m[0428] [remove/etcd-fix-perm] Successfully removed container on host [104.45.84.172] \r\n"]
[1213.073822, "o", "\u001b[36mINFO\u001b[0m[0428] Checking if container [etcd] is running on host [104.45.84.172], try #1 \r\n"]
[1213.284705, "o", "\u001b[36mINFO\u001b[0m[0428] Pulling image [rancher/coreos-etcd:v3.4.13-rancher1] on host [104.45.84.172], try #1 \r\n"]
[1219.694572, "o", "\u001b[36mINFO\u001b[0m[0435] Image [rancher/coreos-etcd:v3.4.13-rancher1] exists on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0435] Checking if container [old-etcd] is running on host [104.45.84.172], try #1 \r\n"]
[1219.848543, "o", "\u001b[36mINFO\u001b[0m[0435] Stopping container [etcd] on host [104.45.84.172] with stopTimeoutDuration [5s], try #1 \r\n"]
[1222.364884, "o", "\u001b[36mINFO\u001b[0m[0437] Waiting for [etcd] container to exit on host [104.45.84.172] \r\n"]
[1222.43207, "o", "\u001b[36mINFO\u001b[0m[0437] Renaming container [etcd] to [old-etcd] on host [104.45.84.172], try #1 \r\n"]
[1222.874286, "o", "\u001b[36mINFO\u001b[0m[0438] Starting container [etcd] on host [104.45.84.172], try #1 \r\n"]
[1223.718048, "o", "\u001b[36mINFO\u001b[0m[0439] [etcd] Successfully updated [etcd] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0439] Removing container [old-etcd] on host [104.45.84.172], try #1 \r\n"]
[1223.837753, "o", "\u001b[36mINFO\u001b[0m[0439] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [104.45.84.172] \r\n"]
[1223.905363, "o", "\u001b[36mINFO\u001b[0m[0439] Removing container [etcd-rolling-snapshots] on host [104.45.84.172], try #1 \r\n"]
[1225.049585, "o", "\u001b[36mINFO\u001b[0m[0440] [remove/etcd-rolling-snapshots] Successfully removed container on host [104.45.84.172] \r\n"]
[1225.257828, "o", "\u001b[36mINFO\u001b[0m[0440] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[1226.383006, "o", "\u001b[36mINFO\u001b[0m[0441] Starting container [etcd-rolling-snapshots] on host [104.45.84.172], try #1 \r\n"]
[1228.121755, "o", "\u001b[36mINFO\u001b[0m[0443] [etcd] Successfully started [etcd-rolling-snapshots] container on host [104.45.84.172] \r\n"]
[1233.393111, "o", "\u001b[36mINFO\u001b[0m[0448] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[1234.778025, "o", "\u001b[36mINFO\u001b[0m[0450] Starting container [rke-bundle-cert] on host [104.45.84.172], try #1 \r\n"]
[1237.116947, "o", "\u001b[36mINFO\u001b[0m[0452] [certificates] Successfully started [rke-bundle-cert] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0452] Waiting for [rke-bundle-cert] container to exit on host [104.45.84.172] \r\n"]
[1238.222328, "o", "\u001b[36mINFO\u001b[0m[0453] Container [rke-bundle-cert] is still running on host [104.45.84.172]: stderr: [], stdout: [] \r\n"]
[1239.222401, "o", "\u001b[36mINFO\u001b[0m[0454] Waiting for [rke-bundle-cert] container to exit on host [104.45.84.172] \r\n"]
[1239.291767, "o", "\u001b[36mINFO\u001b[0m[0454] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0454] Removing container [rke-bundle-cert] on host [104.45.84.172], try #1 \r\n"]
[1239.827281, "o", "\u001b[36mINFO\u001b[0m[0455] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[1240.511782, "o", "\u001b[36mINFO\u001b[0m[0456] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1242.150742, "o", "\u001b[36mINFO\u001b[0m[0457] [etcd] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[1242.288692, "o", "\u001b[36mINFO\u001b[0m[0457] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1243.07458, "o", "\u001b[36mINFO\u001b[0m[0458] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n"]
[1243.478849, "o", "\u001b[36mINFO\u001b[0m[0458] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[1244.301727, "o", "\u001b[36mINFO\u001b[0m[0459] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1245.667866, "o", "\u001b[36mINFO\u001b[0m[0461] [etcd] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[1245.806235, "o", "\u001b[36mINFO\u001b[0m[0461] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1246.554678, "o", "\u001b[36mINFO\u001b[0m[0462] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0462] [etcd] Successfully started etcd plane.. Checking etcd cluster health \r\n"]
[1248.758631, "o", "\u001b[36mINFO\u001b[0m[0464] [etcd] etcd host [104.45.86.4] reported healthy=true \r\n"]
[1248.759221, "o", "\u001b[36mINFO\u001b[0m[0464] [controlplane] Now checking status of node 104.45.86.4, try #1 \r\n"]
[1249.09321, "o", "\u001b[36mINFO\u001b[0m[0464] [controlplane] Now checking status of node 23.102.48.17, try #1 \r\n"]
[1249.180447, "o", "\u001b[36mINFO\u001b[0m[0464] [controlplane] Now checking status of node 104.45.84.172, try #1 \r\n"]
[1249.280414, "o", "\u001b[36mINFO\u001b[0m[0464] [controlplane] Processing controlplane hosts for upgrade 1 at a time \r\n\u001b[36mINFO\u001b[0m[0464] Processing controlplane host 104.45.86.4     \r\n\u001b[36mINFO\u001b[0m[0464] [controlplane] Now checking status of node 104.45.86.4, try #1 \r\n"]
[1249.460251, "o", "\u001b[36mINFO\u001b[0m[0464] [controlplane] Getting list of nodes for upgrade \r\n"]
[1252.537822, "o", "\u001b[36mINFO\u001b[0m[0468] Upgrading controlplane components for control host 104.45.86.4 \r\n\u001b[36mINFO\u001b[0m[0468] Checking if container [service-sidekick] is running on host [104.45.86.4], try #1 \r\n"]
[1252.942885, "o", "\u001b[36mINFO\u001b[0m[0468] [sidekick] Sidekick container already created on host [104.45.86.4] \r\n"]
[1253.217269, "o", "\u001b[36mINFO\u001b[0m[0468] Checking if container [kube-apiserver] is running on host [104.45.86.4], try #1 \r\n"]
[1253.485025, "o", "\u001b[36mINFO\u001b[0m[0468] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0468] Checking if container [old-kube-apiserver] is running on host [104.45.86.4], try #1 \r\n"]
[1253.63075, "o", "\u001b[36mINFO\u001b[0m[0469] Stopping container [kube-apiserver] on host [104.45.86.4] with stopTimeoutDuration [5s], try #1 \r\n"]
[1254.643521, "o", "\u001b[36mINFO\u001b[0m[0470] Waiting for [kube-apiserver] container to exit on host [104.45.86.4] \r\n"]
[1254.772026, "o", "\u001b[36mINFO\u001b[0m[0470] Renaming container [kube-apiserver] to [old-kube-apiserver] on host [104.45.86.4], try #1 \r\n"]
[1255.308752, "o", "\u001b[36mINFO\u001b[0m[0470] Starting container [kube-apiserver] on host [104.45.86.4], try #1 \r\n"]
[1256.287898, "o", "\u001b[36mINFO\u001b[0m[0471] [controlplane] Successfully updated [kube-apiserver] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0471] Removing container [old-kube-apiserver] on host [104.45.86.4], try #1 \r\n"]
[1256.500582, "o", "\u001b[36mINFO\u001b[0m[0472] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [104.45.86.4] \r\n"]
[1286.490853, "o", "\u001b[36mINFO\u001b[0m[0501] [healthcheck] service [kube-apiserver] on host [104.45.86.4] is healthy \r\n"]
[1286.824478, "o", "\u001b[36mINFO\u001b[0m[0502] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[1287.514026, "o", "\u001b[36mINFO\u001b[0m[0503] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1289.202893, "o", "\u001b[36mINFO\u001b[0m[0504] [controlplane] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[1289.335287, "o", "\u001b[36mINFO\u001b[0m[0504] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1290.075123, "o", "\u001b[36mINFO\u001b[0m[0505] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n"]
[1290.403131, "o", "\u001b[36mINFO\u001b[0m[0505] Checking if container [kube-controller-manager] is running on host [104.45.86.4], try #1 \r\n"]
[1290.677314, "o", "\u001b[36mINFO\u001b[0m[0506] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0506] Checking if container [old-kube-controller-manager] is running on host [104.45.86.4], try #1 \r\n"]
[1290.827372, "o", "\u001b[36mINFO\u001b[0m[0506] Stopping container [kube-controller-manager] on host [104.45.86.4] with stopTimeoutDuration [5s], try #1 \r\n"]
[1291.50877, "o", "\u001b[36mINFO\u001b[0m[0507] Waiting for [kube-controller-manager] container to exit on host [104.45.86.4] \r\n"]
[1291.636391, "o", "\u001b[36mINFO\u001b[0m[0507] Renaming container [kube-controller-manager] to [old-kube-controller-manager] on host [104.45.86.4], try #1 \r\n"]
[1292.123023, "o", "\u001b[36mINFO\u001b[0m[0507] Starting container [kube-controller-manager] on host [104.45.86.4], try #1 \r\n"]
[1292.941207, "o", "\u001b[36mINFO\u001b[0m[0508] [controlplane] Successfully updated [kube-controller-manager] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0508] Removing container [old-kube-controller-manager] on host [104.45.86.4], try #1 \r\n"]
[1293.065575, "o", "\u001b[36mINFO\u001b[0m[0508] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [104.45.86.4] \r\n"]
[1301.844299, "o", "\u001b[36mINFO\u001b[0m[0517] [healthcheck] service [kube-controller-manager] on host [104.45.86.4] is healthy \r\n"]
[1302.179546, "o", "\u001b[36mINFO\u001b[0m[0517] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[1302.978287, "o", "\u001b[36mINFO\u001b[0m[0518] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1304.460599, "o", "\u001b[36mINFO\u001b[0m[0519] [controlplane] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[1305.402388, "o", "\u001b[36mINFO\u001b[0m[0520] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1305.640069, "o", "\u001b[36mINFO\u001b[0m[0521] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n"]
[1305.914258, "o", "\u001b[36mINFO\u001b[0m[0521] Checking if container [kube-scheduler] is running on host [104.45.86.4], try #1 \r\n"]
[1306.183537, "o", "\u001b[36mINFO\u001b[0m[0521] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0521] Checking if container [old-kube-scheduler] is running on host [104.45.86.4], try #1 \r\n"]
[1306.338045, "o", "\u001b[36mINFO\u001b[0m[0521] Stopping container [kube-scheduler] on host [104.45.86.4] with stopTimeoutDuration [5s], try #1 \r\n"]
[1306.968606, "o", "\u001b[36mINFO\u001b[0m[0522] Waiting for [kube-scheduler] container to exit on host [104.45.86.4] \r\n"]
[1307.032793, "o", "\u001b[36mINFO\u001b[0m[0522] Renaming container [kube-scheduler] to [old-kube-scheduler] on host [104.45.86.4], try #1 \r\n"]
[1307.465394, "o", "\u001b[36mINFO\u001b[0m[0522] Starting container [kube-scheduler] on host [104.45.86.4], try #1 \r\n"]
[1308.506, "o", "\u001b[36mINFO\u001b[0m[0524] [controlplane] Successfully updated [kube-scheduler] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0524] Removing container [old-kube-scheduler] on host [104.45.86.4], try #1 \r\n"]
[1308.631587, "o", "\u001b[36mINFO\u001b[0m[0524] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [104.45.86.4] \r\n"]
[1317.55082, "o", "\u001b[36mINFO\u001b[0m[0533] [healthcheck] service [kube-scheduler] on host [104.45.86.4] is healthy \r\n"]
[1317.889886, "o", "\u001b[36mINFO\u001b[0m[0533] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[1318.644684, "o", "\u001b[36mINFO\u001b[0m[0534] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1320.080364, "o", "\u001b[36mINFO\u001b[0m[0535] [controlplane] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[1320.214793, "o", "\u001b[36mINFO\u001b[0m[0535] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1320.999988, "o", "\u001b[36mINFO\u001b[0m[0536] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0536] Upgrading workerplane components for control host 104.45.86.4 \r\n\u001b[36mINFO\u001b[0m[0536] Checking if container [service-sidekick] is running on host [104.45.86.4], try #1 \r\n"]
[1321.456502, "o", "\u001b[36mINFO\u001b[0m[0536] [sidekick] Sidekick container already created on host [104.45.86.4] \r\n"]
[1321.796533, "o", "\u001b[36mINFO\u001b[0m[0537] Checking if container [kubelet] is running on host [104.45.86.4], try #1 \r\n"]
[1322.064592, "o", "\u001b[36mINFO\u001b[0m[0537] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0537] Checking if container [old-kubelet] is running on host [104.45.86.4], try #1 \r\n"]
[1322.219177, "o", "\u001b[36mINFO\u001b[0m[0537] Stopping container [kubelet] on host [104.45.86.4] with stopTimeoutDuration [5s], try #1 \r\n"]
[1322.875359, "o", "\u001b[36mINFO\u001b[0m[0538] Waiting for [kubelet] container to exit on host [104.45.86.4] \r\n"]
[1322.942365, "o", "\u001b[36mINFO\u001b[0m[0538] Renaming container [kubelet] to [old-kubelet] on host [104.45.86.4], try #1 \r\n"]
[1323.461428, "o", "\u001b[36mINFO\u001b[0m[0538] Starting container [kubelet] on host [104.45.86.4], try #1 \r\n"]
[1324.27643, "o", "\u001b[36mINFO\u001b[0m[0539] [worker] Successfully updated [kubelet] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0539] Removing container [old-kubelet] on host [104.45.86.4], try #1 \r\n"]
[1324.419559, "o", "\u001b[36mINFO\u001b[0m[0539] [healthcheck] Start Healthcheck on service [kubelet] on host [104.45.86.4] \r\n"]
[1333.767117, "o", "\u001b[36mINFO\u001b[0m[0549] [healthcheck] service [kubelet] on host [104.45.86.4] is healthy \r\n"]
[1334.197139, "o", "\u001b[36mINFO\u001b[0m[0549] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[1335.642402, "o", "\u001b[36mINFO\u001b[0m[0551] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1337.384765, "o", "\u001b[36mINFO\u001b[0m[0552] [worker] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[1337.512306, "o", "\u001b[36mINFO\u001b[0m[0553] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1338.251507, "o", "\u001b[36mINFO\u001b[0m[0553] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n"]
[1338.525922, "o", "\u001b[36mINFO\u001b[0m[0554] Checking if container [kube-proxy] is running on host [104.45.86.4], try #1 \r\n"]
[1338.828986, "o", "\u001b[36mINFO\u001b[0m[0554] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0554] Checking if container [old-kube-proxy] is running on host [104.45.86.4], try #1 \r\n"]
[1339.002434, "o", "\u001b[36mINFO\u001b[0m[0554] Stopping container [kube-proxy] on host [104.45.86.4] with stopTimeoutDuration [5s], try #1 \r\n"]
[1339.738805, "o", "\u001b[36mINFO\u001b[0m[0555] Waiting for [kube-proxy] container to exit on host [104.45.86.4] \r\n"]
[1339.806954, "o", "\u001b[36mINFO\u001b[0m[0555] Renaming container [kube-proxy] to [old-kube-proxy] on host [104.45.86.4], try #1 \r\n"]
[1340.353506, "o", "\u001b[36mINFO\u001b[0m[0555] Starting container [kube-proxy] on host [104.45.86.4], try #1 \r\n"]
[1341.277984, "o", "\u001b[36mINFO\u001b[0m[0556] [worker] Successfully updated [kube-proxy] container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0556] Removing container [old-kube-proxy] on host [104.45.86.4], try #1 \r\n"]
[1341.428949, "o", "\u001b[36mINFO\u001b[0m[0556] [healthcheck] Start Healthcheck on service [kube-proxy] on host [104.45.86.4] \r\n"]
[1343.294598, "o", "\u001b[36mINFO\u001b[0m[0558] [healthcheck] service [kube-proxy] on host [104.45.86.4] is healthy \r\n"]
[1343.616395, "o", "\u001b[36mINFO\u001b[0m[0559] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[1344.654828, "o", "\u001b[36mINFO\u001b[0m[0560] Starting container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1346.292837, "o", "\u001b[36mINFO\u001b[0m[0561] [worker] Successfully started [rke-log-linker] container on host [104.45.86.4] \r\n"]
[1347.073359, "o", "\u001b[36mINFO\u001b[0m[0562] Removing container [rke-log-linker] on host [104.45.86.4], try #1 \r\n"]
[1347.233865, "o", "\u001b[36mINFO\u001b[0m[0562] [remove/rke-log-linker] Successfully removed container on host [104.45.86.4] \r\n\u001b[36mINFO\u001b[0m[0562] [controlplane] Now checking status of node 104.45.86.4, try #1 \r\n"]
[1348.088373, "o", "\u001b[36mINFO\u001b[0m[0563] Processing controlplane host 23.102.48.17    \r\n\u001b[36mINFO\u001b[0m[0563] [controlplane] Now checking status of node 23.102.48.17, try #1 \r\n"]
[1348.309043, "o", "\u001b[36mINFO\u001b[0m[0563] [controlplane] Getting list of nodes for upgrade \r\n"]
[1351.506686, "o", "\u001b[36mINFO\u001b[0m[0567] Upgrading controlplane components for control host 23.102.48.17 \r\n\u001b[36mINFO\u001b[0m[0567] Checking if container [service-sidekick] is running on host [23.102.48.17], try #1 \r\n"]
[1351.868654, "o", "\u001b[36mINFO\u001b[0m[0567] [sidekick] Sidekick container already created on host [23.102.48.17] \r\n"]
[1352.137864, "o", "\u001b[36mINFO\u001b[0m[0567] Checking if container [kube-apiserver] is running on host [23.102.48.17], try #1 \r\n"]
[1352.420541, "o", "\u001b[36mINFO\u001b[0m[0567] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0567] Checking if container [old-kube-apiserver] is running on host [23.102.48.17], try #1 \r\n"]
[1352.569104, "o", "\u001b[36mINFO\u001b[0m[0568] Stopping container [kube-apiserver] on host [23.102.48.17] with stopTimeoutDuration [5s], try #1 \r\n"]
[1353.977828, "o", "\u001b[36mINFO\u001b[0m[0569] Waiting for [kube-apiserver] container to exit on host [23.102.48.17] \r\n"]
[1354.111401, "o", "\u001b[36mINFO\u001b[0m[0569] Renaming container [kube-apiserver] to [old-kube-apiserver] on host [23.102.48.17], try #1 \r\n"]
[1354.587282, "o", "\u001b[36mINFO\u001b[0m[0570] Starting container [kube-apiserver] on host [23.102.48.17], try #1 \r\n"]
[1355.508579, "o", "\u001b[36mINFO\u001b[0m[0571] [controlplane] Successfully updated [kube-apiserver] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0571] Removing container [old-kube-apiserver] on host [23.102.48.17], try #1 \r\n"]
[1355.629431, "o", "\u001b[36mINFO\u001b[0m[0571] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [23.102.48.17] \r\n"]
[1402.597711, "o", "\u001b[36mINFO\u001b[0m[0618] [healthcheck] service [kube-apiserver] on host [23.102.48.17] is healthy \r\n"]
[1402.935465, "o", "\u001b[36mINFO\u001b[0m[0618] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[1403.919733, "o", "\u001b[36mINFO\u001b[0m[0619] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1405.684375, "o", "\u001b[36mINFO\u001b[0m[0621] [controlplane] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[1406.433503, "o", "\u001b[36mINFO\u001b[0m[0621] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1406.709124, "o", "\u001b[36mINFO\u001b[0m[0622] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[1406.98111, "o", "\u001b[36mINFO\u001b[0m[0622] Checking if container [kube-controller-manager] is running on host [23.102.48.17], try #1 \r\n"]
[1407.255441, "o", "\u001b[36mINFO\u001b[0m[0622] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0622] Checking if container [old-kube-controller-manager] is running on host [23.102.48.17], try #1 \r\n"]
[1407.404783, "o", "\u001b[36mINFO\u001b[0m[0622] Stopping container [kube-controller-manager] on host [23.102.48.17] with stopTimeoutDuration [5s], try #1 \r\n"]
[1408.142772, "o", "\u001b[36mINFO\u001b[0m[0623] Waiting for [kube-controller-manager] container to exit on host [23.102.48.17] \r\n"]
[1408.277429, "o", "\u001b[36mINFO\u001b[0m[0623] Renaming container [kube-controller-manager] to [old-kube-controller-manager] on host [23.102.48.17], try #1 \r\n"]
[1408.756335, "o", "\u001b[36mINFO\u001b[0m[0624] Starting container [kube-controller-manager] on host [23.102.48.17], try #1 \r\n"]
[1409.632794, "o", "\u001b[36mINFO\u001b[0m[0625] [controlplane] Successfully updated [kube-controller-manager] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0625] Removing container [old-kube-controller-manager] on host [23.102.48.17], try #1 \r\n"]
[1409.735455, "o", "\u001b[36mINFO\u001b[0m[0625] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [23.102.48.17] \r\n"]
[1432.187292, "o", "\u001b[36mINFO\u001b[0m[0647] [healthcheck] service [kube-controller-manager] on host [23.102.48.17] is healthy \r\n"]
[1432.521224, "o", "\u001b[36mINFO\u001b[0m[0648] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[1433.502959, "o", "\u001b[36mINFO\u001b[0m[0649] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1435.168299, "o", "\u001b[36mINFO\u001b[0m[0650] [controlplane] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[1435.304175, "o", "\u001b[36mINFO\u001b[0m[0650] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1436.200106, "o", "\u001b[36mINFO\u001b[0m[0651] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[1436.468774, "o", "\u001b[36mINFO\u001b[0m[0651] Checking if container [kube-scheduler] is running on host [23.102.48.17], try #1 \r\n"]
[1436.763518, "o", "\u001b[36mINFO\u001b[0m[0652] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0652] Checking if container [old-kube-scheduler] is running on host [23.102.48.17], try #1 \r\n"]
[1436.907831, "o", "\u001b[36mINFO\u001b[0m[0652] Stopping container [kube-scheduler] on host [23.102.48.17] with stopTimeoutDuration [5s], try #1 \r\n"]
[1437.478261, "o", "\u001b[36mINFO\u001b[0m[0652] Waiting for [kube-scheduler] container to exit on host [23.102.48.17] \r\n"]
[1437.60818, "o", "\u001b[36mINFO\u001b[0m[0653] Renaming container [kube-scheduler] to [old-kube-scheduler] on host [23.102.48.17], try #1 \r\n"]
[1438.14827, "o", "\u001b[36mINFO\u001b[0m[0653] Starting container [kube-scheduler] on host [23.102.48.17], try #1 \r\n"]
[1438.965841, "o", "\u001b[36mINFO\u001b[0m[0654] [controlplane] Successfully updated [kube-scheduler] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0654] Removing container [old-kube-scheduler] on host [23.102.48.17], try #1 \r\n"]
[1439.082828, "o", "\u001b[36mINFO\u001b[0m[0654] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [23.102.48.17] \r\n"]
[1448.02484, "o", "\u001b[36mINFO\u001b[0m[0663] [healthcheck] service [kube-scheduler] on host [23.102.48.17] is healthy \r\n"]
[1448.356936, "o", "\u001b[36mINFO\u001b[0m[0663] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[1449.410223, "o", "\u001b[36mINFO\u001b[0m[0664] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1451.357108, "o", "\u001b[36mINFO\u001b[0m[0666] [controlplane] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[1452.244591, "o", "\u001b[36mINFO\u001b[0m[0667] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1452.428027, "o", "\u001b[36mINFO\u001b[0m[0667] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0667] Upgrading workerplane components for control host 23.102.48.17 \r\n\u001b[36mINFO\u001b[0m[0667] Checking if container [service-sidekick] is running on host [23.102.48.17], try #1 \r\n"]
[1452.794431, "o", "\u001b[36mINFO\u001b[0m[0668] [sidekick] Sidekick container already created on host [23.102.48.17] \r\n"]
[1453.073217, "o", "\u001b[36mINFO\u001b[0m[0668] Checking if container [kubelet] is running on host [23.102.48.17], try #1 \r\n"]
[1453.396412, "o", "\u001b[36mINFO\u001b[0m[0668] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0668] Checking if container [old-kubelet] is running on host [23.102.48.17], try #1 \r\n"]
[1453.556659, "o", "\u001b[36mINFO\u001b[0m[0669] Stopping container [kubelet] on host [23.102.48.17] with stopTimeoutDuration [5s], try #1 \r\n"]
[1454.223762, "o", "\u001b[36mINFO\u001b[0m[0669] Waiting for [kubelet] container to exit on host [23.102.48.17] \r\n"]
[1454.292161, "o", "\u001b[36mINFO\u001b[0m[0669] Renaming container [kubelet] to [old-kubelet] on host [23.102.48.17], try #1 \r\n"]
[1454.838976, "o", "\u001b[36mINFO\u001b[0m[0670] Starting container [kubelet] on host [23.102.48.17], try #1 \r\n"]
[1455.681775, "o", "\u001b[36mINFO\u001b[0m[0671] [worker] Successfully updated [kubelet] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0671] Removing container [old-kubelet] on host [23.102.48.17], try #1 \r\n"]
[1455.828192, "o", "\u001b[36mINFO\u001b[0m[0671] [healthcheck] Start Healthcheck on service [kubelet] on host [23.102.48.17] \r\n"]
[1471.492685, "o", "\u001b[36mINFO\u001b[0m[0686] [healthcheck] service [kubelet] on host [23.102.48.17] is healthy \r\n"]
[1471.832806, "o", "\u001b[36mINFO\u001b[0m[0687] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[1472.758505, "o", "\u001b[36mINFO\u001b[0m[0688] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1474.498199, "o", "\u001b[36mINFO\u001b[0m[0690] [worker] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[1475.113727, "o", "\u001b[36mINFO\u001b[0m[0690] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1475.317458, "o", "\u001b[36mINFO\u001b[0m[0690] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n"]
[1475.588746, "o", "\u001b[36mINFO\u001b[0m[0691] Checking if container [kube-proxy] is running on host [23.102.48.17], try #1 \r\n"]
[1475.870143, "o", "\u001b[36mINFO\u001b[0m[0691] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0691] Checking if container [old-kube-proxy] is running on host [23.102.48.17], try #1 \r\n"]
[1476.014091, "o", "\u001b[36mINFO\u001b[0m[0691] Stopping container [kube-proxy] on host [23.102.48.17] with stopTimeoutDuration [5s], try #1 \r\n"]
[1476.750969, "o", "\u001b[36mINFO\u001b[0m[0692] Waiting for [kube-proxy] container to exit on host [23.102.48.17] \r\n"]
[1476.880128, "o", "\u001b[36mINFO\u001b[0m[0692] Renaming container [kube-proxy] to [old-kube-proxy] on host [23.102.48.17], try #1 \r\n"]
[1477.365689, "o", "\u001b[36mINFO\u001b[0m[0692] Starting container [kube-proxy] on host [23.102.48.17], try #1 \r\n"]
[1478.186834, "o", "\u001b[36mINFO\u001b[0m[0693] [worker] Successfully updated [kube-proxy] container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0693] Removing container [old-kube-proxy] on host [23.102.48.17], try #1 \r\n"]
[1478.349407, "o", "\u001b[36mINFO\u001b[0m[0693] [healthcheck] Start Healthcheck on service [kube-proxy] on host [23.102.48.17] \r\n"]
[1487.368292, "o", "\u001b[36mINFO\u001b[0m[0702] [healthcheck] service [kube-proxy] on host [23.102.48.17] is healthy \r\n"]
[1487.708249, "o", "\u001b[36mINFO\u001b[0m[0703] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[1488.629231, "o", "\u001b[36mINFO\u001b[0m[0704] Starting container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1490.268902, "o", "\u001b[36mINFO\u001b[0m[0705] [worker] Successfully started [rke-log-linker] container on host [23.102.48.17] \r\n"]
[1490.405059, "o", "\u001b[36mINFO\u001b[0m[0705] Removing container [rke-log-linker] on host [23.102.48.17], try #1 \r\n"]
[1491.499339, "o", "\u001b[36mINFO\u001b[0m[0707] [remove/rke-log-linker] Successfully removed container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0707] [controlplane] Now checking status of node 23.102.48.17, try #1 \r\n"]
[1492.326705, "o", "\u001b[36mINFO\u001b[0m[0707] Processing controlplane host 104.45.84.172   \r\n\u001b[36mINFO\u001b[0m[0707] [controlplane] Now checking status of node 104.45.84.172, try #1 \r\n"]
[1492.516251, "o", "\u001b[36mINFO\u001b[0m[0708] [controlplane] Getting list of nodes for upgrade \r\n"]
[1496.533901, "o", "\u001b[36mINFO\u001b[0m[0712] Upgrading controlplane components for control host 104.45.84.172 \r\n\u001b[36mINFO\u001b[0m[0712] Checking if container [service-sidekick] is running on host [104.45.84.172], try #1 \r\n"]
[1496.935339, "o", "\u001b[36mINFO\u001b[0m[0712] [sidekick] Sidekick container already created on host [104.45.84.172] \r\n"]
[1497.218421, "o", "\u001b[36mINFO\u001b[0m[0712] Checking if container [kube-apiserver] is running on host [104.45.84.172], try #1 \r\n"]
[1497.500032, "o", "\u001b[36mINFO\u001b[0m[0713] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0713] Checking if container [old-kube-apiserver] is running on host [104.45.84.172], try #1 \r\n"]
[1497.636395, "o", "\u001b[36mINFO\u001b[0m[0713] Stopping container [kube-apiserver] on host [104.45.84.172] with stopTimeoutDuration [5s], try #1 \r\n"]
[1498.580979, "o", "\u001b[36mINFO\u001b[0m[0714] Waiting for [kube-apiserver] container to exit on host [104.45.84.172] \r\n"]
[1498.70956, "o", "\u001b[36mINFO\u001b[0m[0714] Renaming container [kube-apiserver] to [old-kube-apiserver] on host [104.45.84.172], try #1 \r\n"]
[1499.140538, "o", "\u001b[36mINFO\u001b[0m[0714] Starting container [kube-apiserver] on host [104.45.84.172], try #1 \r\n"]
[1500.094101, "o", "\u001b[36mINFO\u001b[0m[0715] [controlplane] Successfully updated [kube-apiserver] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0715] Removing container [old-kube-apiserver] on host [104.45.84.172], try #1 \r\n"]
[1500.227029, "o", "\u001b[36mINFO\u001b[0m[0715] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [104.45.84.172] \r\n"]
[1546.865759, "o", "\u001b[36mINFO\u001b[0m[0762] [healthcheck] service [kube-apiserver] on host [104.45.84.172] is healthy \r\n"]
[1547.246868, "o", "\u001b[36mINFO\u001b[0m[0762] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[1548.186221, "o", "\u001b[36mINFO\u001b[0m[0763] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1549.706235, "o", "\u001b[36mINFO\u001b[0m[0765] [controlplane] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[1550.392904, "o", "\u001b[36mINFO\u001b[0m[0765] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1550.605237, "o", "\u001b[36mINFO\u001b[0m[0766] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n"]
[1550.994716, "o", "\u001b[36mINFO\u001b[0m[0766] Checking if container [kube-controller-manager] is running on host [104.45.84.172], try #1 \r\n"]
[1551.302044, "o", "\u001b[36mINFO\u001b[0m[0766] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0766] Checking if container [old-kube-controller-manager] is running on host [104.45.84.172], try #1 \r\n"]
[1551.43453, "o", "\u001b[36mINFO\u001b[0m[0766] Stopping container [kube-controller-manager] on host [104.45.84.172] with stopTimeoutDuration [5s], try #1 \r\n"]
[1552.119885, "o", "\u001b[36mINFO\u001b[0m[0767] Waiting for [kube-controller-manager] container to exit on host [104.45.84.172] \r\n"]
[1552.24909, "o", "\u001b[36mINFO\u001b[0m[0767] Renaming container [kube-controller-manager] to [old-kube-controller-manager] on host [104.45.84.172], try #1 \r\n"]
[1552.732525, "o", "\u001b[36mINFO\u001b[0m[0768] Starting container [kube-controller-manager] on host [104.45.84.172], try #1 \r\n"]
[1553.654104, "o", "\u001b[36mINFO\u001b[0m[0769] [controlplane] Successfully updated [kube-controller-manager] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0769] Removing container [old-kube-controller-manager] on host [104.45.84.172], try #1 \r\n"]
[1553.771487, "o", "\u001b[36mINFO\u001b[0m[0769] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [104.45.84.172] \r\n"]
[1569.396674, "o", "\u001b[36mINFO\u001b[0m[0784] [healthcheck] service [kube-controller-manager] on host [104.45.84.172] is healthy \r\n"]
[1569.726794, "o", "\u001b[36mINFO\u001b[0m[0785] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[1570.448862, "o", "\u001b[36mINFO\u001b[0m[0785] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1572.236122, "o", "\u001b[36mINFO\u001b[0m[0787] [controlplane] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[1572.378733, "o", "\u001b[36mINFO\u001b[0m[0787] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1573.135607, "o", "\u001b[36mINFO\u001b[0m[0788] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n"]
[1573.403374, "o", "\u001b[36mINFO\u001b[0m[0788] Checking if container [kube-scheduler] is running on host [104.45.84.172], try #1 \r\n"]
[1573.668565, "o", "\u001b[36mINFO\u001b[0m[0789] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0789] Checking if container [old-kube-scheduler] is running on host [104.45.84.172], try #1 \r\n"]
[1573.809009, "o", "\u001b[36mINFO\u001b[0m[0789] Stopping container [kube-scheduler] on host [104.45.84.172] with stopTimeoutDuration [5s], try #1 \r\n"]
[1574.392442, "o", "\u001b[36mINFO\u001b[0m[0789] Waiting for [kube-scheduler] container to exit on host [104.45.84.172] \r\n"]
[1574.519549, "o", "\u001b[36mINFO\u001b[0m[0790] Renaming container [kube-scheduler] to [old-kube-scheduler] on host [104.45.84.172], try #1 \r\n"]
[1574.954234, "o", "\u001b[36mINFO\u001b[0m[0790] Starting container [kube-scheduler] on host [104.45.84.172], try #1 \r\n"]
[1575.977169, "o", "\u001b[36mINFO\u001b[0m[0791] [controlplane] Successfully updated [kube-scheduler] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0791] Removing container [old-kube-scheduler] on host [104.45.84.172], try #1 \r\n"]
[1576.59299, "o", "\u001b[36mINFO\u001b[0m[0792] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [104.45.84.172] \r\n"]
[1585.220786, "o", "\u001b[36mINFO\u001b[0m[0800] [healthcheck] service [kube-scheduler] on host [104.45.84.172] is healthy \r\n"]
[1585.555323, "o", "\u001b[36mINFO\u001b[0m[0801] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[1586.37457, "o", "\u001b[36mINFO\u001b[0m[0801] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1588.197885, "o", "\u001b[36mINFO\u001b[0m[0803] [controlplane] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[1588.343866, "o", "\u001b[36mINFO\u001b[0m[0803] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1589.289338, "o", "\u001b[36mINFO\u001b[0m[0804] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0804] Upgrading workerplane components for control host 104.45.84.172 \r\n\u001b[36mINFO\u001b[0m[0804] Checking if container [service-sidekick] is running on host [104.45.84.172], try #1 \r\n"]
[1589.708617, "o", "\u001b[36mINFO\u001b[0m[0805] [sidekick] Sidekick container already created on host [104.45.84.172] \r\n"]
[1589.917101, "o", "\u001b[36mINFO\u001b[0m[0805] Checking if container [kubelet] is running on host [104.45.84.172], try #1 \r\n"]
[1590.187835, "o", "\u001b[36mINFO\u001b[0m[0805] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0805] Checking if container [old-kubelet] is running on host [104.45.84.172], try #1 \r\n"]
[1590.326895, "o", "\u001b[36mINFO\u001b[0m[0805] Stopping container [kubelet] on host [104.45.84.172] with stopTimeoutDuration [5s], try #1 \r\n"]
[1591.030869, "o", "\u001b[36mINFO\u001b[0m[0806] Waiting for [kubelet] container to exit on host [104.45.84.172] \r\n"]
[1591.159062, "o", "\u001b[36mINFO\u001b[0m[0806] Renaming container [kubelet] to [old-kubelet] on host [104.45.84.172], try #1 \r\n"]
[1591.548196, "o", "\u001b[36mINFO\u001b[0m[0807] Starting container [kubelet] on host [104.45.84.172], try #1 \r\n"]
[1592.296846, "o", "\u001b[36mINFO\u001b[0m[0807] [worker] Successfully updated [kubelet] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0807] Removing container [old-kubelet] on host [104.45.84.172], try #1 \r\n"]
[1592.568093, "o", "\u001b[36mINFO\u001b[0m[0808] [healthcheck] Start Healthcheck on service [kubelet] on host [104.45.84.172] \r\n"]
[1607.633363, "o", "\u001b[36mINFO\u001b[0m[0823] [healthcheck] service [kubelet] on host [104.45.84.172] is healthy \r\n"]
[1607.95178, "o", "\u001b[36mINFO\u001b[0m[0823] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[1608.609231, "o", "\u001b[36mINFO\u001b[0m[0824] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1610.180203, "o", "\u001b[36mINFO\u001b[0m[0825] [worker] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[1611.214005, "o", "\u001b[36mINFO\u001b[0m[0826] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1611.510427, "o", "\u001b[36mINFO\u001b[0m[0827] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n"]
[1611.766557, "o", "\u001b[36mINFO\u001b[0m[0827] Checking if container [kube-proxy] is running on host [104.45.84.172], try #1 \r\n"]
[1612.053073, "o", "\u001b[36mINFO\u001b[0m[0827] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0827] Checking if container [old-kube-proxy] is running on host [104.45.84.172], try #1 \r\n"]
[1612.234952, "o", "\u001b[36mINFO\u001b[0m[0827] Stopping container [kube-proxy] on host [104.45.84.172] with stopTimeoutDuration [5s], try #1 \r\n"]
[1612.937754, "o", "\u001b[36mINFO\u001b[0m[0828] Waiting for [kube-proxy] container to exit on host [104.45.84.172] \r\n"]
[1613.063682, "o", "\u001b[36mINFO\u001b[0m[0828] Renaming container [kube-proxy] to [old-kube-proxy] on host [104.45.84.172], try #1 \r\n"]
[1613.559892, "o", "\u001b[36mINFO\u001b[0m[0829] Starting container [kube-proxy] on host [104.45.84.172], try #1 \r\n"]
[1614.331616, "o", "\u001b[36mINFO\u001b[0m[0829] [worker] Successfully updated [kube-proxy] container on host [104.45.84.172] \r\n\u001b[36mINFO\u001b[0m[0829] Removing container [old-kube-proxy] on host [104.45.84.172], try #1 \r\n"]
[1614.43923, "o", "\u001b[36mINFO\u001b[0m[0829] [healthcheck] Start Healthcheck on service [kube-proxy] on host [104.45.84.172] \r\n"]
[1622.89857, "o", "\u001b[36mINFO\u001b[0m[0838] [healthcheck] service [kube-proxy] on host [104.45.84.172] is healthy \r\n"]
[1623.21683, "o", "\u001b[36mINFO\u001b[0m[0838] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[1623.900504, "o", "\u001b[36mINFO\u001b[0m[0839] Starting container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1625.4377, "o", "\u001b[36mINFO\u001b[0m[0840] [worker] Successfully started [rke-log-linker] container on host [104.45.84.172] \r\n"]
[1626.217872, "o", "\u001b[36mINFO\u001b[0m[0841] Removing container [rke-log-linker] on host [104.45.84.172], try #1 \r\n"]
[1626.370529, "o", "\u001b[36mINFO\u001b[0m[0841] [remove/rke-log-linker] Successfully removed container on host [104.45.84.172] \r\n"]
[1626.370674, "o", "\u001b[36mINFO\u001b[0m[0841] [controlplane] Now checking status of node 104.45.84.172, try #1 \r\n"]
[1627.104373, "o", "\u001b[36mINFO\u001b[0m[0842] [controlplane] Successfully upgraded Controller Plane.. \r\n"]
[1627.104601, "o", "\u001b[36mINFO\u001b[0m[0842] [authz] Creating rke-job-deployer ServiceAccount \r\n"]
[1627.574176, "o", "\u001b[36mINFO\u001b[0m[0843] [authz] rke-job-deployer ServiceAccount created successfully \r\n\u001b[36mINFO\u001b[0m[0843] [authz] Creating system:node ClusterRoleBinding \r\n"]
[1627.798416, "o", "\u001b[36mINFO\u001b[0m[0843] [authz] system:node ClusterRoleBinding created successfully \r\n\u001b[36mINFO\u001b[0m[0843] [authz] Creating kube-apiserver proxy ClusterRole and ClusterRoleBinding \r\n"]
[1628.213702, "o", "\u001b[36mINFO\u001b[0m[0843] [authz] kube-apiserver proxy ClusterRole and ClusterRoleBinding created successfully \r\n"]
[1628.22087, "o", "\u001b[36mINFO\u001b[0m[0843] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/au1.rkestate] \r\n"]
[1628.223896, "o", "\u001b[36mINFO\u001b[0m[0843] [state] Saving full cluster state to Kubernetes \r\n"]
[1629.847255, "o", "\u001b[36mINFO\u001b[0m[0845] [state] Successfully Saved full cluster state to Kubernetes ConfigMap: full-cluster-state \r\n"]
[1629.850261, "o", "\u001b[36mINFO\u001b[0m[0845] [worker] Now checking status of node 23.102.22.29, try #1 \r\n"]
[1629.935896, "o", "\u001b[36mINFO\u001b[0m[0845] [worker] Now checking status of node 137.135.244.150, try #1 \r\n"]
[1630.049329, "o", "\u001b[36mINFO\u001b[0m[0845] [worker] Now checking status of node 23.102.23.168, try #1 \r\n"]
[1630.176387, "o", "\u001b[36mINFO\u001b[0m[0845] [worker] Upgrading Worker Plane..            \r\n"]
[1630.561831, "o", "\u001b[36mINFO\u001b[0m[0846] Now checking and upgrading worker components on nodes with only worker role 1 at a time \r\n\u001b[36mINFO\u001b[0m[0846] [workerplane] Processing host 23.102.22.29   \r\n"]
[1630.562097, "o", "\u001b[36mINFO\u001b[0m[0846] [worker] Now checking status of node 23.102.22.29, try #1 \r\n"]
[1630.77584, "o", "\u001b[36mINFO\u001b[0m[0846] [worker] Getting list of nodes for upgrade   \r\n"]
[1635.812721, "o", "\u001b[36mINFO\u001b[0m[0851] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[1637.110645, "o", "\u001b[36mINFO\u001b[0m[0852] Starting container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[1638.651707, "o", "\u001b[36mINFO\u001b[0m[0854] [worker] Successfully started [rke-log-linker] container on host [23.102.22.29] \r\n"]
[1639.365913, "o", "\u001b[36mINFO\u001b[0m[0854] Removing container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[1639.516938, "o", "\u001b[36mINFO\u001b[0m[0855] [remove/rke-log-linker] Successfully removed container on host [23.102.22.29] \r\n\u001b[36mINFO\u001b[0m[0855] Checking if container [service-sidekick] is running on host [23.102.22.29], try #1 \r\n"]
[1639.877761, "o", "\u001b[36mINFO\u001b[0m[0855] [sidekick] Sidekick container already created on host [23.102.22.29] \r\n"]
[1640.222699, "o", "\u001b[36mINFO\u001b[0m[0855] Checking if container [kubelet] is running on host [23.102.22.29], try #1 \r\n"]
[1640.464823, "o", "\u001b[36mINFO\u001b[0m[0855] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.22.29] \r\n\u001b[36mINFO\u001b[0m[0855] Checking if container [old-kubelet] is running on host [23.102.22.29], try #1 \r\n"]
[1640.589476, "o", "\u001b[36mINFO\u001b[0m[0856] Stopping container [kubelet] on host [23.102.22.29] with stopTimeoutDuration [5s], try #1 \r\n"]
[1641.411112, "o", "\u001b[36mINFO\u001b[0m[0856] Waiting for [kubelet] container to exit on host [23.102.22.29] \r\n"]
[1641.47544, "o", "\u001b[36mINFO\u001b[0m[0856] Renaming container [kubelet] to [old-kubelet] on host [23.102.22.29], try #1 \r\n"]
[1641.951071, "o", "\u001b[36mINFO\u001b[0m[0857] Starting container [kubelet] on host [23.102.22.29], try #1 \r\n"]
[1642.894037, "o", "\u001b[36mINFO\u001b[0m[0858] [worker] Successfully updated [kubelet] container on host [23.102.22.29] \r\n\u001b[36mINFO\u001b[0m[0858] Removing container [old-kubelet] on host [23.102.22.29], try #1 \r\n"]
[1643.27565, "o", "\u001b[36mINFO\u001b[0m[0858] [healthcheck] Start Healthcheck on service [kubelet] on host [23.102.22.29] \r\n"]
[1654.263371, "o", "\u001b[36mINFO\u001b[0m[0869] [healthcheck] service [kubelet] on host [23.102.22.29] is healthy \r\n"]
[1654.563462, "o", "\u001b[36mINFO\u001b[0m[0870] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[1655.542282, "o", "\u001b[36mINFO\u001b[0m[0871] Starting container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[1657.078651, "o", "\u001b[36mINFO\u001b[0m[0872] [worker] Successfully started [rke-log-linker] container on host [23.102.22.29] \r\n"]
[1657.952246, "o", "\u001b[36mINFO\u001b[0m[0873] Removing container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[1658.205213, "o", "\u001b[36mINFO\u001b[0m[0873] [remove/rke-log-linker] Successfully removed container on host [23.102.22.29] \r\n"]
[1658.54721, "o", "\u001b[36mINFO\u001b[0m[0874] Checking if container [kube-proxy] is running on host [23.102.22.29], try #1 \r\n"]
[1658.785904, "o", "\u001b[36mINFO\u001b[0m[0874] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.22.29] \r\n\u001b[36mINFO\u001b[0m[0874] Checking if container [old-kube-proxy] is running on host [23.102.22.29], try #1 \r\n"]
[1658.913809, "o", "\u001b[36mINFO\u001b[0m[0874] Stopping container [kube-proxy] on host [23.102.22.29] with stopTimeoutDuration [5s], try #1 \r\n"]
[1659.638215, "o", "\u001b[36mINFO\u001b[0m[0875] Waiting for [kube-proxy] container to exit on host [23.102.22.29] \r\n"]
[1659.753709, "o", "\u001b[36mINFO\u001b[0m[0875] Renaming container [kube-proxy] to [old-kube-proxy] on host [23.102.22.29], try #1 \r\n"]
[1660.253062, "o", "\u001b[36mINFO\u001b[0m[0875] Starting container [kube-proxy] on host [23.102.22.29], try #1 \r\n"]
[1661.074184, "o", "\u001b[36mINFO\u001b[0m[0876] [worker] Successfully updated [kube-proxy] container on host [23.102.22.29] \r\n\u001b[36mINFO\u001b[0m[0876] Removing container [old-kube-proxy] on host [23.102.22.29], try #1 \r\n"]
[1661.276863, "o", "\u001b[36mINFO\u001b[0m[0876] [healthcheck] Start Healthcheck on service [kube-proxy] on host [23.102.22.29] \r\n"]
[1663.502615, "o", "\u001b[36mINFO\u001b[0m[0879] [healthcheck] service [kube-proxy] on host [23.102.22.29] is healthy \r\n"]
[1663.954163, "o", "\u001b[36mINFO\u001b[0m[0879] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[1664.861971, "o", "\u001b[36mINFO\u001b[0m[0880] Starting container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[1666.182465, "o", "\u001b[36mINFO\u001b[0m[0881] [worker] Successfully started [rke-log-linker] container on host [23.102.22.29] \r\n"]
[1667.017674, "o", "\u001b[36mINFO\u001b[0m[0882] Removing container [rke-log-linker] on host [23.102.22.29], try #1 \r\n"]
[1667.21633, "o", "\u001b[36mINFO\u001b[0m[0882] [remove/rke-log-linker] Successfully removed container on host [23.102.22.29] \r\n\u001b[36mINFO\u001b[0m[0882] [worker] Now checking status of node 23.102.22.29, try #1 \r\n"]
[1667.843689, "o", "\u001b[36mINFO\u001b[0m[0883] [workerplane] Processing host 137.135.244.150 \r\n\u001b[36mINFO\u001b[0m[0883] [worker] Now checking status of node 137.135.244.150, try #1 \r\n"]
[1668.052363, "o", "\u001b[36mINFO\u001b[0m[0883] [worker] Getting list of nodes for upgrade   \r\n"]
[1672.397989, "o", "\u001b[36mINFO\u001b[0m[0887] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[1676.639109, "o", "\u001b[36mINFO\u001b[0m[0892] Starting container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[1678.284587, "o", "\u001b[36mINFO\u001b[0m[0893] [worker] Successfully started [rke-log-linker] container on host [137.135.244.150] \r\n"]
[1678.879117, "o", "\u001b[36mINFO\u001b[0m[0894] Removing container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[1678.993725, "o", "\u001b[36mINFO\u001b[0m[0894] [remove/rke-log-linker] Successfully removed container on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0894] Checking if container [service-sidekick] is running on host [137.135.244.150], try #1 \r\n"]
[1679.40158, "o", "\u001b[36mINFO\u001b[0m[0894] [sidekick] Sidekick container already created on host [137.135.244.150] \r\n"]
[1679.801075, "o", "\u001b[36mINFO\u001b[0m[0895] Checking if container [kubelet] is running on host [137.135.244.150], try #1 \r\n"]
[1680.084282, "o", "\u001b[36mINFO\u001b[0m[0895] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0895] Checking if container [old-kubelet] is running on host [137.135.244.150], try #1 \r\n"]
[1680.222052, "o", "\u001b[36mINFO\u001b[0m[0895] Stopping container [kubelet] on host [137.135.244.150] with stopTimeoutDuration [5s], try #1 \r\n"]
[1680.855564, "o", "\u001b[36mINFO\u001b[0m[0896] Waiting for [kubelet] container to exit on host [137.135.244.150] \r\n"]
[1680.990529, "o", "\u001b[36mINFO\u001b[0m[0896] Renaming container [kubelet] to [old-kubelet] on host [137.135.244.150], try #1 \r\n"]
[1681.347579, "o", "\u001b[36mINFO\u001b[0m[0896] Starting container [kubelet] on host [137.135.244.150], try #1 \r\n"]
[1682.064849, "o", "\u001b[36mINFO\u001b[0m[0897] [worker] Successfully updated [kubelet] container on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0897] Removing container [old-kubelet] on host [137.135.244.150], try #1 \r\n"]
[1682.45563, "o", "\u001b[36mINFO\u001b[0m[0897] [healthcheck] Start Healthcheck on service [kubelet] on host [137.135.244.150] \r\n"]
[1686.106902, "o", "\u001b[36mINFO\u001b[0m[0901] [healthcheck] service [kubelet] on host [137.135.244.150] is healthy \r\n"]
[1686.548225, "o", "\u001b[36mINFO\u001b[0m[0902] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[1690.058051, "o", "\u001b[36mINFO\u001b[0m[0905] Starting container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[1691.335132, "o", "\u001b[36mINFO\u001b[0m[0906] [worker] Successfully started [rke-log-linker] container on host [137.135.244.150] \r\n"]
[1691.471078, "o", "\u001b[36mINFO\u001b[0m[0906] Removing container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[1692.051415, "o", "\u001b[36mINFO\u001b[0m[0907] [remove/rke-log-linker] Successfully removed container on host [137.135.244.150] \r\n"]
[1692.453863, "o", "\u001b[36mINFO\u001b[0m[0907] Checking if container [kube-proxy] is running on host [137.135.244.150], try #1 \r\n"]
[1692.724673, "o", "\u001b[36mINFO\u001b[0m[0908] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0908] Checking if container [old-kube-proxy] is running on host [137.135.244.150], try #1 \r\n"]
[1692.865237, "o", "\u001b[36mINFO\u001b[0m[0908] Stopping container [kube-proxy] on host [137.135.244.150] with stopTimeoutDuration [5s], try #1 \r\n"]
[1693.843303, "o", "\u001b[36mINFO\u001b[0m[0909] Waiting for [kube-proxy] container to exit on host [137.135.244.150] \r\n"]
[1693.975182, "o", "\u001b[36mINFO\u001b[0m[0909] Renaming container [kube-proxy] to [old-kube-proxy] on host [137.135.244.150], try #1 \r\n"]
[1694.605207, "o", "\u001b[36mINFO\u001b[0m[0910] Starting container [kube-proxy] on host [137.135.244.150], try #1 \r\n"]
[1695.274218, "o", "\u001b[36mINFO\u001b[0m[0910] [worker] Successfully updated [kube-proxy] container on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0910] Removing container [old-kube-proxy] on host [137.135.244.150], try #1 \r\n"]
[1695.434216, "o", "\u001b[36mINFO\u001b[0m[0910] [healthcheck] Start Healthcheck on service [kube-proxy] on host [137.135.244.150] \r\n"]
[1698.10715, "o", "\u001b[36mINFO\u001b[0m[0913] [healthcheck] service [kube-proxy] on host [137.135.244.150] is healthy \r\n"]
[1698.417749, "o", "\u001b[36mINFO\u001b[0m[0913] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n"]
[1701.771221, "o", "\u001b[36mINFO\u001b[0m[0917] Starting container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[1703.057774, "o", "\u001b[36mINFO\u001b[0m[0918] [worker] Successfully started [rke-log-linker] container on host [137.135.244.150] \r\n"]
[1703.679173, "o", "\u001b[36mINFO\u001b[0m[0919] Removing container [rke-log-linker] on host [137.135.244.150], try #1 \r\n"]
[1703.794219, "o", "\u001b[36mINFO\u001b[0m[0919] [remove/rke-log-linker] Successfully removed container on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0919] [worker] Now checking status of node 137.135.244.150, try #1 \r\n"]
[1704.421251, "o", "\u001b[36mINFO\u001b[0m[0919] [workerplane] Processing host 23.102.23.168  \r\n\u001b[36mINFO\u001b[0m[0919] [worker] Now checking status of node 23.102.23.168, try #1 \r\n"]
[1704.61451, "o", "\u001b[36mINFO\u001b[0m[0920] [worker] Getting list of nodes for upgrade   \r\n"]
[1710.322694, "o", "\u001b[36mINFO\u001b[0m[0925] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[1711.508368, "o", "\u001b[36mINFO\u001b[0m[0927] Starting container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[1713.50255, "o", "\u001b[36mINFO\u001b[0m[0929] [worker] Successfully started [rke-log-linker] container on host [23.102.23.168] \r\n"]
[1713.632541, "o", "\u001b[36mINFO\u001b[0m[0929] Removing container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[1714.458068, "o", "\u001b[36mINFO\u001b[0m[0929] [remove/rke-log-linker] Successfully removed container on host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0929] Checking if container [service-sidekick] is running on host [23.102.23.168], try #1 \r\n"]
[1714.882927, "o", "\u001b[36mINFO\u001b[0m[0930] [sidekick] Sidekick container already created on host [23.102.23.168] \r\n"]
[1715.283242, "o", "\u001b[36mINFO\u001b[0m[0930] Checking if container [kubelet] is running on host [23.102.23.168], try #1 \r\n"]
[1715.494547, "o", "\u001b[36mINFO\u001b[0m[0930] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0930] Checking if container [old-kubelet] is running on host [23.102.23.168], try #1 \r\n"]
[1715.648423, "o", "\u001b[36mINFO\u001b[0m[0931] Stopping container [kubelet] on host [23.102.23.168] with stopTimeoutDuration [5s], try #1 \r\n"]
[1716.473475, "o", "\u001b[36mINFO\u001b[0m[0931] Waiting for [kubelet] container to exit on host [23.102.23.168] \r\n"]
[1716.610955, "o", "\u001b[36mINFO\u001b[0m[0932] Renaming container [kubelet] to [old-kubelet] on host [23.102.23.168], try #1 \r\n"]
[1717.085387, "o", "\u001b[36mINFO\u001b[0m[0932] Starting container [kubelet] on host [23.102.23.168], try #1 \r\n"]
[1718.10988, "o", "\u001b[36mINFO\u001b[0m[0933] [worker] Successfully updated [kubelet] container on host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0933] Removing container [old-kubelet] on host [23.102.23.168], try #1 \r\n"]
[1718.417037, "o", "\u001b[36mINFO\u001b[0m[0933] [healthcheck] Start Healthcheck on service [kubelet] on host [23.102.23.168] \r\n"]
[1727.809043, "o", "\u001b[36mINFO\u001b[0m[0943] [healthcheck] service [kubelet] on host [23.102.23.168] is healthy \r\n"]
[1728.157909, "o", "\u001b[36mINFO\u001b[0m[0943] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[1729.16835, "o", "\u001b[36mINFO\u001b[0m[0944] Starting container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[1731.217279, "o", "\u001b[36mINFO\u001b[0m[0946] [worker] Successfully started [rke-log-linker] container on host [23.102.23.168] \r\n"]
[1731.349027, "o", "\u001b[36mINFO\u001b[0m[0946] Removing container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[1732.111198, "o", "\u001b[36mINFO\u001b[0m[0947] [remove/rke-log-linker] Successfully removed container on host [23.102.23.168] \r\n"]
[1732.510814, "o", "\u001b[36mINFO\u001b[0m[0948] Checking if container [kube-proxy] is running on host [23.102.23.168], try #1 \r\n"]
[1732.782859, "o", "\u001b[36mINFO\u001b[0m[0948] Image [rancher/hyperkube:v1.19.4-rancher1] exists on host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0948] Checking if container [old-kube-proxy] is running on host [23.102.23.168], try #1 \r\n"]
[1732.944951, "o", "\u001b[36mINFO\u001b[0m[0948] Stopping container [kube-proxy] on host [23.102.23.168] with stopTimeoutDuration [5s], try #1 \r\n"]
[1733.777012, "o", "\u001b[36mINFO\u001b[0m[0949] Waiting for [kube-proxy] container to exit on host [23.102.23.168] \r\n"]
[1733.91149, "o", "\u001b[36mINFO\u001b[0m[0949] Renaming container [kube-proxy] to [old-kube-proxy] on host [23.102.23.168], try #1 \r\n"]
[1734.493629, "o", "\u001b[36mINFO\u001b[0m[0949] Starting container [kube-proxy] on host [23.102.23.168], try #1 \r\n"]
[1735.591083, "o", "\u001b[36mINFO\u001b[0m[0951] [worker] Successfully updated [kube-proxy] container on host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0951] Removing container [old-kube-proxy] on host [23.102.23.168], try #1 \r\n"]
[1735.824774, "o", "\u001b[36mINFO\u001b[0m[0951] [healthcheck] Start Healthcheck on service [kube-proxy] on host [23.102.23.168] \r\n"]
[1737.716046, "o", "\u001b[36mINFO\u001b[0m[0953] [healthcheck] service [kube-proxy] on host [23.102.23.168] is healthy \r\n"]
[1738.044172, "o", "\u001b[36mINFO\u001b[0m[0953] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[1739.101282, "o", "\u001b[36mINFO\u001b[0m[0954] Starting container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[1741.252265, "o", "\u001b[36mINFO\u001b[0m[0956] [worker] Successfully started [rke-log-linker] container on host [23.102.23.168] \r\n"]
[1742.278844, "o", "\u001b[36mINFO\u001b[0m[0957] Removing container [rke-log-linker] on host [23.102.23.168], try #1 \r\n"]
[1742.495565, "o", "\u001b[36mINFO\u001b[0m[0958] [remove/rke-log-linker] Successfully removed container on host [23.102.23.168] \r\n\u001b[36mINFO\u001b[0m[0958] [worker] Now checking status of node 23.102.23.168, try #1 \r\n"]
[1743.105654, "o", "\u001b[36mINFO\u001b[0m[0958] [worker] Successfully upgraded Worker Plane.. \r\n"]
[1743.386387, "o", "\u001b[36mINFO\u001b[0m[0958] Image [rancher/rke-tools:v0.1.66] exists on host [137.135.244.150] \r\n\u001b[36mINFO\u001b[0m[0958] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.23.168] \r\n"]
[1744.010078, "o", "\u001b[36mINFO\u001b[0m[0959] Starting container [rke-log-cleaner] on host [23.102.23.168], try #1 \r\n"]
[1744.96491, "o", "\u001b[36mINFO\u001b[0m[0960] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.86.4] \r\n"]
[1745.552677, "o", "\u001b[36mINFO\u001b[0m[0961] [cleanup] Successfully started [rke-log-cleaner] container on host [23.102.23.168] \r\n"]
[1745.699171, "o", "\u001b[36mINFO\u001b[0m[0961] Starting container [rke-log-cleaner] on host [104.45.86.4], try #1 \r\n"]
[1745.946028, "o", "\u001b[36mINFO\u001b[0m[0961] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.22.29] \r\n"]
[1746.361861, "o", "\u001b[36mINFO\u001b[0m[0961] Image [rancher/rke-tools:v0.1.66] exists on host [104.45.84.172] \r\n"]
[1746.524981, "o", "\u001b[36mINFO\u001b[0m[0962] Image [rancher/rke-tools:v0.1.66] exists on host [23.102.48.17] \r\n"]
[1746.577729, "o", "\u001b[36mINFO\u001b[0m[0962] Starting container [rke-log-cleaner] on host [137.135.244.150], try #1 \r\n"]
[1746.65068, "o", "\u001b[36mINFO\u001b[0m[0962] Removing container [rke-log-cleaner] on host [23.102.23.168], try #1 \r\n"]
[1746.741108, "o", "\u001b[36mINFO\u001b[0m[0962] Starting container [rke-log-cleaner] on host [23.102.22.29], try #1 \r\n"]
[1746.834985, "o", "\u001b[36mINFO\u001b[0m[0962] [remove/rke-log-cleaner] Successfully removed container on host [23.102.23.168] \r\n"]
[1746.868319, "o", "\u001b[36mINFO\u001b[0m[0962] Starting container [rke-log-cleaner] on host [104.45.84.172], try #1 \r\n"]
[1747.19128, "o", "\u001b[36mINFO\u001b[0m[0962] Starting container [rke-log-cleaner] on host [23.102.48.17], try #1 \r\n"]
[1747.230063, "o", "\u001b[36mINFO\u001b[0m[0962] [cleanup] Successfully started [rke-log-cleaner] container on host [104.45.86.4] \r\n"]
[1747.349241, "o", "\u001b[36mINFO\u001b[0m[0962] Removing container [rke-log-cleaner] on host [104.45.86.4], try #1 \r\n"]
[1747.80566, "o", "\u001b[36mINFO\u001b[0m[0963] [cleanup] Successfully started [rke-log-cleaner] container on host [137.135.244.150] \r\n"]
[1747.989068, "o", "\u001b[36mINFO\u001b[0m[0963] Removing container [rke-log-cleaner] on host [137.135.244.150], try #1 \r\n"]
[1748.147816, "o", "\u001b[36mINFO\u001b[0m[0963] [cleanup] Successfully started [rke-log-cleaner] container on host [23.102.22.29] \r\n"]
[1748.43792, "o", "\u001b[36mINFO\u001b[0m[0963] [cleanup] Successfully started [rke-log-cleaner] container on host [104.45.84.172] \r\n"]
[1748.56556, "o", "\u001b[36mINFO\u001b[0m[0964] Removing container [rke-log-cleaner] on host [104.45.84.172], try #1 \r\n"]
[1748.585565, "o", "\u001b[36mINFO\u001b[0m[0964] [remove/rke-log-cleaner] Successfully removed container on host [137.135.244.150] \r\n"]
[1748.616036, "o", "\u001b[36mINFO\u001b[0m[0964] [remove/rke-log-cleaner] Successfully removed container on host [104.45.86.4] \r\n"]
[1748.932223, "o", "\u001b[36mINFO\u001b[0m[0964] [cleanup] Successfully started [rke-log-cleaner] container on host [23.102.48.17] \r\n"]
[1749.001544, "o", "\u001b[36mINFO\u001b[0m[0964] Removing container [rke-log-cleaner] on host [23.102.22.29], try #1 \r\n"]
[1749.239132, "o", "\u001b[36mINFO\u001b[0m[0964] [remove/rke-log-cleaner] Successfully removed container on host [23.102.22.29] \r\n"]
[1749.308969, "o", "\u001b[36mINFO\u001b[0m[0964] [remove/rke-log-cleaner] Successfully removed container on host [104.45.84.172] \r\n"]
[1749.899538, "o", "\u001b[36mINFO\u001b[0m[0965] Removing container [rke-log-cleaner] on host [23.102.48.17], try #1 \r\n"]
[1750.160908, "o", "\u001b[36mINFO\u001b[0m[0965] [remove/rke-log-cleaner] Successfully removed container on host [23.102.48.17] \r\n\u001b[36mINFO\u001b[0m[0965] [sync] Syncing nodes Labels and Taints       \r\n"]
[1750.593935, "o", "\u001b[36mINFO\u001b[0m[0966] [sync] Successfully synced nodes Labels and Taints \r\n"]
[1750.594276, "o", "\u001b[36mINFO\u001b[0m[0966] [network] Setting up network plugin: calico  \r\n"]
[1750.597464, "o", "\u001b[36mINFO\u001b[0m[0966] [addons] Saving ConfigMap for addon rke-network-plugin to Kubernetes \r\n"]
[1752.884944, "o", "\u001b[36mINFO\u001b[0m[0968] [addons] Successfully saved ConfigMap for addon rke-network-plugin to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0968] [addons] Executing deploy job rke-network-plugin \r\n"]
[1779.59799, "o", "\u001b[36mINFO\u001b[0m[0995] [addons] Setting up coredns                  \r\n"]
[1779.598274, "o", "\u001b[36mINFO\u001b[0m[0995] [addons] Saving ConfigMap for addon rke-coredns-addon to Kubernetes \r\n"]
[1779.812912, "o", "\u001b[36mINFO\u001b[0m[0995] [addons] Successfully saved ConfigMap for addon rke-coredns-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0995] [addons] Executing deploy job rke-coredns-addon \r\n"]
[1795.923743, "o", "\u001b[36mINFO\u001b[0m[1011] [addons] CoreDNS deployed successfully       \r\n\u001b[36mINFO\u001b[0m[1011] [dns] DNS provider coredns deployed successfully \r\n"]
[1796.04798, "o", "\u001b[36mINFO\u001b[0m[1011] [addons] Setting up Metrics Server           \r\n"]
[1796.04887, "o", "\u001b[36mINFO\u001b[0m[1011] [addons] Saving ConfigMap for addon rke-metrics-addon to Kubernetes \r\n"]
[1796.142332, "o", "\u001b[36mINFO\u001b[0m[1011] [addons] Successfully saved ConfigMap for addon rke-metrics-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[1011] [addons] Executing deploy job rke-metrics-addon \r\n"]
[1796.565537, "o", "\u001b[36mINFO\u001b[0m[1012] [addons] Metrics Server deployed successfully \r\n\u001b[36mINFO\u001b[0m[1012] [ingress] Setting up nginx ingress controller \r\n"]
[1796.566065, "o", "\u001b[36mINFO\u001b[0m[1012] [addons] Saving ConfigMap for addon rke-ingress-controller to Kubernetes \r\n"]
[1797.03976, "o", "\u001b[36mINFO\u001b[0m[1012] [addons] Successfully saved ConfigMap for addon rke-ingress-controller to Kubernetes \r\n\u001b[36mINFO\u001b[0m[1012] [addons] Executing deploy job rke-ingress-controller \r\n"]
[1799.132039, "o", "\u001b[36mINFO\u001b[0m[1014] [ingress] ingress controller nginx deployed successfully \r\n\u001b[36mINFO\u001b[0m[1014] [addons] Setting up user addons              \r\n"]
[1799.277587, "o", "\u001b[36mINFO\u001b[0m[1014] [addons] no user addons defined              \r\n\u001b[36mINFO\u001b[0m[1014] Finished building Kubernetes cluster successfully \r\n"]
[1799.280618, "o", "\r\nreal\t16m55.058s\r\nuser\t0m3.097s\r\nsys\t0m1.597s\r\n"]
[1832.261732, "o", "NAME                   STATUS   ROLES               AGE   VERSION\r\nnode/104.45.84.172     Ready    controlplane,etcd   "]
[1832.262068, "o", "20m   v1.19.4\r\nnode/104.45.86.4       Ready    controlplane,etcd   20m   v1.19.4\r\nnode/137.135.244.150   Ready    worker              20m   v1.19.4\r\nnode/23.102.22.29"]
[1832.262328, "o", "      Ready    worker              20m   v1.19.4\r\nnode/23.102.23.168     Ready    worker              20m   v1.19.4\r\nnode/23.102.48.17      Ready    controlplane,etcd   20m   v1.19.4\r\n"]
[1832.291655, "o", "\r\nNAMESPACE       NAME                                          READY   STATUS              RESTARTS   AGE\r\ningress-nginx   pod/default-http-backend-598b7d7dbd-zhmc8     1/1     "]
[1832.291793, "o", "Running             0          18m\r\ningress-nginx   pod/nginx-ingress-controller-2fh76            1/1     Running             0          18m\r\ningress-nginx   pod/nginx-ingress-controller-4jrjv            1/1     Running             0          18m\r\n"]
[1832.292136, "o", "ingress-nginx   pod/nginx-ingress-controller-p9kd5            1/1     Running             0          18m\r\nkube-system     pod/calico-kube-controllers-87d89ff98-vq9bs   0/1     ContainerCreating   0          56s\r\nkube-system     pod/calico-node-2mgph                         1/1     Running             0          19m"]
[1832.29302, "o", "\r\nkube-system     pod/calico-node-7n2r7                         1/1     Running             0          19m\r\nkube-system     pod/calico-node-dttns                         1/1     Running             0          19m\r\nkube-system     pod/calico-node-kswdv                         0/1     PodInitializing     0          59s\r\nkube-system     pod/calico-node-lnbbc                         1/1     Running             1          19m\r\nkube-system     pod/calico-node-wb5z7                         1/1     Running             0          19m\r\nkube-system     pod/coredns-6f85d5fb88-cfjq7                  0/1     ContainerCreating   0          36s\r\nkube-system     pod/coredns-849545576b-7gw77                  1/1     Running             0          12m\r\nkube-system     pod/coredns-autoscaler-5dcd676cbd-kbqh9       1/1     Running             0          18m\r\nkube-system     pod/coredns-autoscaler-79599b9dc6-bmfgg       0/1     ContainerCreating   0          38s\r\nkube-system     pod/metrics-server-697746ff48-jqzqt           1/1   "]
[1832.293342, "o", "  Running             0          18m\r\nkube-system     pod/rke-coredns-addon-deploy-job-fdjr6        0/1     Completed           0          46s\r\nkube-system     pod/rke-ingress-controller-deploy-job-wsf8d   0/1     Completed           0          18m\r\nkube-system     pod/rke-metrics-addon-deploy-job-7xbdd        0/1     Completed           0          18m\r\nkube-system     pod/rke-network-plugin-deploy-job-h7lrn       0/1     Completed           0          73s\r\n"]
