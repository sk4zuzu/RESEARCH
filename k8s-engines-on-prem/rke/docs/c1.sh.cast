{"version": 2, "width": 191, "height": 48, "timestamp": 1607173676, "env": {"SHELL": "/run/current-system/sw/bin/bash", "TERM": "xterm-256color"}}
[0.289512, "o", "cd /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ && ansible-playbook -vv --inventory=/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/c1.ini prereq.yml\r\n"]
[3.380012, "o", "\u001b[0;34mansible-playbook 2.10.3\u001b[0m\r\n\u001b[0;34m  config file = /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ansible.cfg\u001b[0m\r\n\u001b[0;34m  configured module search path = ['/home/asd/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\u001b[0m\r\n\u001b[0;34m  ansible python module location = /home/asd/3.7/lib/python3.7/site-packages/ansible\u001b[0m\r\n\u001b[0;34m  executable location = /home/asd/3.7/bin/ansible-playbook\u001b[0m\r\n\u001b[0;34m  python version = 3.7.8 (default, Jun 27 2020, 09:38:56) [GCC 9.3.0]\u001b[0m\r\n\u001b[0;34mUsing /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ansible.cfg as config file\u001b[0m\r\n"]
[3.78536, "o", "\u001b[0;34mredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\u001b[0m\r\n"]
[3.874893, "o", "\u001b[0;34mredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\u001b[0m\r\n"]
[3.913428, "o", "\u001b[0;34mSkipping callback 'default', as we already have a stdout callback.\u001b[0m\r\n\u001b[0;34mSkipping callback 'minimal', as we already have a stdout callback.\u001b[0m\r\n\u001b[0;34mSkipping callback 'oneline', as we already have a stdout callback.\u001b[0m\r\n"]
[3.913569, "o", "\r\nPLAYBOOK: prereq.yml **************************************************************************************************************************************************************************\r\n\u001b[0;34m1 plays in prereq.yml\u001b[0m\r\n"]
[3.915798, "o", "\r\nPLAY [m:n] ************************************************************************************************************************************************************************************\r\n"]
[3.9299, "o", "\r\nTASK [Gathering Facts] ************************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/prereq.yml:2\u001b[0m\r\n"]
[5.321107, "o", "\u001b[0;32mok: [c1a3]\u001b[0m\r\n"]
[5.327344, "o", "\u001b[0;32mok: [c1a2]\u001b[0m\r\n"]
[5.369527, "o", "\u001b[0;32mok: [c1a1]\u001b[0m\r\n"]
[5.37644, "o", "\u001b[0;32mok: [c1b2]\u001b[0m\r\n"]
[6.334261, "o", "\u001b[0;32mok: [c1b3]\u001b[0m\r\n"]
[6.341467, "o", "\u001b[0;32mok: [c1b1]\u001b[0m\r\n"]
[6.345864, "o", "\u001b[0;34mMETA: ran handlers\u001b[0m\r\n"]
[6.350815, "o", "\r\nTASK [modprobe : shell] ***********************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/modprobe/tasks/main.yml:2\u001b[0m\r\n"]
[7.169072, "o", "\u001b[0;33mchanged: [c1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.412638'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:02.998129'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:02.585491'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[7.170868, "o", "\u001b[0;33mchanged: [c1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.403208'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:02.981088'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:02.577880'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[7.171898, "o", "\u001b[0;33mchanged: [c1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.377234'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:03.055403'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:02.678169'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[7.172884, "o", "\u001b[0;33mchanged: [c1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.413746'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:02.984446'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:02.570700'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[7.173872, "o", "\u001b[0;33mchanged: [c1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.374910'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:02.940079'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:02.565169'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[7.507425, "o", "\u001b[0;33mchanged: [c1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.125323'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:03.357362'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:03.232039'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[7.511905, "o", "\r\nTASK [sysctl : shell] *************************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/sysctl/tasks/main.yml:2\u001b[0m\r\n"]
[7.791998, "o", "\u001b[0;33mchanged: [c1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.004879'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:03.691735'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:03.686856'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[7.821193, "o", "\u001b[0;33mchanged: [c1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.005227'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:03.644533'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:03.639306'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[7.831234, "o", "\u001b[0;33mchanged: [c1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.005397'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:03.670452'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:03.665055'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[7.834381, "o", "\u001b[0;33mchanged: [c1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.008042'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:03.662189'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:03.654147'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[7.84136, "o", "\u001b[0;33mchanged: [c1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.008457'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:03.624598'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:03.616141'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[8.035257, "o", "\u001b[0;33mchanged: [c1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.004260'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 13:08:03.878228'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 13:08:03.873968'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[8.040239, "o", "\r\nTASK [docker : include_tasks] *****************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/main.yml:2\u001b[0m\r\n"]
[8.126703, "o", "\u001b[0;36mincluded: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/redhat.yml for c1a1, c1a2, c1a3, c1b1, c1b2, c1b3\u001b[0m\r\n"]
[8.136066, "o", "\r\nTASK [docker : yum_repository] ****************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/redhat.yml:2\u001b[0m\r\n"]
[8.594748, "o", "\u001b[0;33mchanged: [c1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: docker-ce-stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[8.597878, "o", "\u001b[0;33mchanged: [c1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: docker-ce-stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[8.601391, "o", "\u001b[0;33mchanged: [c1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: docker-ce-stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[8.609573, "o", "\u001b[0;33mchanged: [c1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: docker-ce-stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[8.623002, "o", "\u001b[0;33mchanged: [c1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: docker-ce-stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[8.864589, "o", "\u001b[0;33mchanged: [c1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: docker-ce-stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[8.870936, "o", "\r\nTASK [docker : yum] ***************************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/redhat.yml:13\u001b[0m\r\n"]
[195.192839, "o", "\u001b[0;33mchanged: [c1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  changes:\u001b[0m\r\n\u001b[0;33m    installed:\u001b[0m\r\n\u001b[0;33m    - docker-ce-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - docker-ce-cli-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - containerd.io-1.3.9-3.1.el7\u001b[0m\r\n\u001b[0;33m  msg: ''\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  results:\u001b[0m\r\n\u001b[0;33m  - |-\u001b[0m\r\n\u001b[0;33m    Loaded plugins: fastestmirror\u001b[0m\r\n\u001b[0;33m    Loading mirror speeds from cached hostfile\u001b[0m\r\n\u001b[0;33m     * base: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * epel: epel.ip-connect.info\u001b[0m\r\n\u001b[0;33m     * extras: ftp.man.poznan.pl\u001b[0m\r\n\u001b[0;33m     * updates: centos2.hti.pl\u001b[0m\r\n\u001b[0;33m    Resolving Dependencies\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package containerd.io.x86_64 0:1.3.9-3.1.el7 will be installed\u001b[0m\r\n\u001b[0;33m    --> Processing Dependency: container-selinux >= 2:2.74 for package: containerd.io-1.3.9-3.1.el7.x86_64\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce.x86_64 3:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce-c"]
[195.192898, "o", "li.x86_64 1:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package container-selinux.noarch 2:2.119.2-1.911c772.el7_8 will be installed\u001b[0m\r\n\u001b[0;33m    --> Finished Dependency Resolution\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependencies Resolved\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m     Package            Arch    Version                     Repository         Size\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Installing:\u001b[0m\r\n\u001b[0;33m     containerd.io      x86_64  1.3.9-3.1.el7               docker-ce-stable   29 M\u001b[0m\r\n\u001b[0;33m     docker-ce          x86_64  3:19.03.14-3.el7            docker-ce-stable   24 M\u001b[0m\r\n\u001b[0;33m     docker-ce-cli      x86_64  1:19.03.14-3.el7            docker-ce-stable   38 M\u001b[0m\r\n\u001b[0;33m    Installing for dependencies:\u001b[0m\r\n\u001b[0;33m     container-selinux  noarch  2:2.119.2-1.911c772.e"]
[195.193122, "o", "l7_8   extras             40 k\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Transaction Summary\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Install  3 Packages (+1 Dependent package)\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Total download size: 92 M\u001b[0m\r\n\u001b[0;33m    Installed size: 388 M\u001b[0m\r\n\u001b[0;33m    Downloading packages:\u001b[0m\r\n\u001b[0;33m    --------------------------------------------------------------------------------\u001b[0m\r\n\u001b[0;33m    Total                                              780 kB/s |  92 MB  02:00\u001b[0m\r\n\u001b[0;33m    Running transaction check\u001b[0m\r\n\u001b[0;33m    Running transaction test\u001b[0m\r\n\u001b[0;33m    Transaction test succeeded\u001b[0m\r\n\u001b[0;33m    Running transaction\u001b[0m\r\n\u001b[0;33m      Installing : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Installing : containerd.io-1.3.9-3.1.el7.x86_64                           2/4\u001b[0m\r\n\u001b[0;33m      Installing : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        3/4\u001b[0m\r\n\u001b[0;33m  "]
[195.193153, "o", "    Installing : 3:docker-ce-19.03.14-3.el7.x86_64                            4/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 3:docker-ce-19.03.14-3.el7.x86_64                            2/4\u001b[0m\r\n\u001b[0;33m      Verifying  : containerd.io-1.3.9-3.1.el7.x86_64                           3/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        4/4\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Installed:\u001b[0m\r\n\u001b[0;33m      containerd.io.x86_64 0:1.3.9-3.1.el7     docker-ce.x86_64 3:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m      docker-ce-cli.x86_64 1:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependency Installed:\u001b[0m\r\n\u001b[0;33m      container-selinux.noarch 2:2.119.2-1.911c772.el7_8\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Complete!\u001b[0m\r\n"]
[198.633741, "o", "\u001b[0;33mchanged: [c1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  changes:\u001b[0m\r\n\u001b[0;33m    installed:\u001b[0m\r\n\u001b[0;33m    - docker-ce-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - docker-ce-cli-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - containerd.io-1.3.9-3.1.el7\u001b[0m\r\n\u001b[0;33m  msg: ''\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  results:\u001b[0m\r\n\u001b[0;33m  - |-\u001b[0m\r\n\u001b[0;33m    Loaded plugins: fastestmirror\u001b[0m\r\n\u001b[0;33m    Loading mirror speeds from cached hostfile\u001b[0m\r\n\u001b[0;33m     * base: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * epel: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * extras: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * updates: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m    Resolving Dependencies\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package containerd.io.x86_64 0:1.3.9-3.1.el7 will be installed\u001b[0m\r\n\u001b[0;33m    --> Processing Dependency: container-selinux >= 2:2.74 for package: containerd.io-1.3.9-3.1.el7.x86_64\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce.x86_64 3:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce-cli.x86_64"]
[198.633774, "o", " 1:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package container-selinux.noarch 2:2.119.2-1.911c772.el7_8 will be installed\u001b[0m\r\n\u001b[0;33m    --> Finished Dependency Resolution\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependencies Resolved\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m     Package            Arch    Version                     Repository         Size\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Installing:\u001b[0m\r\n\u001b[0;33m     containerd.io      x86_64  1.3.9-3.1.el7               docker-ce-stable   29 M\u001b[0m\r\n\u001b[0;33m     docker-ce          x86_64  3:19.03.14-3.el7            docker-ce-stable   24 M\u001b[0m\r\n\u001b[0;33m     docker-ce-cli      x86_64  1:19.03.14-3.el7            docker-ce-stable   38 M\u001b[0m\r\n\u001b[0;33m    Installing for dependencies:\u001b[0m\r\n\u001b[0;33m     container-selinux  noarch  2:2.119.2-1.911c772.el7_8   ex"]
[198.633782, "o", "tras             40 k\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Transaction Summary\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Install  3 Packages (+1 Dependent package)\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Total download size: 92 M\u001b[0m\r\n\u001b[0;33m    Installed size: 388 M\u001b[0m\r\n\u001b[0;33m    Downloading packages:\u001b[0m\r\n\u001b[0;33m    --------------------------------------------------------------------------------\u001b[0m\r\n\u001b[0;33m    Total                                              753 kB/s |  92 MB  02:04\u001b[0m\r\n\u001b[0;33m    Running transaction check\u001b[0m\r\n\u001b[0;33m    Running transaction test\u001b[0m\r\n\u001b[0;33m    Transaction test succeeded\u001b[0m\r\n\u001b[0;33m    Running transaction\u001b[0m\r\n\u001b[0;33m      Installing : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Installing : containerd.io-1.3.9-3.1.el7.x86_64                           2/4\u001b[0m\r\n\u001b[0;33m      Installing : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        3/4\u001b[0m\r\n\u001b[0;33m      Insta"]
[198.633793, "o", "lling : 3:docker-ce-19.03.14-3.el7.x86_64                            4/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 3:docker-ce-19.03.14-3.el7.x86_64                            2/4\u001b[0m\r\n\u001b[0;33m      Verifying  : containerd.io-1.3.9-3.1.el7.x86_64                           3/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        4/4\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Installed:\u001b[0m\r\n\u001b[0;33m      containerd.io.x86_64 0:1.3.9-3.1.el7     docker-ce.x86_64 3:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m      docker-ce-cli.x86_64 1:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependency Installed:\u001b[0m\r\n\u001b[0;33m      container-selinux.noarch 2:2.119.2-1.911c772.el7_8\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Complete!\u001b[0m\r\n"]
[198.638783, "o", "\u001b[0;33mchanged: [c1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  changes:\u001b[0m\r\n\u001b[0;33m    installed:\u001b[0m\r\n\u001b[0;33m    - docker-ce-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - docker-ce-cli-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - containerd.io-1.3.9-3.1.el7\u001b[0m\r\n\u001b[0;33m  msg: ''\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  results:\u001b[0m\r\n\u001b[0;33m  - |-\u001b[0m\r\n\u001b[0;33m    Loaded plugins: fastestmirror\u001b[0m\r\n\u001b[0;33m    Loading mirror speeds from cached hostfile\u001b[0m\r\n\u001b[0;33m     * base: centos.hitme.net.pl\u001b[0m\r\n\u001b[0;33m     * epel: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * extras: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * updates: centos.hitme.net.pl\u001b[0m\r\n\u001b[0;33m    Resolving Dependencies\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package containerd.io.x86_64 0:1.3.9-3.1.el7 will be installed\u001b[0m\r\n\u001b[0;33m    --> Processing Dependency: container-selinux >= 2:2.74 for package: containerd.io-1.3.9-3.1.el7.x86_64\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce.x86_64 3:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce-"]
[198.638816, "o", "cli.x86_64 1:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package container-selinux.noarch 2:2.119.2-1.911c772.el7_8 will be installed\u001b[0m\r\n\u001b[0;33m    --> Finished Dependency Resolution\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependencies Resolved\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m     Package            Arch    Version                     Repository         Size\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Installing:\u001b[0m\r\n\u001b[0;33m     containerd.io      x86_64  1.3.9-3.1.el7               docker-ce-stable   29 M\u001b[0m\r\n\u001b[0;33m     docker-ce          x86_64  3:19.03.14-3.el7            docker-ce-stable   24 M\u001b[0m\r\n\u001b[0;33m     docker-ce-cli      x86_64  1:19.03.14-3.el7            docker-ce-stable   38 M\u001b[0m\r\n\u001b[0;33m    Installing for dependencies:\u001b[0m\r\n\u001b[0;33m     container-selinux  noarch  2:2.119.2-1.911c772."]
[198.638823, "o", "el7_8   extras             40 k\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Transaction Summary\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Install  3 Packages (+1 Dependent package)\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Total download size: 92 M\u001b[0m\r\n\u001b[0;33m    Installed size: 388 M\u001b[0m\r\n\u001b[0;33m    Downloading packages:\u001b[0m\r\n\u001b[0;33m    --------------------------------------------------------------------------------\u001b[0m\r\n\u001b[0;33m    Total                                              737 kB/s |  92 MB  02:07\u001b[0m\r\n\u001b[0;33m    Running transaction check\u001b[0m\r\n\u001b[0;33m    Running transaction test\u001b[0m\r\n\u001b[0;33m    Transaction test succeeded\u001b[0m\r\n\u001b[0;33m    Running transaction\u001b[0m\r\n\u001b[0;33m      Installing : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Installing : containerd.io-1.3.9-3.1.el7.x86_64                           2/4\u001b[0m\r\n\u001b[0;33m      Installing : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        3/4\u001b[0m\r\n\u001b[0;33m "]
[198.63883, "o", "     Installing : 3:docker-ce-19.03.14-3.el7.x86_64                            4/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 3:docker-ce-19.03.14-3.el7.x86_64                            2/4\u001b[0m\r\n\u001b[0;33m      Verifying  : containerd.io-1.3.9-3.1.el7.x86_64                           3/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        4/4\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Installed:\u001b[0m\r\n\u001b[0;33m      containerd.io.x86_64 0:1.3.9-3.1.el7     docker-ce.x86_64 3:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m      docker-ce-cli.x86_64 1:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependency Installed:\u001b[0m\r\n\u001b[0;33m      container-selinux.noarch 2:2.119.2-1.911c772.el7_8\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Complete!\u001b[0m\r\n"]
[215.361627, "o", "\u001b[1;30mFAILED - RETRYING: docker : yum (8 retries left).\u001b[0m\r\n"]
[269.762871, "o", "\u001b[1;30mFAILED - RETRYING: docker : yum (8 retries left).\u001b[0m\r\n"]
[289.312026, "o", "\u001b[0;33mchanged: [c1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 2\u001b[0m\r\n\u001b[0;33m  changes:\u001b[0m\r\n\u001b[0;33m    installed:\u001b[0m\r\n\u001b[0;33m    - docker-ce-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - docker-ce-cli-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - containerd.io-1.3.9-3.1.el7\u001b[0m\r\n\u001b[0;33m  msg: ''\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  results:\u001b[0m\r\n\u001b[0;33m  - |-\u001b[0m\r\n\u001b[0;33m    Loaded plugins: fastestmirror\u001b[0m\r\n\u001b[0;33m    Loading mirror speeds from cached hostfile\u001b[0m\r\n\u001b[0;33m     * base: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * epel: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * extras: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * updates: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m    Resolving Dependencies\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package containerd.io.x86_64 0:1.3.9-3.1.el7 will be installed\u001b[0m\r\n\u001b[0;33m    --> Processing Dependency: container-selinux >= 2:2.74 for package: containerd.io-1.3.9-3.1.el7.x86_64\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce.x86_64 3:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce-cli.x86_64"]
[289.312059, "o", " 1:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package container-selinux.noarch 2:2.119.2-1.911c772.el7_8 will be installed\u001b[0m\r\n\u001b[0;33m    --> Finished Dependency Resolution\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependencies Resolved\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m     Package            Arch    Version                     Repository         Size\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Installing:\u001b[0m\r\n\u001b[0;33m     containerd.io      x86_64  1.3.9-3.1.el7               docker-ce-stable   29 M\u001b[0m\r\n\u001b[0;33m     docker-ce          x86_64  3:19.03.14-3.el7            docker-ce-stable   24 M\u001b[0m\r\n\u001b[0;33m     docker-ce-cli      x86_64  1:19.03.14-3.el7            docker-ce-stable   38 M\u001b[0m\r\n\u001b[0;33m    Installing for dependencies:\u001b[0m\r\n\u001b[0;33m     container-selinux  noarch  2:2.119.2-1.911c772.el7_8   ex"]
[289.312066, "o", "tras             40 k\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Transaction Summary\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Install  3 Packages (+1 Dependent package)\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Total size: 92 M\u001b[0m\r\n\u001b[0;33m    Total download size: 53 M\u001b[0m\r\n\u001b[0;33m    Installed size: 388 M\u001b[0m\r\n\u001b[0;33m    Downloading packages:\u001b[0m\r\n\u001b[0;33m    Delta RPMs disabled because /usr/bin/applydeltarpm not installed.\u001b[0m\r\n\u001b[0;33m    --------------------------------------------------------------------------------\u001b[0m\r\n\u001b[0;33m    Total                                              1.3 MB/s |  53 MB  00:41\u001b[0m\r\n\u001b[0;33m    Running transaction check\u001b[0m\r\n\u001b[0;33m    Running transaction test\u001b[0m\r\n\u001b[0;33m    Transaction test succeeded\u001b[0m\r\n\u001b[0;33m    Running transaction\u001b[0m\r\n\u001b[0;33m      Installing : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Installing : containerd.io-1.3.9-3.1.el7.x86_64                           2/4\u001b[0m\r"]
[289.312073, "o", "\n\u001b[0;33m      Installing : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        3/4\u001b[0m\r\n\u001b[0;33m      Installing : 3:docker-ce-19.03.14-3.el7.x86_64                            4/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 3:docker-ce-19.03.14-3.el7.x86_64                            2/4\u001b[0m\r\n\u001b[0;33m      Verifying  : containerd.io-1.3.9-3.1.el7.x86_64                           3/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        4/4\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Installed:\u001b[0m\r\n\u001b[0;33m      containerd.io.x86_64 0:1.3.9-3.1.el7     docker-ce.x86_64 3:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m      docker-ce-cli.x86_64 1:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependency Installed:\u001b[0m\r\n\u001b[0;33m      container-selinux.noarch 2:2.119.2-1.911c772.el7_8\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Complete!\u001b[0m\r\n"]
[338.375332, "o", "\u001b[0;33mchanged: [c1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 2\u001b[0m\r\n\u001b[0;33m  changes:\u001b[0m\r\n\u001b[0;33m    installed:\u001b[0m\r\n\u001b[0;33m    - docker-ce-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - docker-ce-cli-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - containerd.io-1.3.9-3.1.el7\u001b[0m\r\n\u001b[0;33m  msg: ''\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  results:\u001b[0m\r\n\u001b[0;33m  - |-\u001b[0m\r\n\u001b[0;33m    Loaded plugins: fastestmirror\u001b[0m\r\n\u001b[0;33m    Loading mirror speeds from cached hostfile\u001b[0m\r\n\u001b[0;33m     * base: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * epel: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * extras: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * updates: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m    Resolving Dependencies\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package containerd.io.x86_64 0:1.3.9-3.1.el7 will be installed\u001b[0m\r\n\u001b[0;33m    --> Processing Dependency: container-selinux >= 2:2.74 for package: containerd.io-1.3.9-3.1.el7.x86_64\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce.x86_64 3:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce-cli.x86_64"]
[338.375368, "o", " 1:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package container-selinux.noarch 2:2.119.2-1.911c772.el7_8 will be installed\u001b[0m\r\n\u001b[0;33m    --> Finished Dependency Resolution\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependencies Resolved\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m     Package            Arch    Version                     Repository         Size\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Installing:\u001b[0m\r\n\u001b[0;33m     containerd.io      x86_64  1.3.9-3.1.el7               docker-ce-stable   29 M\u001b[0m\r\n\u001b[0;33m     docker-ce          x86_64  3:19.03.14-3.el7            docker-ce-stable   24 M\u001b[0m\r\n\u001b[0;33m     docker-ce-cli      x86_64  1:19.03.14-3.el7            docker-ce-stable   38 M\u001b[0m\r\n\u001b[0;33m    Installing for dependencies:\u001b[0m\r\n\u001b[0;33m     container-selinux  noarch  2:2.119.2-1.911c772.el7_8   ex"]
[338.375376, "o", "tras             40 k\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Transaction Summary\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Install  3 Packages (+1 Dependent package)\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Total size: 92 M\u001b[0m\r\n\u001b[0;33m    Total download size: 24 M\u001b[0m\r\n\u001b[0;33m    Installed size: 388 M\u001b[0m\r\n\u001b[0;33m    Downloading packages:\u001b[0m\r\n\u001b[0;33m    Delta RPMs disabled because /usr/bin/applydeltarpm not installed.\u001b[0m\r\n\u001b[0;33m    Running transaction check\u001b[0m\r\n\u001b[0;33m    Running transaction test\u001b[0m\r\n\u001b[0;33m    Transaction test succeeded\u001b[0m\r\n\u001b[0;33m    Running transaction\u001b[0m\r\n\u001b[0;33m      Installing : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Installing : containerd.io-1.3.9-3.1.el7.x86_64                           2/4\u001b[0m\r\n\u001b[0;33m      Installing : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        3/4\u001b[0m\r\n\u001b[0;33m      Installing : 3:docker-ce-19.03.14-3.el7.x86_64                            4/4\u001b["]
[338.375471, "o", "0m\r\n\u001b[0;33m      Verifying  : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 3:docker-ce-19.03.14-3.el7.x86_64                            2/4\u001b[0m\r\n\u001b[0;33m      Verifying  : containerd.io-1.3.9-3.1.el7.x86_64                           3/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        4/4\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Installed:\u001b[0m\r\n\u001b[0;33m      containerd.io.x86_64 0:1.3.9-3.1.el7     docker-ce.x86_64 3:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m      docker-ce-cli.x86_64 1:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependency Installed:\u001b[0m\r\n\u001b[0;33m      container-selinux.noarch 2:2.119.2-1.911c772.el7_8\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Complete!\u001b[0m\r\n"]
[343.307797, "o", "\u001b[0;33mchanged: [c1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  changes:\u001b[0m\r\n\u001b[0;33m    installed:\u001b[0m\r\n\u001b[0;33m    - docker-ce-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - docker-ce-cli-19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m    - containerd.io-1.3.9-3.1.el7\u001b[0m\r\n\u001b[0;33m  msg: |-\u001b[0m\r\n\u001b[0;33m    http://ftp.man.poznan.pl/pub/centos/7.9.2009/extras/x86_64/Packages/container-selinux-2.119.2-1.911c772.el7_8.noarch.rpm: [Errno 14] curl#6 - \"Could not resolve host: ftp.man.poznan.pl; Unknown error\"\u001b[0m\r\n\u001b[0;33m    Trying other mirror.\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  results:\u001b[0m\r\n\u001b[0;33m  - |-\u001b[0m\r\n\u001b[0;33m    Loaded plugins: fastestmirror\u001b[0m\r\n\u001b[0;33m    Loading mirror speeds from cached hostfile\u001b[0m\r\n\u001b[0;33m     * base: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * epel: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * extras: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m     * updates: ftp.icm.edu.pl\u001b[0m\r\n\u001b[0;33m    Resolving Dependencies\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package containerd.io.x86_64 0:1.3.9-3.1.el7 will be insta"]
[343.307829, "o", "lled\u001b[0m\r\n\u001b[0;33m    --> Processing Dependency: container-selinux >= 2:2.74 for package: containerd.io-1.3.9-3.1.el7.x86_64\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce.x86_64 3:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    ---> Package docker-ce-cli.x86_64 1:19.03.14-3.el7 will be installed\u001b[0m\r\n\u001b[0;33m    --> Running transaction check\u001b[0m\r\n\u001b[0;33m    ---> Package container-selinux.noarch 2:2.119.2-1.911c772.el7_8 will be installed\u001b[0m\r\n\u001b[0;33m    --> Finished Dependency Resolution\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependencies Resolved\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m     Package            Arch    Version                     Repository         Size\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Installing:\u001b[0m\r\n\u001b[0;33m     containerd.io      x86_64  1.3.9-3.1.el7               docker-ce-stable   29 M\u001b[0m\r\n\u001b[0;33m     docker-ce          x86_64  3:19.03.14-3."]
[343.307839, "o", "el7            docker-ce-stable   24 M\u001b[0m\r\n\u001b[0;33m     docker-ce-cli      x86_64  1:19.03.14-3.el7            docker-ce-stable   38 M\u001b[0m\r\n\u001b[0;33m    Installing for dependencies:\u001b[0m\r\n\u001b[0;33m     container-selinux  noarch  2:2.119.2-1.911c772.el7_8   extras             40 k\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Transaction Summary\u001b[0m\r\n\u001b[0;33m    ================================================================================\u001b[0m\r\n\u001b[0;33m    Install  3 Packages (+1 Dependent package)\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Total download size: 92 M\u001b[0m\r\n\u001b[0;33m    Installed size: 388 M\u001b[0m\r\n\u001b[0;33m    Downloading packages:\u001b[0m\r\n\u001b[0;33m    --------------------------------------------------------------------------------\u001b[0m\r\n\u001b[0;33m    Total                                              1.2 MB/s |  92 MB  01:15\u001b[0m\r\n\u001b[0;33m    Running transaction check\u001b[0m\r\n\u001b[0;33m    Running transaction test\u001b[0m\r\n\u001b[0;33m    Transaction test succeeded\u001b[0m\r\n\u001b[0;33m    Running transaction\u001b[0m\r\n\u001b[0;33m      Installing : 2:container-selinux-2.119."]
[343.307848, "o", "2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Installing : containerd.io-1.3.9-3.1.el7.x86_64                           2/4\u001b[0m\r\n\u001b[0;33m      Installing : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        3/4\u001b[0m\r\n\u001b[0;33m      Installing : 3:docker-ce-19.03.14-3.el7.x86_64                            4/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch           1/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 3:docker-ce-19.03.14-3.el7.x86_64                            2/4\u001b[0m\r\n\u001b[0;33m      Verifying  : containerd.io-1.3.9-3.1.el7.x86_64                           3/4\u001b[0m\r\n\u001b[0;33m      Verifying  : 1:docker-ce-cli-19.03.14-3.el7.x86_64                        4/4\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Installed:\u001b[0m\r\n\u001b[0;33m      containerd.io.x86_64 0:1.3.9-3.1.el7     docker-ce.x86_64 3:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m      docker-ce-cli.x86_64 1:19.03.14-3.el7\u001b[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Dependency Installed:\u001b[0m\r\n\u001b[0;33m      container-selinux.noarch 2:2.119.2-1.911c772.el7_8\u001b"]
[343.307945, "o", "[0m\r\n\u001b[0;33m  \u001b[0m\r\n\u001b[0;33m    Complete!\u001b[0m\r\n"]
[343.313076, "o", "\r\nTASK [docker : user] **************************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/redhat.yml:25\u001b[0m\r\n"]
[343.830372, "o", "\u001b[0;33mchanged: [c1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Cloud User\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/centos\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: centos\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[343.833143, "o", "\u001b[0;33mchanged: [c1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Cloud User\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/centos\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: centos\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[343.94211, "o", "\u001b[0;33mchanged: [c1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Cloud User\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/centos\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: centos\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[343.952296, "o", "\u001b[0;33mchanged: [c1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Cloud User\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/centos\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: centos\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[343.957505, "o", "\u001b[0;33mchanged: [c1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Cloud User\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/centos\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: centos\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[344.209012, "o", "\u001b[0;33mchanged: [c1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Cloud User\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/centos\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: centos\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[344.2202, "o", "\r\nTASK [docker : file] **************************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/main.yml:4\u001b[0m\r\n"]
[344.610574, "o", "\u001b[0;33mchanged: [c1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  mode: '0755'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  path: /etc/docker/\u001b[0m\r\n\u001b[0;33m  secontext: unconfined_u:object_r:etc_t:s0\u001b[0m\r\n\u001b[0;33m  size: 6\u001b[0m\r\n\u001b[0;33m  state: directory\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[344.612321, "o", "\u001b[0;33mchanged: [c1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  mode: '0755'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  path: /etc/docker/\u001b[0m\r\n\u001b[0;33m  secontext: unconfined_u:object_r:etc_t:s0\u001b[0m\r\n\u001b[0;33m  size: 6\u001b[0m\r\n\u001b[0;33m  state: directory\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[344.613399, "o", "\u001b[0;33mchanged: [c1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  mode: '0755'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  path: /etc/docker/\u001b[0m\r\n\u001b[0;33m  secontext: unconfined_u:object_r:etc_t:s0\u001b[0m\r\n\u001b[0;33m  size: 6\u001b[0m\r\n\u001b[0;33m  state: directory\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[344.629697, "o", "\u001b[0;33mchanged: [c1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  mode: '0755'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  path: /etc/docker/\u001b[0m\r\n\u001b[0;33m  secontext: unconfined_u:object_r:etc_t:s0\u001b[0m\r\n\u001b[0;33m  size: 6\u001b[0m\r\n\u001b[0;33m  state: directory\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[344.633136, "o", "\u001b[0;33mchanged: [c1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  mode: '0755'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  path: /etc/docker/\u001b[0m\r\n\u001b[0;33m  secontext: unconfined_u:object_r:etc_t:s0\u001b[0m\r\n\u001b[0;33m  size: 6\u001b[0m\r\n\u001b[0;33m  state: directory\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[344.817859, "o", "\u001b[0;33mchanged: [c1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  mode: '0755'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  path: /etc/docker/\u001b[0m\r\n\u001b[0;33m  secontext: unconfined_u:object_r:etc_t:s0\u001b[0m\r\n\u001b[0;33m  size: 6\u001b[0m\r\n\u001b[0;33m  state: directory\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[344.824204, "o", "\r\nTASK [docker : copy] **************************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/main.yml:8\u001b[0m\r\n"]
[345.555176, "o", "\u001b[0;33mchanged: [c1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  secontext: system_u:object_r:container_config_t:s0\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/centos/.ansible/tmp/ansible-tmp-1607174021.6311429-29113-253577170359196/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[345.557693, "o", "\u001b[0;33mchanged: [c1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  secontext: system_u:object_r:container_config_t:s0\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/centos/.ansible/tmp/ansible-tmp-1607174021.622376-29112-166956858737774/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[345.559066, "o", "\u001b[0;33mchanged: [c1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  secontext: system_u:object_r:container_config_t:s0\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/centos/.ansible/tmp/ansible-tmp-1607174021.665405-29118-177516655962213/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[345.561657, "o", "\u001b[0;33mchanged: [c1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  secontext: system_u:object_r:container_config_t:s0\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/centos/.ansible/tmp/ansible-tmp-1607174021.6411612-29114-6040966138116/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[345.571269, "o", "\u001b[0;33mchanged: [c1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  secontext: system_u:object_r:container_config_t:s0\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/centos/.ansible/tmp/ansible-tmp-1607174021.6530898-29116-123502604000720/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[345.958838, "o", "\u001b[0;33mchanged: [c1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  secontext: system_u:object_r:container_config_t:s0\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/centos/.ansible/tmp/ansible-tmp-1607174022.3495917-29189-104971299284056/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[345.963417, "o", "\r\nTASK [docker : systemd] ***********************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/main.yml:21\u001b[0m\r\n"]
[347.656365, "o", "\u001b[0;33mchanged: [c1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: inactive\u001b[0m\r\n\u001b[0;33m    After: systemd-journald.socket firewalld.service system.slice containerd.service docker.socket basic.target network-online.target\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: '0'\u001b[0m\r\n\u001b[0;33m    AssertResult: 'no'\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Before: shutdown.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CanIsola"]
[347.656921, "o", "te: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'no'\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/"]
[347.657014, "o", "bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /usr/lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOScheduling: '0'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;33m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreOnSnapshot: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: '0'\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCORE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCPU: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitDATA: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitFSIZE"]
[347.657033, "o", ": '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK: '65536'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitNPROC: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRSS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '11195'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    MainPID: '0'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MemoryLimit: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MountFlags: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailur"]
[347.657041, "o", "eJobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: docker.socket system.slice basic.target\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0;33m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitInterval: '60000000'\u001b[0"]
[347.657047, "o", "m\r\n\u001b[0;33m    StartupBlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: dead\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TasksMax: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimeoutStopUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: disabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: disabled\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Wa"]
[347.657058, "o", "tchdogUSec: '0'\u001b[0m\r\n"]
[347.669031, "o", "\u001b[0;33mchanged: [c1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: inactive\u001b[0m\r\n\u001b[0;33m    After: basic.target system.slice containerd.service docker.socket systemd-journald.socket network-online.target firewalld.service\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: '0'\u001b[0m\r\n\u001b[0;33m    AssertResult: 'no'\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Before: shutdown.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CanIsola"]
[347.669149, "o", "te: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'no'\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/"]
[347.669167, "o", "bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /usr/lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOScheduling: '0'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;33m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreOnSnapshot: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: '0'\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCORE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCPU: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitDATA: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitFSIZE"]
[347.669178, "o", ": '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK: '65536'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitNPROC: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRSS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '7163'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    MainPID: '0'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MemoryLimit: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MountFlags: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailure"]
[347.669187, "o", "JobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: docker.socket system.slice basic.target\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0;33m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitInterval: '60000000'\u001b[0m"]
[347.669266, "o", "\r\n\u001b[0;33m    StartupBlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: dead\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TasksMax: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimeoutStopUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: disabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: disabled\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Wat"]
[347.669301, "o", "chdogUSec: '0'\u001b[0m\r\n"]
[347.679842, "o", "\u001b[0;33mchanged: [c1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: inactive\u001b[0m\r\n\u001b[0;33m    After: basic.target system.slice network-online.target firewalld.service systemd-journald.socket docker.socket containerd.service\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: '0'\u001b[0m\r\n\u001b[0;33m    AssertResult: 'no'\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Before: shutdown.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CanIsola"]
[347.679887, "o", "te: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'no'\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/"]
[347.679897, "o", "bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /usr/lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOScheduling: '0'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;33m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreOnSnapshot: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: '0'\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCORE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCPU: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitDATA: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitFSIZE"]
[347.680007, "o", ": '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK: '65536'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitNPROC: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRSS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '7163'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    MainPID: '0'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MemoryLimit: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MountFlags: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailure"]
[347.680045, "o", "JobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: system.slice docker.socket basic.target\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0;33m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitInterval: '60000000'\u001b[0m"]
[347.680093, "o", "\r\n\u001b[0;33m    StartupBlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: dead\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TasksMax: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimeoutStopUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: disabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: disabled\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Wat"]
[347.68013, "o", "chdogUSec: '0'\u001b[0m\r\n"]
[347.691881, "o", "\u001b[0;33mchanged: [c1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: inactive\u001b[0m\r\n\u001b[0;33m    After: containerd.service system.slice systemd-journald.socket basic.target network-online.target firewalld.service docker.socket\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: '0'\u001b[0m\r\n\u001b[0;33m    AssertResult: 'no'\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Before: shutdown.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CanIsola"]
[347.691917, "o", "te: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'no'\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/"]
[347.691926, "o", "bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /usr/lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOScheduling: '0'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;33m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreOnSnapshot: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: '0'\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCORE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCPU: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitDATA: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitFSIZE"]
[347.69198, "o", ": '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK: '65536'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitNPROC: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRSS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '7163'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    MainPID: '0'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MemoryLimit: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MountFlags: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailure"]
[347.692047, "o", "JobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: basic.target docker.socket system.slice\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0;33m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitInterval: '60000000'\u001b[0m"]
[347.692066, "o", "\r\n\u001b[0;33m    StartupBlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: dead\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TasksMax: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimeoutStopUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: disabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: disabled\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Wat"]
[347.692075, "o", "chdogUSec: '0'\u001b[0m\r\n"]
[347.708328, "o", "\u001b[0;33mchanged: [c1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: inactive\u001b[0m\r\n\u001b[0;33m    After: docker.socket containerd.service system.slice basic.target systemd-journald.socket network-online.target firewalld.service\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: '0'\u001b[0m\r\n\u001b[0;33m    AssertResult: 'no'\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Before: shutdown.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CanIsola"]
[347.708426, "o", "te: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'no'\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/"]
[347.70846, "o", "bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /usr/lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOScheduling: '0'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;33m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreOnSnapshot: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: '0'\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCORE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCPU: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitDATA: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitFSIZE"]
[347.708504, "o", ": '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK: '65536'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitNPROC: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRSS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '11195'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    MainPID: '0'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MemoryLimit: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MountFlags: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailur"]
[347.708516, "o", "eJobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: basic.target system.slice docker.socket\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0;33m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitInterval: '60000000'\u001b[0"]
[347.708522, "o", "m\r\n\u001b[0;33m    StartupBlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: dead\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TasksMax: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimeoutStopUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: disabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: disabled\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Wa"]
[347.708529, "o", "tchdogUSec: '0'\u001b[0m\r\n"]
[349.014259, "o", "\u001b[0;33mchanged: [c1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: inactive\u001b[0m\r\n\u001b[0;33m    After: system.slice systemd-journald.socket basic.target network-online.target firewalld.service docker.socket containerd.service\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: '0'\u001b[0m\r\n\u001b[0;33m    AssertResult: 'no'\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Before: shutdown.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    CanIsola"]
[349.014296, "o", "te: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'no'\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/"]
[349.014306, "o", "bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /usr/lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOScheduling: '0'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;33m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreOnSnapshot: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: '0'\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCORE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitCPU: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitDATA: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitFSIZE"]
[349.014314, "o", ": '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK: '65536'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitNPROC: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRSS: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '11195'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    MainPID: '0'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MemoryLimit: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    MountFlags: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailur"]
[349.01432, "o", "eJobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: system.slice basic.target docker.socket\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0;33m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitInterval: '60000000'\u001b[0"]
[349.014378, "o", "m\r\n\u001b[0;33m    StartupBlockIOWeight: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: dead\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TasksMax: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimeoutStopUSec: '0'\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: disabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: disabled\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    Wa"]
[349.014453, "o", "tchdogUSec: '0'\u001b[0m\r\n"]
[349.019258, "o", "\u001b[0;34mMETA: ran handlers\u001b[0m\r\n"]
[349.02346, "o", "\u001b[0;34mMETA: ran handlers\u001b[0m\r\n"]
[349.024333, "o", "\r\nPLAY RECAP ************************************************************************************************************************************************************************************\r\n"]
[349.024423, "o", "\u001b[0;33mc1a1\u001b[0m                       : \u001b[0;32mok=10  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n\u001b[0;33mc1a2\u001b[0m                       : \u001b[0;32mok=10  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n"]
[349.02446, "o", "\u001b[0;33mc1a3\u001b[0m                       : \u001b[0;32mok=10  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n"]
[349.0245, "o", "\u001b[0;33mc1b1\u001b[0m                       : \u001b[0;32mok=10  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n"]
[349.024558, "o", "\u001b[0;33mc1b2\u001b[0m                       : \u001b[0;32mok=10  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n\u001b[0;33mc1b3\u001b[0m                       : \u001b[0;32mok=10  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n"]
[349.024623, "o", "\r\n"]
[349.067625, "o", "\r\nreal\t5m49.049s\r\nuser\t1m4.046s\r\nsys\t0m13.000s\r\n"]
[349.229734, "o", "cd /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ && tee /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/c1.yml </home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/c1-v1.18.12-rancher1-1.yml \\\r\n            && ./files/rke up --config=/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/c1.yml\r\n"]
[349.312832, "o", "---\r\ncluster_name: c1\r\nkubernetes_version: v1.18.12-rancher1-1\r\n\r\nssh_agent_auth: true\r\n\r\nnodes:\r\n  - &m1\r\n    address: 10.20.2.10\r\n    user: centos\r\n    role: [etcd, controlplane]\r\n  - <<: *m1\r\n    address: 10.20.2.11\r\n  - <<: *m1\r\n    address: 10.20.2.12\r\n\r\n  - &n1\r\n    address: 10.20.2.20\r\n    user: centos\r\n    role: [worker]\r\n  - <<: *n1\r\n    address: 10.20.2.21\r\n  - <<: *n1\r\n    address: 10.20.2.22\r\n\r\nnetwork:\r\n  plugin: calico\r\n\r\nprivate_registries:\r\n  - url: 10.8.101.2:5000\r\n    is_default: true\r\n"]
[349.333857, "o", "\u001b[36mINFO\u001b[0m[0000] Running RKE version: v1.2.3                  \r\n"]
[349.334985, "o", "\u001b[36mINFO\u001b[0m[0000] Initiating Kubernetes cluster                \r\n"]
[349.352878, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.21]  \r\n\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.10]  \r\n"]
[349.352915, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.20]  \r\n"]
[349.353041, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.11]  \r\n\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.22]  \r\n"]
[349.353453, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.12]  "]
[349.353554, "o", "\r\n"]
[349.6826, "o", "\u001b[36mINFO\u001b[0m[0000] Checking if container [cluster-state-deployer] is running on host [10.20.2.10], try #1 \r\n"]
[349.684002, "o", "\u001b[36mINFO\u001b[0m[0000] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.20.2.10], try #1 \r\n"]
[352.097087, "o", "\u001b[36mINFO\u001b[0m[0002] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[352.643707, "o", "\u001b[36mINFO\u001b[0m[0003] Starting container [cluster-state-deployer] on host [10.20.2.10], try #1 \r\n"]
[352.833369, "o", "\u001b[36mINFO\u001b[0m[0003] [state] Successfully started [cluster-state-deployer] container on host [10.20.2.10] \r\n"]
[352.861508, "o", "\u001b[36mINFO\u001b[0m[0003] Checking if container [cluster-state-deployer] is running on host [10.20.2.11], try #1 \r\n"]
[352.863428, "o", "\u001b[36mINFO\u001b[0m[0003] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.20.2.11], try #1 \r\n"]
[355.329022, "o", "\u001b[36mINFO\u001b[0m[0006] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[355.848532, "o", "\u001b[36mINFO\u001b[0m[0006] Starting container [cluster-state-deployer] on host [10.20.2.11], try #1 \r\n"]
[356.060041, "o", "\u001b[36mINFO\u001b[0m[0006] [state] Successfully started [cluster-state-deployer] container on host [10.20.2.11] \r\n"]
[356.090425, "o", "\u001b[36mINFO\u001b[0m[0006] Checking if container [cluster-state-deployer] is running on host [10.20.2.12], try #1 \r\n"]
[356.092502, "o", "\u001b[36mINFO\u001b[0m[0006] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.20.2.12], try #1 \r\n"]
[358.526345, "o", "\u001b[36mINFO\u001b[0m[0009] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[359.049958, "o", "\u001b[36mINFO\u001b[0m[0009] Starting container [cluster-state-deployer] on host [10.20.2.12], try #1 \r\n"]
[359.247659, "o", "\u001b[36mINFO\u001b[0m[0009] [state] Successfully started [cluster-state-deployer] container on host [10.20.2.12] \r\n"]
[359.278101, "o", "\u001b[36mINFO\u001b[0m[0009] Checking if container [cluster-state-deployer] is running on host [10.20.2.20], try #1 \r\n"]
[359.280062, "o", "\u001b[36mINFO\u001b[0m[0009] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.20.2.20], try #1 \r\n"]
[361.691849, "o", "\u001b[36mINFO\u001b[0m[0012] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[362.251925, "o", "\u001b[36mINFO\u001b[0m[0012] Starting container [cluster-state-deployer] on host [10.20.2.20], try #1 \r\n"]
[362.466312, "o", "\u001b[36mINFO\u001b[0m[0013] [state] Successfully started [cluster-state-deployer] container on host [10.20.2.20] \r\n"]
[362.493217, "o", "\u001b[36mINFO\u001b[0m[0013] Checking if container [cluster-state-deployer] is running on host [10.20.2.21], try #1 \r\n"]
[362.495077, "o", "\u001b[36mINFO\u001b[0m[0013] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.20.2.21], try #1 \r\n"]
[365.184685, "o", "\u001b[36mINFO\u001b[0m[0015] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[365.715206, "o", "\u001b[36mINFO\u001b[0m[0016] Starting container [cluster-state-deployer] on host [10.20.2.21], try #1 \r\n"]
[365.941222, "o", "\u001b[36mINFO\u001b[0m[0016] [state] Successfully started [cluster-state-deployer] container on host [10.20.2.21] \r\n"]
[365.974971, "o", "\u001b[36mINFO\u001b[0m[0016] Checking if container [cluster-state-deployer] is running on host [10.20.2.22], try #1 \r\n"]
[365.977353, "o", "\u001b[36mINFO\u001b[0m[0016] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.20.2.22], try #1 \r\n"]
[368.469439, "o", "\u001b[36mINFO\u001b[0m[0019] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[369.014106, "o", "\u001b[36mINFO\u001b[0m[0019] Starting container [cluster-state-deployer] on host [10.20.2.22], try #1 \r\n"]
[369.213078, "o", "\u001b[36mINFO\u001b[0m[0019] [state] Successfully started [cluster-state-deployer] container on host [10.20.2.22] \r\n"]
[369.243668, "o", "\u001b[36mINFO\u001b[0m[0019] [certificates] Generating CA kubernetes certificates \r\n"]
[369.356502, "o", "\u001b[36mINFO\u001b[0m[0020] [certificates] Generating Kubernetes API server aggregation layer requestheader client CA certificates \r\n"]
[369.481409, "o", "\u001b[36mINFO\u001b[0m[0020] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates \r\n\u001b[36mINFO\u001b[0m[0020] [certificates] Generating Kubernetes API server certificates \r\n"]
[369.765054, "o", "\u001b[36mINFO\u001b[0m[0020] [certificates] Generating Service account token key \r\n\u001b[36mINFO\u001b[0m[0020] [certificates] Generating Kube Controller certificates \r\n"]
[370.32842, "o", "\u001b[36mINFO\u001b[0m[0021] [certificates] Generating Kube Scheduler certificates \r\n"]
[370.373193, "o", "\u001b[36mINFO\u001b[0m[0021] [certificates] Generating Kube Proxy certificates \r\n"]
[370.527324, "o", "\u001b[36mINFO\u001b[0m[0021] [certificates] Generating Node certificate   \r\n"]
[370.528866, "o", "\u001b[36mINFO\u001b[0m[0021] [certificates] Generating admin certificates and kubeconfig \r\n"]
[370.689687, "o", "\u001b[36mINFO\u001b[0m[0021] [certificates] Generating Kubernetes API server proxy client certificates \r\n"]
[370.890704, "o", "\u001b[36mINFO\u001b[0m[0021] [certificates] Generating kube-etcd-10-20-2-10 certificate and key \r\n"]
[370.964029, "o", "\u001b[36mINFO\u001b[0m[0021] [certificates] Generating kube-etcd-10-20-2-11 certificate and key \r\n"]
[371.078105, "o", "\u001b[36mINFO\u001b[0m[0021] [certificates] Generating kube-etcd-10-20-2-12 certificate and key \r\n"]
[371.146877, "o", "\u001b[36mINFO\u001b[0m[0021] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/c1.rkestate] \r\n"]
[371.148393, "o", "\u001b[36mINFO\u001b[0m[0021] Building Kubernetes cluster                  \r\n\u001b[36mINFO\u001b[0m[0021] [dialer] Setup tunnel for host [10.20.2.22]  \r\n"]
[371.148573, "o", "\u001b[36mINFO\u001b[0m[0021] [dialer] Setup tunnel for host [10.20.2.20]  \r\n\u001b[36mINFO\u001b[0m[0021] [dialer] Setup tunnel for host [10.20.2.21]  \r\n"]
[371.148745, "o", "\u001b[36mINFO\u001b[0m[0021] [dialer] Setup tunnel for host [10.20.2.12]  \r\n\u001b[36mINFO\u001b[0m[0021] [dialer] Setup tunnel for host [10.20.2.10]  \r\n"]
[371.148931, "o", "\u001b[36mINFO\u001b[0m[0021] [dialer] Setup tunnel for host [10.20.2.11]  \r\n"]
[371.473015, "o", "\u001b[36mINFO\u001b[0m[0022] [network] Deploying port listener containers \r\n"]
[371.475025, "o", "\u001b[36mINFO\u001b[0m[0022] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[371.475441, "o", "\u001b[36mINFO\u001b[0m[0022] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0022] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[371.706734, "o", "\u001b[36mINFO\u001b[0m[0022] Starting container [rke-etcd-port-listener] on host [10.20.2.11], try #1 \r\n\u001b[36mINFO\u001b[0m[0022] Starting container [rke-etcd-port-listener] on host [10.20.2.12], try #1 \r\n"]
[371.707109, "o", "\u001b[36mINFO\u001b[0m[0022] Starting container [rke-etcd-port-listener] on host [10.20.2.10], try #1 \r\n"]
[372.046052, "o", "\u001b[36mINFO\u001b[0m[0022] [network] Successfully started [rke-etcd-port-listener] container on host [10.20.2.11] \r\n"]
[372.050586, "o", "\u001b[36mINFO\u001b[0m[0022] [network] Successfully started [rke-etcd-port-listener] container on host [10.20.2.12] \r\n"]
[372.059027, "o", "\u001b[36mINFO\u001b[0m[0022] [network] Successfully started [rke-etcd-port-listener] container on host [10.20.2.10] \r\n"]
[372.060508, "o", "\u001b[36mINFO\u001b[0m[0022] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[372.06073, "o", "\u001b[36mINFO\u001b[0m[0022] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[372.060803, "o", "\u001b[36mINFO\u001b[0m[0022] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[372.315212, "o", "\u001b[36mINFO\u001b[0m[0022] Starting container [rke-cp-port-listener] on host [10.20.2.12], try #1 \r\n"]
[372.317715, "o", "\u001b[36mINFO\u001b[0m[0023] Starting container [rke-cp-port-listener] on host [10.20.2.11], try #1 \r\n"]
[372.318357, "o", "\u001b[36mINFO\u001b[0m[0023] Starting container [rke-cp-port-listener] on host [10.20.2.10], try #1 \r\n"]
[372.69414, "o", "\u001b[36mINFO\u001b[0m[0023] [network] Successfully started [rke-cp-port-listener] container on host [10.20.2.11] \r\n"]
[372.706482, "o", "\u001b[36mINFO\u001b[0m[0023] [network] Successfully started [rke-cp-port-listener] container on host [10.20.2.10] \r\n"]
[372.722646, "o", "\u001b[36mINFO\u001b[0m[0023] [network] Successfully started [rke-cp-port-listener] container on host [10.20.2.12] \r\n"]
[372.724884, "o", "\u001b[36mINFO\u001b[0m[0023] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[372.725018, "o", "\u001b[36mINFO\u001b[0m[0023] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n\u001b[36mINFO\u001b[0m[0023] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[372.95785, "o", "\u001b[36mINFO\u001b[0m[0023] Starting container [rke-worker-port-listener] on host [10.20.2.21], try #1 \r\n"]
[372.959547, "o", "\u001b[36mINFO\u001b[0m[0023] Starting container [rke-worker-port-listener] on host [10.20.2.20], try #1 \r\n"]
[372.960892, "o", "\u001b[36mINFO\u001b[0m[0023] Starting container [rke-worker-port-listener] on host [10.20.2.22], try #1 \r\n"]
[373.335694, "o", "\u001b[36mINFO\u001b[0m[0024] [network] Successfully started [rke-worker-port-listener] container on host [10.20.2.20] \r\n"]
[373.3363, "o", "\u001b[36mINFO\u001b[0m[0024] [network] Successfully started [rke-worker-port-listener] container on host [10.20.2.21] \r\n"]
[373.344446, "o", "\u001b[36mINFO\u001b[0m[0024] [network] Successfully started [rke-worker-port-listener] container on host [10.20.2.22] \r\n\u001b[36mINFO\u001b[0m[0024] [network] Port listener containers deployed successfully \r\n\u001b[36mINFO\u001b[0m[0024] [network] Running etcd <-> etcd port checks  \r\n"]
[373.346438, "o", "\u001b[36mINFO\u001b[0m[0024] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0024] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[373.346543, "o", "\u001b[36mINFO\u001b[0m[0024] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[373.59195, "o", "\u001b[36mINFO\u001b[0m[0024] Starting container [rke-port-checker] on host [10.20.2.10], try #1 \r\n\u001b[36mINFO\u001b[0m[0024] Starting container [rke-port-checker] on host [10.20.2.11], try #1 \r\n"]
[373.59532, "o", "\u001b[36mINFO\u001b[0m[0024] Starting container [rke-port-checker] on host [10.20.2.12], try #1 \r\n"]
[373.826808, "o", "\u001b[36mINFO\u001b[0m[0024] [network] Successfully started [rke-port-checker] container on host [10.20.2.11] \r\n"]
[373.82921, "o", "\u001b[36mINFO\u001b[0m[0024] [network] Successfully started [rke-port-checker] container on host [10.20.2.10] \r\n"]
[373.833797, "o", "\u001b[36mINFO\u001b[0m[0024] [network] Successfully started [rke-port-checker] container on host [10.20.2.12] \r\n"]
[373.881602, "o", "\u001b[36mINFO\u001b[0m[0024] Removing container [rke-port-checker] on host [10.20.2.10], try #1 \r\n"]
[373.881719, "o", "\u001b[36mINFO\u001b[0m[0024] Removing container [rke-port-checker] on host [10.20.2.11], try #1 \r\n"]
[373.889597, "o", "\u001b[36mINFO\u001b[0m[0024] Removing container [rke-port-checker] on host [10.20.2.12], try #1 \r\n"]
[373.916272, "o", "\u001b[36mINFO\u001b[0m[0024] [network] Running control plane -> etcd port checks \r\n"]
[373.91885, "o", "\u001b[36mINFO\u001b[0m[0024] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0024] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[373.918964, "o", "\u001b[36mINFO\u001b[0m[0024] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[374.169225, "o", "\u001b[36mINFO\u001b[0m[0024] Starting container [rke-port-checker] on host [10.20.2.12], try #1 \r\n\u001b[36mINFO\u001b[0m[0024] Starting container [rke-port-checker] on host [10.20.2.10], try #1 \r\n"]
[374.170765, "o", "\u001b[36mINFO\u001b[0m[0024] Starting container [rke-port-checker] on host [10.20.2.11], try #1 \r\n"]
[374.39768, "o", "\u001b[36mINFO\u001b[0m[0025] [network] Successfully started [rke-port-checker] container on host [10.20.2.12] \r\n"]
[374.406638, "o", "\u001b[36mINFO\u001b[0m[0025] [network] Successfully started [rke-port-checker] container on host [10.20.2.11] \r\n"]
[374.421117, "o", "\u001b[36mINFO\u001b[0m[0025] [network] Successfully started [rke-port-checker] container on host [10.20.2.10] \r\n"]
[374.475891, "o", "\u001b[36mINFO\u001b[0m[0025] Removing container [rke-port-checker] on host [10.20.2.12], try #1 \r\n"]
[374.485162, "o", "\u001b[36mINFO\u001b[0m[0025] Removing container [rke-port-checker] on host [10.20.2.11], try #1 \r\n"]
[374.494448, "o", "\u001b[36mINFO\u001b[0m[0025] Removing container [rke-port-checker] on host [10.20.2.10], try #1 \r\n"]
[374.519233, "o", "\u001b[36mINFO\u001b[0m[0025] [network] Running control plane -> worker port checks \r\n"]
[374.521298, "o", "\u001b[36mINFO\u001b[0m[0025] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[374.521569, "o", "\u001b[36mINFO\u001b[0m[0025] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[374.521704, "o", "\u001b[36mINFO\u001b[0m[0025] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[375.286615, "o", "\u001b[36mINFO\u001b[0m[0025] Starting container [rke-port-checker] on host [10.20.2.10], try #1 \r\n\u001b[36mINFO\u001b[0m[0025] Starting container [rke-port-checker] on host [10.20.2.11], try #1 \r\n"]
[375.287046, "o", "\u001b[36mINFO\u001b[0m[0025] Starting container [rke-port-checker] on host [10.20.2.12], try #1 \r\n"]
[375.411519, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Successfully started [rke-port-checker] container on host [10.20.2.11] \r\n"]
[375.415208, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Successfully started [rke-port-checker] container on host [10.20.2.10] \r\n"]
[375.418274, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Successfully started [rke-port-checker] container on host [10.20.2.12] \r\n"]
[375.470383, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-port-checker] on host [10.20.2.12], try #1 \r\n"]
[375.471797, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-port-checker] on host [10.20.2.11], try #1 \r\n"]
[375.475289, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-port-checker] on host [10.20.2.10], try #1 \r\n"]
[375.497043, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Running workers -> control plane port checks \r\n"]
[375.498787, "o", "\u001b[36mINFO\u001b[0m[0026] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[375.499289, "o", "\u001b[36mINFO\u001b[0m[0026] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n\u001b[36mINFO\u001b[0m[0026] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[375.706036, "o", "\u001b[36mINFO\u001b[0m[0026] Starting container [rke-port-checker] on host [10.20.2.22], try #1 \r\n"]
[375.706423, "o", "\u001b[36mINFO\u001b[0m[0026] Starting container [rke-port-checker] on host [10.20.2.20], try #1 \r\n\u001b[36mINFO\u001b[0m[0026] Starting container [rke-port-checker] on host [10.20.2.21], try #1 \r\n"]
[375.948002, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Successfully started [rke-port-checker] container on host [10.20.2.22] \r\n"]
[375.952738, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Successfully started [rke-port-checker] container on host [10.20.2.20] \r\n"]
[375.952846, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Successfully started [rke-port-checker] container on host [10.20.2.21] \r\n"]
[375.999948, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-port-checker] on host [10.20.2.22], try #1 \r\n"]
[376.008504, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-port-checker] on host [10.20.2.20], try #1 \r\n"]
[376.015708, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-port-checker] on host [10.20.2.21], try #1 \r\n"]
[376.037865, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Checking KubeAPI port Control Plane hosts \r\n"]
[376.03847, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Removing port listener containers  \r\n"]
[376.039739, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-etcd-port-listener] on host [10.20.2.11], try #1 \r\n"]
[376.03978, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-etcd-port-listener] on host [10.20.2.10], try #1 \r\n"]
[376.040166, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-etcd-port-listener] on host [10.20.2.12], try #1 \r\n"]
[376.167828, "o", "\u001b[36mINFO\u001b[0m[0026] [remove/rke-etcd-port-listener] Successfully removed container on host [10.20.2.11] \r\n"]
[376.176583, "o", "\u001b[36mINFO\u001b[0m[0026] [remove/rke-etcd-port-listener] Successfully removed container on host [10.20.2.12] \r\n"]
[376.182216, "o", "\u001b[36mINFO\u001b[0m[0026] [remove/rke-etcd-port-listener] Successfully removed container on host [10.20.2.10] \r\n"]
[376.183023, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-cp-port-listener] on host [10.20.2.10], try #1 \r\n"]
[376.183383, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-cp-port-listener] on host [10.20.2.12], try #1 \r\n"]
[376.183564, "o", "\u001b[36mINFO\u001b[0m[0026] Removing container [rke-cp-port-listener] on host [10.20.2.11], try #1 \r\n"]
[376.318722, "o", "\u001b[36mINFO\u001b[0m[0027] [remove/rke-cp-port-listener] Successfully removed container on host [10.20.2.10] \r\n"]
[376.319708, "o", "\u001b[36mINFO\u001b[0m[0027] [remove/rke-cp-port-listener] Successfully removed container on host [10.20.2.11] \r\n"]
[376.320249, "o", "\u001b[36mINFO\u001b[0m[0027] [remove/rke-cp-port-listener] Successfully removed container on host [10.20.2.12] \r\n"]
[376.321439, "o", "\u001b[36mINFO\u001b[0m[0027] Removing container [rke-worker-port-listener] on host [10.20.2.21], try #1 \r\n\u001b[36mINFO\u001b[0m[0027] Removing container [rke-worker-port-listener] on host [10.20.2.20], try #1 \r\n"]
[376.321585, "o", "\u001b[36mINFO\u001b[0m[0027] Removing container [rke-worker-port-listener] on host [10.20.2.22], try #1 \r\n"]
[376.456563, "o", "\u001b[36mINFO\u001b[0m[0027] [remove/rke-worker-port-listener] Successfully removed container on host [10.20.2.21] \r\n"]
[376.459357, "o", "\u001b[36mINFO\u001b[0m[0027] [remove/rke-worker-port-listener] Successfully removed container on host [10.20.2.20] \r\n"]
[376.461876, "o", "\u001b[36mINFO\u001b[0m[0027] [remove/rke-worker-port-listener] Successfully removed container on host [10.20.2.22] \r\n\u001b[36mINFO\u001b[0m[0027] [network] Port listener containers removed successfully \r\n"]
[376.462024, "o", "\u001b[36mINFO\u001b[0m[0027] [certificates] Deploying kubernetes certificates to Cluster nodes \r\n"]
[376.462132, "o", "\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.20], try #1 \r\n\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.21], try #1 \r\n\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.22], try #1 \r\n"]
[376.462214, "o", "\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.11], try #1 \r\n"]
[376.462281, "o", "\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.10], try #1 \r\n"]
[376.462336, "o", "\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.12], try #1 \r\n"]
[376.465085, "o", "\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[376.465206, "o", "\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[376.465411, "o", "\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[376.465755, "o", "\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[376.835032, "o", "\u001b[36mINFO\u001b[0m[0027] Starting container [cert-deployer] on host [10.20.2.21], try #1 \r\n"]
[376.836122, "o", "\u001b[36mINFO\u001b[0m[0027] Starting container [cert-deployer] on host [10.20.2.20], try #1 \r\n"]
[376.839275, "o", "\u001b[36mINFO\u001b[0m[0027] Starting container [cert-deployer] on host [10.20.2.22], try #1 \r\n"]
[376.841746, "o", "\u001b[36mINFO\u001b[0m[0027] Starting container [cert-deployer] on host [10.20.2.11], try #1 \r\n"]
[376.843871, "o", "\u001b[36mINFO\u001b[0m[0027] Starting container [cert-deployer] on host [10.20.2.12], try #1 \r\n"]
[376.863203, "o", "\u001b[36mINFO\u001b[0m[0027] Starting container [cert-deployer] on host [10.20.2.10], try #1 \r\n"]
[377.121518, "o", "\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.20], try #1 \r\n"]
[377.142027, "o", "\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.12], try #1 \r\n"]
[377.14759, "o", "\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.22], try #1 \r\n"]
[377.15124, "o", "\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.11], try #1 \r\n"]
[377.153302, "o", "\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.21], try #1 \r\n"]
[377.157603, "o", "\u001b[36mINFO\u001b[0m[0027] Checking if container [cert-deployer] is running on host [10.20.2.10], try #1 \r\n"]
[382.124937, "o", "\u001b[36mINFO\u001b[0m[0032] Checking if container [cert-deployer] is running on host [10.20.2.20], try #1 \r\n"]
[382.126735, "o", "\u001b[36mINFO\u001b[0m[0032] Removing container [cert-deployer] on host [10.20.2.20], try #1 \r\n"]
[382.145231, "o", "\u001b[36mINFO\u001b[0m[0032] Checking if container [cert-deployer] is running on host [10.20.2.12], try #1 \r\n"]
[382.147263, "o", "\u001b[36mINFO\u001b[0m[0032] Removing container [cert-deployer] on host [10.20.2.12], try #1 \r\n"]
[382.149431, "o", "\u001b[36mINFO\u001b[0m[0032] Checking if container [cert-deployer] is running on host [10.20.2.22], try #1 \r\n"]
[382.151252, "o", "\u001b[36mINFO\u001b[0m[0032] Removing container [cert-deployer] on host [10.20.2.22], try #1 \r\n"]
[382.153699, "o", "\u001b[36mINFO\u001b[0m[0032] Checking if container [cert-deployer] is running on host [10.20.2.11], try #1 \r\n"]
[382.155285, "o", "\u001b[36mINFO\u001b[0m[0032] Checking if container [cert-deployer] is running on host [10.20.2.21], try #1 \r\n"]
[382.155749, "o", "\u001b[36mINFO\u001b[0m[0032] Removing container [cert-deployer] on host [10.20.2.11], try #1 \r\n"]
[382.156761, "o", "\u001b[36mINFO\u001b[0m[0032] Removing container [cert-deployer] on host [10.20.2.21], try #1 \r\n"]
[382.15954, "o", "\u001b[36mINFO\u001b[0m[0032] Checking if container [cert-deployer] is running on host [10.20.2.10], try #1 \r\n"]
[382.164768, "o", "\u001b[36mINFO\u001b[0m[0032] Removing container [cert-deployer] on host [10.20.2.10], try #1 \r\n"]
[382.179713, "o", "\u001b[36mINFO\u001b[0m[0032] [reconcile] Rebuilding and updating local kube config \r\n"]
[382.179846, "o", "\u001b[36mINFO\u001b[0m[0032] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_c1.yml] \r\n"]
[382.181717, "o", "\u001b[36mINFO\u001b[0m[0032] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_c1.yml] \r\n"]
[382.182751, "o", "\u001b[36mINFO\u001b[0m[0032] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_c1.yml] \r\n"]
[382.184423, "o", "\u001b[36mINFO\u001b[0m[0032] [certificates] Successfully deployed kubernetes certificates to Cluster nodes \r\n"]
[382.184499, "o", "\u001b[36mINFO\u001b[0m[0032] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.20.2.10] \r\n"]
[382.186512, "o", "\u001b[36mINFO\u001b[0m[0032] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[382.274911, "o", "\u001b[36mINFO\u001b[0m[0032] Starting container [file-deployer] on host [10.20.2.10], try #1 \r\n"]
[382.486882, "o", "\u001b[36mINFO\u001b[0m[0033] Successfully started [file-deployer] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0033] Waiting for [file-deployer] container to exit on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0033] Waiting for [file-deployer] container to exit on host [10.20.2.10] \r\n"]
[382.487862, "o", "\u001b[36mINFO\u001b[0m[0033] Container [file-deployer] is still running on host [10.20.2.10]: stderr: [], stdout: [] \r\n"]
[383.488129, "o", "\u001b[36mINFO\u001b[0m[0034] Waiting for [file-deployer] container to exit on host [10.20.2.10] \r\n"]
[383.490431, "o", "\u001b[36mINFO\u001b[0m[0034] Removing container [file-deployer] on host [10.20.2.10], try #1 \r\n"]
[383.499084, "o", "\u001b[36mINFO\u001b[0m[0034] [remove/file-deployer] Successfully removed container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0034] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.20.2.11] \r\n"]
[383.501038, "o", "\u001b[36mINFO\u001b[0m[0034] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[383.598239, "o", "\u001b[36mINFO\u001b[0m[0034] Starting container [file-deployer] on host [10.20.2.11], try #1 \r\n"]
[383.796978, "o", "\u001b[36mINFO\u001b[0m[0034] Successfully started [file-deployer] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0034] Waiting for [file-deployer] container to exit on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0034] Waiting for [file-deployer] container to exit on host [10.20.2.11] \r\n"]
[383.798148, "o", "\u001b[36mINFO\u001b[0m[0034] Container [file-deployer] is still running on host [10.20.2.11]: stderr: [], stdout: [] \r\n"]
[384.798522, "o", "\u001b[36mINFO\u001b[0m[0035] Waiting for [file-deployer] container to exit on host [10.20.2.11] \r\n"]
[384.804878, "o", "\u001b[36mINFO\u001b[0m[0035] Removing container [file-deployer] on host [10.20.2.11], try #1 \r\n"]
[384.835067, "o", "\u001b[36mINFO\u001b[0m[0035] [remove/file-deployer] Successfully removed container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0035] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.20.2.12] \r\n"]
[384.840095, "o", "\u001b[36mINFO\u001b[0m[0035] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[384.961748, "o", "\u001b[36mINFO\u001b[0m[0035] Starting container [file-deployer] on host [10.20.2.12], try #1 \r\n"]
[385.189021, "o", "\u001b[36mINFO\u001b[0m[0035] Successfully started [file-deployer] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0035] Waiting for [file-deployer] container to exit on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0035] Waiting for [file-deployer] container to exit on host [10.20.2.12] \r\n"]
[385.190185, "o", "\u001b[36mINFO\u001b[0m[0035] Container [file-deployer] is still running on host [10.20.2.12]: stderr: [], stdout: [] \r\n"]
[386.19063, "o", "\u001b[36mINFO\u001b[0m[0036] Waiting for [file-deployer] container to exit on host [10.20.2.12] \r\n"]
[386.196767, "o", "\u001b[36mINFO\u001b[0m[0036] Removing container [file-deployer] on host [10.20.2.12], try #1 \r\n"]
[386.237528, "o", "\u001b[36mINFO\u001b[0m[0036] [remove/file-deployer] Successfully removed container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0036] [/etc/kubernetes/audit-policy.yaml] Successfully deployed audit policy file to Cluster control nodes \r\n\u001b[36mINFO\u001b[0m[0036] [reconcile] Reconciling cluster state        \r\n"]
[386.237583, "o", "\u001b[36mINFO\u001b[0m[0036] [reconcile] This is newly generated cluster  \r\n\u001b[36mINFO\u001b[0m[0036] Pre-pulling kubernetes images                \r\n"]
[386.238754, "o", "\u001b[36mINFO\u001b[0m[0036] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.20.2.12], try #1 \r\n"]
[386.239067, "o", "\u001b[36mINFO\u001b[0m[0036] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.20.2.21], try #1 \r\n\u001b[36mINFO\u001b[0m[0036] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.20.2.10], try #1 \r\n\u001b[36mINFO\u001b[0m[0036] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.20.2.11], try #1 \r\n\u001b[36mINFO\u001b[0m[0036] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.20.2.22], try #1 \r\n"]
[386.239137, "o", "\u001b[36mINFO\u001b[0m[0036] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.20.2.20], try #1 \r\n"]
[430.401948, "o", "\u001b[36mINFO\u001b[0m[0081] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.12] \r\n"]
[430.426935, "o", "\u001b[36mINFO\u001b[0m[0081] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.21] \r\n"]
[430.498095, "o", "\u001b[36mINFO\u001b[0m[0081] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.11] \r\n"]
[430.548138, "o", "\u001b[36mINFO\u001b[0m[0081] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.20] \r\n"]
[430.590996, "o", "\u001b[36mINFO\u001b[0m[0081] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.22] \r\n"]
[430.952854, "o", "\u001b[36mINFO\u001b[0m[0081] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0081] Kubernetes images pulled successfully        \r\n"]
[430.959257, "o", "\u001b[36mINFO\u001b[0m[0081] [etcd] Building up etcd plane..              \r\n"]
[430.966664, "o", "\u001b[36mINFO\u001b[0m[0081] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[431.522022, "o", "\u001b[36mINFO\u001b[0m[0082] Starting container [etcd-fix-perm] on host [10.20.2.10], try #1 \r\n"]
[431.853332, "o", "\u001b[36mINFO\u001b[0m[0082] Successfully started [etcd-fix-perm] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0082] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0082] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.10] \r\n"]
[431.854579, "o", "\u001b[36mINFO\u001b[0m[0082] Container [etcd-fix-perm] is still running on host [10.20.2.10]: stderr: [], stdout: [] \r\n"]
[432.854795, "o", "\u001b[36mINFO\u001b[0m[0083] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.10] \r\n"]
[432.861027, "o", "\u001b[36mINFO\u001b[0m[0083] Removing container [etcd-fix-perm] on host [10.20.2.10], try #1 \r\n"]
[432.893614, "o", "\u001b[36mINFO\u001b[0m[0083] [remove/etcd-fix-perm] Successfully removed container on host [10.20.2.10] \r\n"]
[432.896744, "o", "\u001b[36mINFO\u001b[0m[0083] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] on host [10.20.2.10], try #1 \r\n"]
[434.579313, "o", "\u001b[36mINFO\u001b[0m[0085] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] exists on host [10.20.2.10] \r\n"]
[435.04243, "o", "\u001b[36mINFO\u001b[0m[0085] Starting container [etcd] on host [10.20.2.10], try #1 \r\n"]
[435.433061, "o", "\u001b[36mINFO\u001b[0m[0086] [etcd] Successfully started [etcd] container on host [10.20.2.10] \r\n"]
[435.433168, "o", "\u001b[36mINFO\u001b[0m[0086] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.20.2.10] \r\n"]
[435.442708, "o", "\u001b[36mINFO\u001b[0m[0086] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[435.696787, "o", "\u001b[36mINFO\u001b[0m[0086] Starting container [etcd-rolling-snapshots] on host [10.20.2.10], try #1 \r\n"]
[435.847674, "o", "\u001b[36mINFO\u001b[0m[0086] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.20.2.10] \r\n"]
[440.856921, "o", "\u001b[36mINFO\u001b[0m[0091] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[441.03449, "o", "\u001b[36mINFO\u001b[0m[0091] Starting container [rke-bundle-cert] on host [10.20.2.10], try #1 \r\n"]
[441.31725, "o", "\u001b[36mINFO\u001b[0m[0092] [certificates] Successfully started [rke-bundle-cert] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0092] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.10] \r\n"]
[441.318738, "o", "\u001b[36mINFO\u001b[0m[0092] Container [rke-bundle-cert] is still running on host [10.20.2.10]: stderr: [], stdout: [] \r\n"]
[442.319042, "o", "\u001b[36mINFO\u001b[0m[0093] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.10] \r\n"]
[442.322299, "o", "\u001b[36mINFO\u001b[0m[0093] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0093] Removing container [rke-bundle-cert] on host [10.20.2.10], try #1 \r\n"]
[442.374174, "o", "\u001b[36mINFO\u001b[0m[0093] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[442.499632, "o", "\u001b[36mINFO\u001b[0m[0093] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[442.701099, "o", "\u001b[36mINFO\u001b[0m[0093] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[442.701883, "o", "\u001b[36mINFO\u001b[0m[0093] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[442.814122, "o", "\u001b[36mINFO\u001b[0m[0093] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[442.817158, "o", "\u001b[36mINFO\u001b[0m[0093] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[442.940588, "o", "\u001b[36mINFO\u001b[0m[0093] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[443.135268, "o", "\u001b[36mINFO\u001b[0m[0093] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[443.136222, "o", "\u001b[36mINFO\u001b[0m[0093] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[443.280059, "o", "\u001b[36mINFO\u001b[0m[0093] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[443.28255, "o", "\u001b[36mINFO\u001b[0m[0093] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[443.780764, "o", "\u001b[36mINFO\u001b[0m[0094] Starting container [etcd-fix-perm] on host [10.20.2.11], try #1 \r\n"]
[444.146913, "o", "\u001b[36mINFO\u001b[0m[0094] Successfully started [etcd-fix-perm] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0094] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0094] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.11] \r\n"]
[444.147864, "o", "\u001b[36mINFO\u001b[0m[0094] Container [etcd-fix-perm] is still running on host [10.20.2.11]: stderr: [], stdout: [] \r\n"]
[445.14823, "o", "\u001b[36mINFO\u001b[0m[0095] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.11] \r\n"]
[445.154549, "o", "\u001b[36mINFO\u001b[0m[0095] Removing container [etcd-fix-perm] on host [10.20.2.11], try #1 \r\n"]
[445.189273, "o", "\u001b[36mINFO\u001b[0m[0095] [remove/etcd-fix-perm] Successfully removed container on host [10.20.2.11] \r\n"]
[445.191676, "o", "\u001b[36mINFO\u001b[0m[0095] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] on host [10.20.2.11], try #1 \r\n"]
[446.68907, "o", "\u001b[36mINFO\u001b[0m[0097] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] exists on host [10.20.2.11] \r\n"]
[447.012322, "o", "\u001b[36mINFO\u001b[0m[0097] Starting container [etcd] on host [10.20.2.11], try #1 \r\n"]
[447.161732, "o", "\u001b[36mINFO\u001b[0m[0097] [etcd] Successfully started [etcd] container on host [10.20.2.11] \r\n"]
[447.161769, "o", "\u001b[36mINFO\u001b[0m[0097] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.20.2.11] \r\n"]
[447.163855, "o", "\u001b[36mINFO\u001b[0m[0097] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[447.328749, "o", "\u001b[36mINFO\u001b[0m[0098] Starting container [etcd-rolling-snapshots] on host [10.20.2.11], try #1 \r\n"]
[447.451016, "o", "\u001b[36mINFO\u001b[0m[0098] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.20.2.11] \r\n"]
[452.458973, "o", "\u001b[36mINFO\u001b[0m[0103] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[452.605139, "o", "\u001b[36mINFO\u001b[0m[0103] Starting container [rke-bundle-cert] on host [10.20.2.11], try #1 \r\n"]
[452.874149, "o", "\u001b[36mINFO\u001b[0m[0103] [certificates] Successfully started [rke-bundle-cert] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0103] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.11] \r\n"]
[452.874962, "o", "\u001b[36mINFO\u001b[0m[0103] Container [rke-bundle-cert] is still running on host [10.20.2.11]: stderr: [], stdout: [] \r\n"]
[453.875374, "o", "\u001b[36mINFO\u001b[0m[0104] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.11] \r\n"]
[453.878941, "o", "\u001b[36mINFO\u001b[0m[0104] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0104] Removing container [rke-bundle-cert] on host [10.20.2.11], try #1 \r\n"]
[453.919545, "o", "\u001b[36mINFO\u001b[0m[0104] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[454.041293, "o", "\u001b[36mINFO\u001b[0m[0104] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[454.248907, "o", "\u001b[36mINFO\u001b[0m[0104] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[454.249763, "o", "\u001b[36mINFO\u001b[0m[0104] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[454.365263, "o", "\u001b[36mINFO\u001b[0m[0105] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n"]
[454.367701, "o", "\u001b[36mINFO\u001b[0m[0105] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[454.484969, "o", "\u001b[36mINFO\u001b[0m[0105] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[454.683801, "o", "\u001b[36mINFO\u001b[0m[0105] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[454.684607, "o", "\u001b[36mINFO\u001b[0m[0105] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[454.801319, "o", "\u001b[36mINFO\u001b[0m[0105] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n"]
[454.803147, "o", "\u001b[36mINFO\u001b[0m[0105] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[455.561671, "o", "\u001b[36mINFO\u001b[0m[0106] Starting container [etcd-fix-perm] on host [10.20.2.12], try #1 \r\n"]
[455.904563, "o", "\u001b[36mINFO\u001b[0m[0106] Successfully started [etcd-fix-perm] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0106] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0106] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.12] \r\n"]
[455.906487, "o", "\u001b[36mINFO\u001b[0m[0106] Container [etcd-fix-perm] is still running on host [10.20.2.12]: stderr: [], stdout: [] \r\n"]
[456.906804, "o", "\u001b[36mINFO\u001b[0m[0107] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.12] \r\n"]
[456.91358, "o", "\u001b[36mINFO\u001b[0m[0107] Removing container [etcd-fix-perm] on host [10.20.2.12], try #1 \r\n"]
[456.943838, "o", "\u001b[36mINFO\u001b[0m[0107] [remove/etcd-fix-perm] Successfully removed container on host [10.20.2.12] \r\n"]
[456.94567, "o", "\u001b[36mINFO\u001b[0m[0107] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] on host [10.20.2.12], try #1 \r\n"]
[458.548184, "o", "\u001b[36mINFO\u001b[0m[0109] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] exists on host [10.20.2.12] \r\n"]
[458.82634, "o", "\u001b[36mINFO\u001b[0m[0109] Starting container [etcd] on host [10.20.2.12], try #1 \r\n"]
[458.984414, "o", "\u001b[36mINFO\u001b[0m[0109] [etcd] Successfully started [etcd] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0109] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.20.2.12] \r\n"]
[458.986406, "o", "\u001b[36mINFO\u001b[0m[0109] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[459.105931, "o", "\u001b[36mINFO\u001b[0m[0109] Starting container [etcd-rolling-snapshots] on host [10.20.2.12], try #1 \r\n"]
[459.26543, "o", "\u001b[36mINFO\u001b[0m[0109] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.20.2.12] \r\n"]
[464.275104, "o", "\u001b[36mINFO\u001b[0m[0114] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[464.42912, "o", "\u001b[36mINFO\u001b[0m[0115] Starting container [rke-bundle-cert] on host [10.20.2.12], try #1 \r\n"]
[464.682991, "o", "\u001b[36mINFO\u001b[0m[0115] [certificates] Successfully started [rke-bundle-cert] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0115] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.12] \r\n"]
[464.684332, "o", "\u001b[36mINFO\u001b[0m[0115] Container [rke-bundle-cert] is still running on host [10.20.2.12]: stderr: [], stdout: [] \r\n"]
[465.684702, "o", "\u001b[36mINFO\u001b[0m[0116] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.12] \r\n"]
[465.688056, "o", "\u001b[36mINFO\u001b[0m[0116] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0116] Removing container [rke-bundle-cert] on host [10.20.2.12], try #1 \r\n"]
[465.725203, "o", "\u001b[36mINFO\u001b[0m[0116] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[465.847998, "o", "\u001b[36mINFO\u001b[0m[0116] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[466.056114, "o", "\u001b[36mINFO\u001b[0m[0116] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[466.056763, "o", "\u001b[36mINFO\u001b[0m[0116] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[466.176269, "o", "\u001b[36mINFO\u001b[0m[0116] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n"]
[466.180077, "o", "\u001b[36mINFO\u001b[0m[0116] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[466.289251, "o", "\u001b[36mINFO\u001b[0m[0116] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[466.477457, "o", "\u001b[36mINFO\u001b[0m[0117] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[466.478246, "o", "\u001b[36mINFO\u001b[0m[0117] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[466.584741, "o", "\u001b[36mINFO\u001b[0m[0117] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0117] [etcd] Successfully started etcd plane.. Checking etcd cluster health \r\n"]
[466.870009, "o", "\u001b[36mINFO\u001b[0m[0117] [etcd] etcd host [10.20.2.10] reported healthy=true \r\n"]
[466.870669, "o", "\u001b[36mINFO\u001b[0m[0117] [controlplane] Building up Controller Plane.. \r\n\u001b[36mINFO\u001b[0m[0117] Checking if container [service-sidekick] is running on host [10.20.2.11], try #1 \r\n"]
[466.872972, "o", "\u001b[36mINFO\u001b[0m[0117] Checking if container [service-sidekick] is running on host [10.20.2.12], try #1 \r\n"]
[466.87312, "o", "\u001b[36mINFO\u001b[0m[0117] Checking if container [service-sidekick] is running on host [10.20.2.10], try #1 \r\n"]
[466.874409, "o", "\u001b[36mINFO\u001b[0m[0117] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[466.876275, "o", "\u001b[36mINFO\u001b[0m[0117] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[466.878629, "o", "\u001b[36mINFO\u001b[0m[0117] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[467.145177, "o", "\u001b[36mINFO\u001b[0m[0117] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.11] \r\n"]
[467.145797, "o", "\u001b[36mINFO\u001b[0m[0117] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.10] \r\n"]
[467.146319, "o", "\u001b[36mINFO\u001b[0m[0117] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.12] \r\n"]
[467.173987, "o", "\u001b[36mINFO\u001b[0m[0117] Starting container [kube-apiserver] on host [10.20.2.11], try #1 \r\n"]
[467.179102, "o", "\u001b[36mINFO\u001b[0m[0117] Starting container [kube-apiserver] on host [10.20.2.10], try #1 \r\n"]
[467.181845, "o", "\u001b[36mINFO\u001b[0m[0117] Starting container [kube-apiserver] on host [10.20.2.12], try #1 \r\n"]
[467.306896, "o", "\u001b[36mINFO\u001b[0m[0117] [controlplane] Successfully started [kube-apiserver] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0117] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.20.2.11] \r\n"]
[467.3434, "o", "\u001b[36mINFO\u001b[0m[0118] [controlplane] Successfully started [kube-apiserver] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0118] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.20.2.12] \r\n"]
[467.461861, "o", "\u001b[36mINFO\u001b[0m[0118] [controlplane] Successfully started [kube-apiserver] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0118] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.20.2.10] \r\n"]
[473.045547, "o", "\u001b[36mINFO\u001b[0m[0123] [healthcheck] service [kube-apiserver] on host [10.20.2.10] is healthy \r\n"]
[473.051027, "o", "\u001b[36mINFO\u001b[0m[0123] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[473.077863, "o", "\u001b[36mINFO\u001b[0m[0123] [healthcheck] service [kube-apiserver] on host [10.20.2.12] is healthy \r\n"]
[473.082669, "o", "\u001b[36mINFO\u001b[0m[0123] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[473.194319, "o", "\u001b[36mINFO\u001b[0m[0123] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[473.49372, "o", "\u001b[36mINFO\u001b[0m[0124] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[473.519171, "o", "\u001b[36mINFO\u001b[0m[0124] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[473.520029, "o", "\u001b[36mINFO\u001b[0m[0124] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[473.643004, "o", "\u001b[36mINFO\u001b[0m[0124] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[473.64508, "o", "\u001b[36mINFO\u001b[0m[0124] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.10] \r\n"]
[473.669443, "o", "\u001b[36mINFO\u001b[0m[0124] Starting container [kube-controller-manager] on host [10.20.2.10], try #1 \r\n"]
[473.702093, "o", "\u001b[36mINFO\u001b[0m[0124] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[473.703008, "o", "\u001b[36mINFO\u001b[0m[0124] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[473.791515, "o", "\u001b[36mINFO\u001b[0m[0124] [controlplane] Successfully started [kube-controller-manager] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0124] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.20.2.10] \r\n"]
[473.878644, "o", "\u001b[36mINFO\u001b[0m[0124] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n"]
[473.879874, "o", "\u001b[36mINFO\u001b[0m[0124] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.12] \r\n"]
[473.921787, "o", "\u001b[36mINFO\u001b[0m[0124] Starting container [kube-controller-manager] on host [10.20.2.12], try #1 \r\n"]
[474.081914, "o", "\u001b[36mINFO\u001b[0m[0124] [controlplane] Successfully started [kube-controller-manager] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0124] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.20.2.12] \r\n"]
[477.989665, "o", "\u001b[36mINFO\u001b[0m[0128] [healthcheck] service [kube-apiserver] on host [10.20.2.11] is healthy \r\n"]
[478.001745, "o", "\u001b[36mINFO\u001b[0m[0128] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[478.161549, "o", "\u001b[36mINFO\u001b[0m[0128] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[478.405522, "o", "\u001b[36mINFO\u001b[0m[0129] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[478.406254, "o", "\u001b[36mINFO\u001b[0m[0129] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[478.519901, "o", "\u001b[36mINFO\u001b[0m[0129] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n"]
[478.521233, "o", "\u001b[36mINFO\u001b[0m[0129] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.11] \r\n"]
[478.546266, "o", "\u001b[36mINFO\u001b[0m[0129] Starting container [kube-controller-manager] on host [10.20.2.11], try #1 \r\n"]
[478.672076, "o", "\u001b[36mINFO\u001b[0m[0129] [controlplane] Successfully started [kube-controller-manager] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0129] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.20.2.11] \r\n"]
[479.290506, "o", "\u001b[36mINFO\u001b[0m[0129] [healthcheck] service [kube-controller-manager] on host [10.20.2.10] is healthy \r\n"]
[479.295629, "o", "\u001b[36mINFO\u001b[0m[0129] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[479.403444, "o", "\u001b[36mINFO\u001b[0m[0130] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[479.586502, "o", "\u001b[36mINFO\u001b[0m[0130] [healthcheck] service [kube-controller-manager] on host [10.20.2.12] is healthy \r\n"]
[479.590861, "o", "\u001b[36mINFO\u001b[0m[0130] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[479.638217, "o", "\u001b[36mINFO\u001b[0m[0130] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[479.640327, "o", "\u001b[36mINFO\u001b[0m[0130] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[479.709962, "o", "\u001b[36mINFO\u001b[0m[0130] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[479.76058, "o", "\u001b[36mINFO\u001b[0m[0130] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[479.761963, "o", "\u001b[36mINFO\u001b[0m[0130] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.10] \r\n"]
[479.781229, "o", "\u001b[36mINFO\u001b[0m[0130] Starting container [kube-scheduler] on host [10.20.2.10], try #1 \r\n"]
[479.891732, "o", "\u001b[36mINFO\u001b[0m[0130] [controlplane] Successfully started [kube-scheduler] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0130] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.20.2.10] \r\n"]
[479.94048, "o", "\u001b[36mINFO\u001b[0m[0130] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[479.94117, "o", "\u001b[36mINFO\u001b[0m[0130] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[480.066832, "o", "\u001b[36mINFO\u001b[0m[0130] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n"]
[480.068245, "o", "\u001b[36mINFO\u001b[0m[0130] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.12] \r\n"]
[480.088378, "o", "\u001b[36mINFO\u001b[0m[0130] Starting container [kube-scheduler] on host [10.20.2.12], try #1 \r\n"]
[480.202048, "o", "\u001b[36mINFO\u001b[0m[0130] [controlplane] Successfully started [kube-scheduler] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0130] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.20.2.12] \r\n"]
[484.282714, "o", "\u001b[36mINFO\u001b[0m[0134] [healthcheck] service [kube-controller-manager] on host [10.20.2.11] is healthy \r\n"]
[484.288173, "o", "\u001b[36mINFO\u001b[0m[0134] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[484.405955, "o", "\u001b[36mINFO\u001b[0m[0135] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[484.650769, "o", "\u001b[36mINFO\u001b[0m[0135] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[484.651691, "o", "\u001b[36mINFO\u001b[0m[0135] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[484.784828, "o", "\u001b[36mINFO\u001b[0m[0135] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n"]
[484.78643, "o", "\u001b[36mINFO\u001b[0m[0135] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.11] \r\n"]
[484.812057, "o", "\u001b[36mINFO\u001b[0m[0135] Starting container [kube-scheduler] on host [10.20.2.11], try #1 \r\n"]
[484.927924, "o", "\u001b[36mINFO\u001b[0m[0135] [controlplane] Successfully started [kube-scheduler] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0135] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.20.2.11] \r\n"]
[485.39358, "o", "\u001b[36mINFO\u001b[0m[0136] [healthcheck] service [kube-scheduler] on host [10.20.2.10] is healthy \r\n"]
[485.398919, "o", "\u001b[36mINFO\u001b[0m[0136] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[485.508289, "o", "\u001b[36mINFO\u001b[0m[0136] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[485.728185, "o", "\u001b[36mINFO\u001b[0m[0136] [healthcheck] service [kube-scheduler] on host [10.20.2.12] is healthy \r\n"]
[485.734082, "o", "\u001b[36mINFO\u001b[0m[0136] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[485.751705, "o", "\u001b[36mINFO\u001b[0m[0136] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[485.752497, "o", "\u001b[36mINFO\u001b[0m[0136] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[485.840883, "o", "\u001b[36mINFO\u001b[0m[0136] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[485.91205, "o", "\u001b[36mINFO\u001b[0m[0136] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[486.068646, "o", "\u001b[36mINFO\u001b[0m[0136] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[486.069251, "o", "\u001b[36mINFO\u001b[0m[0136] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[486.210098, "o", "\u001b[36mINFO\u001b[0m[0136] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n"]
[490.482351, "o", "\u001b[36mINFO\u001b[0m[0141] [healthcheck] service [kube-scheduler] on host [10.20.2.11] is healthy \r\n"]
[490.486123, "o", "\u001b[36mINFO\u001b[0m[0141] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[490.591241, "o", "\u001b[36mINFO\u001b[0m[0141] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[490.84121, "o", "\u001b[36mINFO\u001b[0m[0141] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[490.842194, "o", "\u001b[36mINFO\u001b[0m[0141] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[490.962818, "o", "\u001b[36mINFO\u001b[0m[0141] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0141] [controlplane] Successfully started Controller Plane.. \r\n"]
[490.963319, "o", "\u001b[36mINFO\u001b[0m[0141] [authz] Creating rke-job-deployer ServiceAccount \r\n"]
[490.998079, "o", "\u001b[36mINFO\u001b[0m[0141] [authz] rke-job-deployer ServiceAccount created successfully \r\n\u001b[36mINFO\u001b[0m[0141] [authz] Creating system:node ClusterRoleBinding \r\n"]
[491.008671, "o", "\u001b[36mINFO\u001b[0m[0141] [authz] system:node ClusterRoleBinding created successfully \r\n\u001b[36mINFO\u001b[0m[0141] [authz] Creating kube-apiserver proxy ClusterRole and ClusterRoleBinding \r\n"]
[491.019767, "o", "\u001b[36mINFO\u001b[0m[0141] [authz] kube-apiserver proxy ClusterRole and ClusterRoleBinding created successfully \r\n"]
[491.022746, "o", "\u001b[36mINFO\u001b[0m[0141] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/c1.rkestate] \r\n"]
[491.023777, "o", "\u001b[36mINFO\u001b[0m[0141] [state] Saving full cluster state to Kubernetes \r\n"]
[491.04131, "o", "\u001b[36mINFO\u001b[0m[0141] [state] Successfully Saved full cluster state to Kubernetes ConfigMap: full-cluster-state \r\n"]
[491.042524, "o", "\u001b[36mINFO\u001b[0m[0141] [worker] Building up Worker Plane..          \r\n"]
[491.042974, "o", "\u001b[36mINFO\u001b[0m[0141] Checking if container [service-sidekick] is running on host [10.20.2.10], try #1 \r\n\u001b[36mINFO\u001b[0m[0141] Checking if container [service-sidekick] is running on host [10.20.2.12], try #1 \r\n\u001b[36mINFO\u001b[0m[0141] Checking if container [service-sidekick] is running on host [10.20.2.11], try #1 \r\n"]
[491.051984, "o", "\u001b[36mINFO\u001b[0m[0141] [sidekick] Sidekick container already created on host [10.20.2.10] \r\n"]
[491.052722, "o", "\u001b[36mINFO\u001b[0m[0141] [sidekick] Sidekick container already created on host [10.20.2.12] \r\n"]
[491.05374, "o", "\u001b[36mINFO\u001b[0m[0141] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.10] \r\n"]
[491.05425, "o", "\u001b[36mINFO\u001b[0m[0141] [sidekick] Sidekick container already created on host [10.20.2.11] \r\n"]
[491.05451, "o", "\u001b[36mINFO\u001b[0m[0141] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.12] \r\n"]
[491.056269, "o", "\u001b[36mINFO\u001b[0m[0141] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.11] \r\n"]
[491.075357, "o", "\u001b[36mINFO\u001b[0m[0141] Starting container [kubelet] on host [10.20.2.11], try #1 \r\n"]
[491.081144, "o", "\u001b[36mINFO\u001b[0m[0141] Starting container [kubelet] on host [10.20.2.12], try #1 \r\n"]
[491.083509, "o", "\u001b[36mINFO\u001b[0m[0141] Starting container [kubelet] on host [10.20.2.10], try #1 \r\n"]
[491.191665, "o", "\u001b[36mINFO\u001b[0m[0141] [worker] Successfully started [kubelet] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0141] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.11] \r\n"]
[491.200242, "o", "\u001b[36mINFO\u001b[0m[0141] [worker] Successfully started [kubelet] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0141] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.12] \r\n"]
[491.204629, "o", "\u001b[36mINFO\u001b[0m[0141] [worker] Successfully started [kubelet] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0141] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.10] \r\n"]
[491.367493, "o", "\u001b[36mINFO\u001b[0m[0142] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[491.378978, "o", "\u001b[36mINFO\u001b[0m[0142] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[491.391886, "o", "\u001b[36mINFO\u001b[0m[0142] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[491.485209, "o", "\u001b[36mINFO\u001b[0m[0142] Starting container [nginx-proxy] on host [10.20.2.21], try #1 \r\n"]
[491.641762, "o", "\u001b[36mINFO\u001b[0m[0142] Starting container [nginx-proxy] on host [10.20.2.20], try #1 \r\n"]
[491.641804, "o", "\u001b[36mINFO\u001b[0m[0142] Starting container [nginx-proxy] on host [10.20.2.22], try #1 \r\n"]
[491.643245, "o", "\u001b[36mINFO\u001b[0m[0142] [worker] Successfully started [nginx-proxy] container on host [10.20.2.21] \r\n"]
[491.646729, "o", "\u001b[36mINFO\u001b[0m[0142] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[491.771144, "o", "\u001b[36mINFO\u001b[0m[0142] Starting container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[491.815056, "o", "\u001b[36mINFO\u001b[0m[0142] [worker] Successfully started [nginx-proxy] container on host [10.20.2.22] \r\n"]
[491.819578, "o", "\u001b[36mINFO\u001b[0m[0142] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[491.852426, "o", "\u001b[36mINFO\u001b[0m[0142] [worker] Successfully started [nginx-proxy] container on host [10.20.2.20] \r\n"]
[491.855593, "o", "\u001b[36mINFO\u001b[0m[0142] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[492.023242, "o", "\u001b[36mINFO\u001b[0m[0142] Starting container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[492.027292, "o", "\u001b[36mINFO\u001b[0m[0142] Starting container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[492.076919, "o", "\u001b[36mINFO\u001b[0m[0142] [worker] Successfully started [rke-log-linker] container on host [10.20.2.21] \r\n"]
[492.079676, "o", "\u001b[36mINFO\u001b[0m[0142] Removing container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[492.217683, "o", "\u001b[36mINFO\u001b[0m[0142] [remove/rke-log-linker] Successfully removed container on host [10.20.2.21] \r\n\u001b[36mINFO\u001b[0m[0142] Checking if container [service-sidekick] is running on host [10.20.2.21], try #1 \r\n"]
[492.221747, "o", "\u001b[36mINFO\u001b[0m[0142] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[492.351708, "o", "\u001b[36mINFO\u001b[0m[0143] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.21] \r\n"]
[492.37695, "o", "\u001b[36mINFO\u001b[0m[0143] [worker] Successfully started [rke-log-linker] container on host [10.20.2.20] \r\n"]
[492.378439, "o", "\u001b[36mINFO\u001b[0m[0143] Removing container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[492.379571, "o", "\u001b[36mINFO\u001b[0m[0143] [worker] Successfully started [rke-log-linker] container on host [10.20.2.22] \r\n"]
[492.380916, "o", "\u001b[36mINFO\u001b[0m[0143] Removing container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[492.381058, "o", "\u001b[36mINFO\u001b[0m[0143] Starting container [kubelet] on host [10.20.2.21], try #1 \r\n"]
[492.507268, "o", "\u001b[36mINFO\u001b[0m[0143] [worker] Successfully started [kubelet] container on host [10.20.2.21] \r\n\u001b[36mINFO\u001b[0m[0143] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.21] \r\n"]
[492.514184, "o", "\u001b[36mINFO\u001b[0m[0143] [remove/rke-log-linker] Successfully removed container on host [10.20.2.20] \r\n\u001b[36mINFO\u001b[0m[0143] Checking if container [service-sidekick] is running on host [10.20.2.20], try #1 \r\n"]
[492.517637, "o", "\u001b[36mINFO\u001b[0m[0143] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[492.520954, "o", "\u001b[36mINFO\u001b[0m[0143] [remove/rke-log-linker] Successfully removed container on host [10.20.2.22] \r\n\u001b[36mINFO\u001b[0m[0143] Checking if container [service-sidekick] is running on host [10.20.2.22], try #1 \r\n"]
[492.524291, "o", "\u001b[36mINFO\u001b[0m[0143] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[492.688021, "o", "\u001b[36mINFO\u001b[0m[0143] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.20] \r\n"]
[492.68846, "o", "\u001b[36mINFO\u001b[0m[0143] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.22] \r\n"]
[492.714856, "o", "\u001b[36mINFO\u001b[0m[0143] Starting container [kubelet] on host [10.20.2.20], try #1 \r\n"]
[492.719925, "o", "\u001b[36mINFO\u001b[0m[0143] Starting container [kubelet] on host [10.20.2.22], try #1 \r\n"]
[492.854746, "o", "\u001b[36mINFO\u001b[0m[0143] [worker] Successfully started [kubelet] container on host [10.20.2.22] \r\n\u001b[36mINFO\u001b[0m[0143] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.22] \r\n"]
[492.874926, "o", "\u001b[36mINFO\u001b[0m[0143] [worker] Successfully started [kubelet] container on host [10.20.2.20] \r\n\u001b[36mINFO\u001b[0m[0143] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.20] \r\n"]
[502.018389, "o", "\u001b[36mINFO\u001b[0m[0152] [healthcheck] service [kubelet] on host [10.20.2.11] is healthy \r\n"]
[502.022497, "o", "\u001b[36mINFO\u001b[0m[0152] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[502.027387, "o", "\u001b[36mINFO\u001b[0m[0152] [healthcheck] service [kubelet] on host [10.20.2.10] is healthy \r\n"]
[502.031771, "o", "\u001b[36mINFO\u001b[0m[0152] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[502.033484, "o", "\u001b[36mINFO\u001b[0m[0152] [healthcheck] service [kubelet] on host [10.20.2.12] is healthy \r\n"]
[502.043015, "o", "\u001b[36mINFO\u001b[0m[0152] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[502.271063, "o", "\u001b[36mINFO\u001b[0m[0152] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[502.271853, "o", "\u001b[36mINFO\u001b[0m[0152] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[502.273154, "o", "\u001b[36mINFO\u001b[0m[0152] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[502.559676, "o", "\u001b[36mINFO\u001b[0m[0153] [worker] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[502.560494, "o", "\u001b[36mINFO\u001b[0m[0153] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[502.563444, "o", "\u001b[36mINFO\u001b[0m[0153] [worker] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[502.565069, "o", "\u001b[36mINFO\u001b[0m[0153] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[502.571816, "o", "\u001b[36mINFO\u001b[0m[0153] [worker] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[502.573529, "o", "\u001b[36mINFO\u001b[0m[0153] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[502.697849, "o", "\u001b[36mINFO\u001b[0m[0153] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[502.699496, "o", "\u001b[36mINFO\u001b[0m[0153] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.10] \r\n"]
[502.707328, "o", "\u001b[36mINFO\u001b[0m[0153] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n"]
[502.707926, "o", "\u001b[36mINFO\u001b[0m[0153] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n"]
[502.708853, "o", "\u001b[36mINFO\u001b[0m[0153] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.12] \r\n"]
[502.709145, "o", "\u001b[36mINFO\u001b[0m[0153] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.11] \r\n"]
[502.721146, "o", "\u001b[36mINFO\u001b[0m[0153] Starting container [kube-proxy] on host [10.20.2.10], try #1 \r\n"]
[502.730351, "o", "\u001b[36mINFO\u001b[0m[0153] Starting container [kube-proxy] on host [10.20.2.11], try #1 \r\n\u001b[36mINFO\u001b[0m[0153] Starting container [kube-proxy] on host [10.20.2.12], try #1 \r\n"]
[502.814212, "o", "\u001b[36mINFO\u001b[0m[0153] [worker] Successfully started [kube-proxy] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0153] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.10] \r\n"]
[502.919909, "o", "\u001b[36mINFO\u001b[0m[0153] [worker] Successfully started [kube-proxy] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0153] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.11] \r\n"]
[502.920233, "o", "\u001b[36mINFO\u001b[0m[0153] [worker] Successfully started [kube-proxy] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0153] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.12] \r\n"]
[503.123654, "o", "\u001b[36mINFO\u001b[0m[0153] [healthcheck] service [kube-proxy] on host [10.20.2.10] is healthy \r\n"]
[503.128221, "o", "\u001b[36mINFO\u001b[0m[0153] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[503.214259, "o", "\u001b[36mINFO\u001b[0m[0153] [healthcheck] service [kube-proxy] on host [10.20.2.11] is healthy \r\n"]
[503.219036, "o", "\u001b[36mINFO\u001b[0m[0153] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[503.242462, "o", "\u001b[36mINFO\u001b[0m[0153] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[503.255861, "o", "\u001b[36mINFO\u001b[0m[0153] [healthcheck] service [kube-proxy] on host [10.20.2.12] is healthy \r\n"]
[503.262185, "o", "\u001b[36mINFO\u001b[0m[0153] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[503.349914, "o", "\u001b[36mINFO\u001b[0m[0154] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[503.358324, "o", "\u001b[36mINFO\u001b[0m[0154] [healthcheck] service [kubelet] on host [10.20.2.21] is healthy \r\n"]
[503.362559, "o", "\u001b[36mINFO\u001b[0m[0154] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[503.440567, "o", "\u001b[36mINFO\u001b[0m[0154] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[503.546819, "o", "\u001b[36mINFO\u001b[0m[0154] Starting container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[503.591968, "o", "\u001b[36mINFO\u001b[0m[0154] [worker] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[503.597036, "o", "\u001b[36mINFO\u001b[0m[0154] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[503.700947, "o", "\u001b[36mINFO\u001b[0m[0154] [worker] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[503.703077, "o", "\u001b[36mINFO\u001b[0m[0154] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[503.737333, "o", "\u001b[36mINFO\u001b[0m[0154] [worker] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[503.741664, "o", "\u001b[36mINFO\u001b[0m[0154] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[503.742405, "o", "\u001b[36mINFO\u001b[0m[0154] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[503.818111, "o", "\u001b[36mINFO\u001b[0m[0154] [worker] Successfully started [rke-log-linker] container on host [10.20.2.21] \r\n"]
[503.82016, "o", "\u001b[36mINFO\u001b[0m[0154] Removing container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[503.832098, "o", "\u001b[36mINFO\u001b[0m[0154] [healthcheck] service [kubelet] on host [10.20.2.20] is healthy \r\n"]
[503.838168, "o", "\u001b[36mINFO\u001b[0m[0154] [healthcheck] service [kubelet] on host [10.20.2.22] is healthy \r\n"]
[503.838847, "o", "\u001b[36mINFO\u001b[0m[0154] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[503.841243, "o", "\u001b[36mINFO\u001b[0m[0154] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n"]
[503.843353, "o", "\u001b[36mINFO\u001b[0m[0154] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[503.879407, "o", "\u001b[36mINFO\u001b[0m[0154] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n"]
[504.025518, "o", "\u001b[36mINFO\u001b[0m[0154] Starting container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[504.027453, "o", "\u001b[36mINFO\u001b[0m[0154] Starting container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[504.157617, "o", "\u001b[36mINFO\u001b[0m[0154] [remove/rke-log-linker] Successfully removed container on host [10.20.2.21] \r\n"]
[504.161372, "o", "\u001b[36mINFO\u001b[0m[0154] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.21] \r\n"]
[504.188229, "o", "\u001b[36mINFO\u001b[0m[0154] Starting container [kube-proxy] on host [10.20.2.21], try #1 \r\n"]
[504.303554, "o", "\u001b[36mINFO\u001b[0m[0154] [worker] Successfully started [kube-proxy] container on host [10.20.2.21] \r\n\u001b[36mINFO\u001b[0m[0154] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.21] \r\n"]
[504.365712, "o", "\u001b[36mINFO\u001b[0m[0155] [worker] Successfully started [rke-log-linker] container on host [10.20.2.22] \r\n"]
[504.366435, "o", "\u001b[36mINFO\u001b[0m[0155] Removing container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[504.392078, "o", "\u001b[36mINFO\u001b[0m[0155] [worker] Successfully started [rke-log-linker] container on host [10.20.2.20] \r\n"]
[504.392894, "o", "\u001b[36mINFO\u001b[0m[0155] Removing container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[504.488162, "o", "\u001b[36mINFO\u001b[0m[0155] [remove/rke-log-linker] Successfully removed container on host [10.20.2.22] \r\n"]
[504.489455, "o", "\u001b[36mINFO\u001b[0m[0155] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.22] \r\n"]
[504.510618, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [kube-proxy] on host [10.20.2.22], try #1 \r\n"]
[504.516219, "o", "\u001b[36mINFO\u001b[0m[0155] [remove/rke-log-linker] Successfully removed container on host [10.20.2.20] \r\n"]
[504.518801, "o", "\u001b[36mINFO\u001b[0m[0155] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.20.2.20] \r\n"]
[504.545938, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [kube-proxy] on host [10.20.2.20], try #1 \r\n"]
[504.579499, "o", "\u001b[36mINFO\u001b[0m[0155] [healthcheck] service [kube-proxy] on host [10.20.2.21] is healthy \r\n"]
[504.586143, "o", "\u001b[36mINFO\u001b[0m[0155] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[504.611713, "o", "\u001b[36mINFO\u001b[0m[0155] [worker] Successfully started [kube-proxy] container on host [10.20.2.22] \r\n\u001b[36mINFO\u001b[0m[0155] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.22] \r\n"]
[504.706411, "o", "\u001b[36mINFO\u001b[0m[0155] [worker] Successfully started [kube-proxy] container on host [10.20.2.20] \r\n\u001b[36mINFO\u001b[0m[0155] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.20] \r\n"]
[504.707841, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[504.882831, "o", "\u001b[36mINFO\u001b[0m[0155] [healthcheck] service [kube-proxy] on host [10.20.2.22] is healthy \r\n"]
[504.887442, "o", "\u001b[36mINFO\u001b[0m[0155] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[504.978225, "o", "\u001b[36mINFO\u001b[0m[0155] [healthcheck] service [kube-proxy] on host [10.20.2.20] is healthy \r\n"]
[504.982167, "o", "\u001b[36mINFO\u001b[0m[0155] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[505.001915, "o", "\u001b[36mINFO\u001b[0m[0155] [worker] Successfully started [rke-log-linker] container on host [10.20.2.21] \r\n"]
[505.004946, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[505.097732, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[505.141033, "o", "\u001b[36mINFO\u001b[0m[0155] Removing container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[505.153333, "o", "\u001b[36mINFO\u001b[0m[0155] [remove/rke-log-linker] Successfully removed container on host [10.20.2.21] \r\n"]
[505.302855, "o", "\u001b[36mINFO\u001b[0m[0155] [worker] Successfully started [rke-log-linker] container on host [10.20.2.22] \r\n"]
[505.304744, "o", "\u001b[36mINFO\u001b[0m[0155] Removing container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[505.334224, "o", "\u001b[36mINFO\u001b[0m[0156] [worker] Successfully started [rke-log-linker] container on host [10.20.2.20] \r\n"]
[505.33617, "o", "\u001b[36mINFO\u001b[0m[0156] Removing container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[505.429087, "o", "\u001b[36mINFO\u001b[0m[0156] [remove/rke-log-linker] Successfully removed container on host [10.20.2.22] \r\n"]
[505.470212, "o", "\u001b[36mINFO\u001b[0m[0156] [remove/rke-log-linker] Successfully removed container on host [10.20.2.20] \r\n"]
[505.470359, "o", "\u001b[36mINFO\u001b[0m[0156] [worker] Successfully started Worker Plane.. \r\n"]
[505.473122, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[505.473255, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[505.473403, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[505.47356, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[505.473735, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[505.475113, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[505.922306, "o", "\u001b[36mINFO\u001b[0m[0156] Starting container [rke-log-cleaner] on host [10.20.2.20], try #1 \r\n"]
[505.925347, "o", "\u001b[36mINFO\u001b[0m[0156] Starting container [rke-log-cleaner] on host [10.20.2.21], try #1 \r\n"]
[505.929806, "o", "\u001b[36mINFO\u001b[0m[0156] Starting container [rke-log-cleaner] on host [10.20.2.22], try #1 \r\n"]
[505.931439, "o", "\u001b[36mINFO\u001b[0m[0156] Starting container [rke-log-cleaner] on host [10.20.2.11], try #1 \r\n"]
[505.940189, "o", "\u001b[36mINFO\u001b[0m[0156] Starting container [rke-log-cleaner] on host [10.20.2.12], try #1 \r\n"]
[505.97161, "o", "\u001b[36mINFO\u001b[0m[0156] Starting container [rke-log-cleaner] on host [10.20.2.10], try #1 \r\n"]
[506.219888, "o", "\u001b[36mINFO\u001b[0m[0156] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.22] \r\n"]
[506.222205, "o", "\u001b[36mINFO\u001b[0m[0156] Removing container [rke-log-cleaner] on host [10.20.2.22], try #1 \r\n"]
[506.237278, "o", "\u001b[36mINFO\u001b[0m[0156] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.21] \r\n"]
[506.238556, "o", "\u001b[36mINFO\u001b[0m[0156] Removing container [rke-log-cleaner] on host [10.20.2.21], try #1 \r\n"]
[506.252663, "o", "\u001b[36mINFO\u001b[0m[0156] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.11] \r\n"]
[506.255331, "o", "\u001b[36mINFO\u001b[0m[0156] Removing container [rke-log-cleaner] on host [10.20.2.11], try #1 \r\n"]
[506.25592, "o", "\u001b[36mINFO\u001b[0m[0156] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.10] \r\n"]
[506.259055, "o", "\u001b[36mINFO\u001b[0m[0156] Removing container [rke-log-cleaner] on host [10.20.2.10], try #1 \r\n"]
[506.268995, "o", "\u001b[36mINFO\u001b[0m[0156] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.20] \r\n"]
[506.269193, "o", "\u001b[36mINFO\u001b[0m[0156] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.12] \r\n"]
[506.273099, "o", "\u001b[36mINFO\u001b[0m[0156] Removing container [rke-log-cleaner] on host [10.20.2.12], try #1 \r\n"]
[506.273571, "o", "\u001b[36mINFO\u001b[0m[0156] Removing container [rke-log-cleaner] on host [10.20.2.20], try #1 \r\n"]
[506.36786, "o", "\u001b[36mINFO\u001b[0m[0157] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.22] \r\n"]
[506.373384, "o", "\u001b[36mINFO\u001b[0m[0157] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.21] \r\n"]
[506.405627, "o", "\u001b[36mINFO\u001b[0m[0157] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0157] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.20] \r\n"]
[506.416901, "o", "\u001b[36mINFO\u001b[0m[0157] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.11] \r\n"]
[506.420138, "o", "\u001b[36mINFO\u001b[0m[0157] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0157] [sync] Syncing nodes Labels and Taints       \r\n"]
[506.844164, "o", "\u001b[36mINFO\u001b[0m[0157] [sync] Successfully synced nodes Labels and Taints \r\n"]
[506.844577, "o", "\u001b[36mINFO\u001b[0m[0157] [network] Setting up network plugin: calico  \r\n"]
[506.848643, "o", "\u001b[36mINFO\u001b[0m[0157] [addons] Saving ConfigMap for addon rke-network-plugin to Kubernetes "]
[506.849029, "o", "\r\n"]
[506.893176, "o", "\u001b[36mINFO\u001b[0m[0157] [addons] Successfully saved ConfigMap for addon rke-network-plugin to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0157] [addons] Executing deploy job rke-network-plugin \r\n"]
[517.050754, "o", "\u001b[36mINFO\u001b[0m[0167] [addons] Setting up coredns                  \r\n"]
[517.051321, "o", "\u001b[36mINFO\u001b[0m[0167] [addons] Saving ConfigMap for addon rke-coredns-addon to Kubernetes \r\n"]
[517.091938, "o", "\u001b[36mINFO\u001b[0m[0167] [addons] Successfully saved ConfigMap for addon rke-coredns-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0167] [addons] Executing deploy job rke-coredns-addon \r\n"]
[522.117061, "o", "\u001b[36mINFO\u001b[0m[0172] [addons] CoreDNS deployed successfully       \r\n\u001b[36mINFO\u001b[0m[0172] [dns] DNS provider coredns deployed successfully \r\n"]
[522.121256, "o", "\u001b[36mINFO\u001b[0m[0172] [addons] Setting up Metrics Server           \r\n"]
[522.124195, "o", "\u001b[36mINFO\u001b[0m[0172] [addons] Saving ConfigMap for addon rke-metrics-addon to Kubernetes \r\n"]
[522.241704, "o", "\u001b[36mINFO\u001b[0m[0172] [addons] Successfully saved ConfigMap for addon rke-metrics-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0172] [addons] Executing deploy job rke-metrics-addon \r\n"]
[532.817378, "o", "\u001b[36mINFO\u001b[0m[0183] [addons] Metrics Server deployed successfully \r\n\u001b[36mINFO\u001b[0m[0183] [ingress] Setting up nginx ingress controller \r\n"]
[532.817924, "o", "\u001b[36mINFO\u001b[0m[0183] [addons] Saving ConfigMap for addon rke-ingress-controller to Kubernetes \r\n"]
[532.836521, "o", "\u001b[36mINFO\u001b[0m[0183] [addons] Successfully saved ConfigMap for addon rke-ingress-controller to Kubernetes \r\n"]
[532.83666, "o", "\u001b[36mINFO\u001b[0m[0183] [addons] Executing deploy job rke-ingress-controller \r\n"]
[537.872241, "o", "\u001b[36mINFO\u001b[0m[0188] [ingress] ingress controller nginx deployed successfully \r\n\u001b[36mINFO\u001b[0m[0188] [addons] Setting up user addons              \r\n"]
[537.906813, "o", "\u001b[36mINFO\u001b[0m[0188] [addons] no user addons defined              \r\n"]
[537.907083, "o", "\u001b[36mINFO\u001b[0m[0188] Finished building Kubernetes cluster successfully \r\n"]
[537.914569, "o", "\r\nreal\t3m8.847s\r\nuser\t0m3.248s\r\nsys\t0m0.498s\r\n"]
[568.437211, "o", "NAME              STATUS   ROLES               AGE   VERSION\r\nnode/10.20.2.10   Ready    controlplane,etcd   69s   v1.18.12\r\nnode/10.20.2.11   Ready    controlplane,etcd   69s   v1.18.12\r\nnode/10.20.2.12   Ready    controlplane,etcd   70s   v1.18.12\r\nnode/10.20.2.20   Ready    worker              68s   v1.18.12\r\nnode/10.20.2.21   Ready    worker              69s   v1.18.12\r\nnode/10.20.2.22   Ready    worker              68s   v1.18.12\r\n"]
[568.448565, "o", "\r\nNAMESPACE       NAME                                           READY   STATUS      RESTARTS   AGE\r\n"]
[568.448631, "o", "ingress-nginx   pod/default-http-backend-5b564dd459-kpngl      1/1     Running     0          34s\r\ningress-nginx   pod/nginx-ingress-controller-hdcbp             1/1     Running     0          34s\r\ningress-nginx   pod/nginx-ingress-controller-rw6vk        "]
[568.448684, "o", "     1/1     Running     0          34s\r\ningress-nginx   pod/nginx-ingress-controller-zvh4c             1/1     Running     0"]
[568.448708, "o", "          34s\r\nkube-system     pod/calico-kube-controllers-6c6fc476f6-sxskt   1/1     "]
[568.448724, "o", "Running     0          "]
[568.448741, "o", "53s\r\nkube-system     pod/calico-node-c5pm4        "]
[568.448995, "o", "                  1/1     Running     0          53s\r\nkube-system     pod/calico-node-hb22h                          1/1     Running     0          53s\r\nkube-system     pod/calico-node-klvbb                          1/1     Running     0          53s\r\nkube-system     pod/calico-node-m7lf2                          1/1     Running     0          53s"]
[568.449111, "o", "\r\nkube-system     pod/calico-node-mlc8z                          1/1     Running     0          53s\r\nkube-system     pod/calico-node-zzwl9                          1/1     Running     0          53s\r\nkube-system     pod/coredns-5dd4dfcb45-dl2nk                   1/1     Running     0          13s\r\nkube-system     pod/coredns-5dd4dfcb45-fcbwb                   1/1     Running     0          50s\r\nkube-system     pod/coredns-autoscaler-557f965569-bgfvj        1/1     Running     0          49s\r\nkube-system     pod/metrics-server-77956db857-69jkz            1/1     Running     0          41s\r\nkube-system     pod/rke-coredns-addon-deploy-job-7l8zr         0/1     Completed   0          52s\r\nkube-system     pod/rke-ingress-controller-deploy-job-2mwl2    0/1     Completed   0          36s\r\nkube-system     "]
[568.449188, "o", "pod/rke-metrics-addon-deploy-job-99gpv         0/1     Completed   0          46s\r\nkube-system     pod/rke-network-plugin-deploy-job-tmg5m        0/1     Completed   0        "]
[568.449576, "o", "  62s\r\n"]
[568.640275, "o", "cd /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ && tee /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/c1.yml </home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/c1-v1.19.4-rancher1-1.yml \\\r\n            && ./files/rke up --config=/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/c1.yml\r\n"]
[568.727963, "o", "---\r\ncluster_name: c1\r\nkubernetes_version: v1.19.4-rancher1-1\r\n\r\nssh_agent_auth: true\r\n\r\nnodes:\r\n  - &m1\r\n    address: 10.20.2.10\r\n    user: centos\r\n    role: [etcd, controlplane]\r\n  - <<: *m1\r\n    address: 10.20.2.11\r\n  - <<: *m1\r\n    address: 10.20.2.12\r\n\r\n  - &n1\r\n    address: 10.20.2.20\r\n    user: centos\r\n    role: [worker]\r\n  - <<: *n1\r\n    address: 10.20.2.21\r\n  - <<: *n1\r\n    address: 10.20.2.22\r\n\r\nnetwork:\r\n  plugin: calico\r\n\r\nprivate_registries:\r\n  - url: 10.8.101.2:5000\r\n    is_default: true\r\n"]
[568.749697, "o", "\u001b[36mINFO\u001b[0m[0000] Running RKE version: v1.2.3                  \r\n"]
[568.750948, "o", "\u001b[36mINFO\u001b[0m[0000] Initiating Kubernetes cluster                \r\n"]
[568.77331, "o", "\u001b[36mINFO\u001b[0m[0000] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates \r\n"]
[568.773426, "o", "\u001b[36mINFO\u001b[0m[0000] [certificates] Generating admin certificates and kubeconfig \r\n"]
[568.77651, "o", "\u001b[36mINFO\u001b[0m[0000] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/c1.rkestate] \r\n"]
[568.779389, "o", "\u001b[36mINFO\u001b[0m[0000] Building Kubernetes cluster                  \r\n"]
[568.779449, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.20]  \r\n"]
[568.779542, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.21]  \r\n"]
[568.779806, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.22]  \r\n"]
[568.779875, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.11]  \r\n"]
[568.779972, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.12]  \r\n"]
[568.780064, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.20.2.10]  "]
[568.780279, "o", "\r\n"]
[569.165107, "o", "\u001b[36mINFO\u001b[0m[0000] [network] No hosts added existing cluster, skipping port check \r\n"]
[569.165231, "o", "\u001b[36mINFO\u001b[0m[0000] [certificates] Deploying kubernetes certificates to Cluster nodes \r\n"]
[569.165712, "o", "\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.20.2.20], try #1 \r\n\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.20.2.22], try #1 \r\n\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.20.2.21], try #1 \r\n\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.20.2.11], try #1 \r\n\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.20.2.10], try #1 \r\n\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.20.2.12], try #1 \r\n"]
[569.175223, "o", "\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[569.178686, "o", "\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[569.178875, "o", "\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[569.182802, "o", "\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[569.185007, "o", "\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[569.670718, "o", "\u001b[36mINFO\u001b[0m[0000] Starting container [cert-deployer] on host [10.20.2.22], try #1 \r\n"]
[569.671451, "o", "\u001b[36mINFO\u001b[0m[0000] Starting container [cert-deployer] on host [10.20.2.21], try #1 \r\n"]
[569.672885, "o", "\u001b[36mINFO\u001b[0m[0000] Starting container [cert-deployer] on host [10.20.2.20], try #1 \r\n"]
[569.803661, "o", "\u001b[36mINFO\u001b[0m[0001] Starting container [cert-deployer] on host [10.20.2.10], try #1 \r\n"]
[569.807584, "o", "\u001b[36mINFO\u001b[0m[0001] Starting container [cert-deployer] on host [10.20.2.11], try #1 \r\n"]
[569.811058, "o", "\u001b[36mINFO\u001b[0m[0001] Starting container [cert-deployer] on host [10.20.2.12], try #1 \r\n"]
[570.178086, "o", "\u001b[36mINFO\u001b[0m[0001] Checking if container [cert-deployer] is running on host [10.20.2.20], try #1 \r\n"]
[570.182993, "o", "\u001b[36mINFO\u001b[0m[0001] Checking if container [cert-deployer] is running on host [10.20.2.22], try #1 \r\n"]
[570.208201, "o", "\u001b[36mINFO\u001b[0m[0001] Checking if container [cert-deployer] is running on host [10.20.2.10], try #1 \r\n"]
[570.210408, "o", "\u001b[36mINFO\u001b[0m[0001] Checking if container [cert-deployer] is running on host [10.20.2.12], try #1 \r\n"]
[570.217664, "o", "\u001b[36mINFO\u001b[0m[0001] Checking if container [cert-deployer] is running on host [10.20.2.11], try #1 \r\n"]
[570.277009, "o", "\u001b[36mINFO\u001b[0m[0001] Checking if container [cert-deployer] is running on host [10.20.2.21], try #1 \r\n"]
[575.188261, "o", "\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [10.20.2.20], try #1 \r\n"]
[575.193271, "o", "\u001b[36mINFO\u001b[0m[0006] Removing container [cert-deployer] on host [10.20.2.20], try #1 \r\n"]
[575.195731, "o", "\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [10.20.2.22], try #1 \r\n"]
[575.202992, "o", "\u001b[36mINFO\u001b[0m[0006] Removing container [cert-deployer] on host [10.20.2.22], try #1 \r\n"]
[575.21673, "o", "\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [10.20.2.10], try #1 \r\n"]
[575.220328, "o", "\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [10.20.2.12], try #1 \r\n"]
[575.222057, "o", "\u001b[36mINFO\u001b[0m[0006] Removing container [cert-deployer] on host [10.20.2.10], try #1 \r\n"]
[575.224607, "o", "\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [10.20.2.11], try #1 \r\n\u001b[36mINFO\u001b[0m[0006] Removing container [cert-deployer] on host [10.20.2.12], try #1 \r\n"]
[575.228534, "o", "\u001b[36mINFO\u001b[0m[0006] Removing container [cert-deployer] on host [10.20.2.11], try #1 \r\n"]
[575.287302, "o", "\u001b[36mINFO\u001b[0m[0006] Checking if container [cert-deployer] is running on host [10.20.2.21], try #1 \r\n"]
[575.291766, "o", "\u001b[36mINFO\u001b[0m[0006] Removing container [cert-deployer] on host [10.20.2.21], try #1 \r\n"]
[575.301044, "o", "\u001b[36mINFO\u001b[0m[0006] [reconcile] Rebuilding and updating local kube config \r\n"]
[575.301243, "o", "\u001b[36mINFO\u001b[0m[0006] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_c1.yml] \r\n"]
[575.308276, "o", "\u001b[36mINFO\u001b[0m[0006] [reconcile] host [10.20.2.10] is active master on the cluster \r\n\u001b[36mINFO\u001b[0m[0006] [certificates] Successfully deployed kubernetes certificates to Cluster nodes \r\n"]
[575.308316, "o", "\u001b[36mINFO\u001b[0m[0006] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.20.2.10] \r\n"]
[575.310256, "o", "\u001b[36mINFO\u001b[0m[0006] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[575.421371, "o", "\u001b[36mINFO\u001b[0m[0006] Starting container [file-deployer] on host [10.20.2.10], try #1 \r\n"]
[575.673518, "o", "\u001b[36mINFO\u001b[0m[0006] Successfully started [file-deployer] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0006] Waiting for [file-deployer] container to exit on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0006] Waiting for [file-deployer] container to exit on host [10.20.2.10] \r\n"]
[575.676225, "o", "\u001b[36mINFO\u001b[0m[0006] Container [file-deployer] is still running on host [10.20.2.10]: stderr: [], stdout: [] \r\n"]
[576.676577, "o", "\u001b[36mINFO\u001b[0m[0007] Waiting for [file-deployer] container to exit on host [10.20.2.10] \r\n"]
[576.683457, "o", "\u001b[36mINFO\u001b[0m[0007] Removing container [file-deployer] on host [10.20.2.10], try #1 \r\n"]
[576.706181, "o", "\u001b[36mINFO\u001b[0m[0007] [remove/file-deployer] Successfully removed container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0007] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.20.2.11] \r\n"]
[576.710882, "o", "\u001b[36mINFO\u001b[0m[0007] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[576.817851, "o", "\u001b[36mINFO\u001b[0m[0008] Starting container [file-deployer] on host [10.20.2.11], try #1 \r\n"]
[577.036967, "o", "\u001b[36mINFO\u001b[0m[0008] Successfully started [file-deployer] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0008] Waiting for [file-deployer] container to exit on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0008] Waiting for [file-deployer] container to exit on host [10.20.2.11] \r\n"]
[577.03847, "o", "\u001b[36mINFO\u001b[0m[0008] Container [file-deployer] is still running on host [10.20.2.11]: stderr: [], stdout: [] \r\n"]
[578.038758, "o", "\u001b[36mINFO\u001b[0m[0009] Waiting for [file-deployer] container to exit on host [10.20.2.11] \r\n"]
[578.044353, "o", "\u001b[36mINFO\u001b[0m[0009] Removing container [file-deployer] on host [10.20.2.11], try #1 \r\n"]
[578.065563, "o", "\u001b[36mINFO\u001b[0m[0009] [remove/file-deployer] Successfully removed container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0009] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.20.2.12] \r\n"]
[578.067877, "o", "\u001b[36mINFO\u001b[0m[0009] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[578.18769, "o", "\u001b[36mINFO\u001b[0m[0009] Starting container [file-deployer] on host [10.20.2.12], try #1 \r\n"]
[578.435001, "o", "\u001b[36mINFO\u001b[0m[0009] Successfully started [file-deployer] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0009] Waiting for [file-deployer] container to exit on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0009] Waiting for [file-deployer] container to exit on host [10.20.2.12] \r\n"]
[578.436088, "o", "\u001b[36mINFO\u001b[0m[0009] Container [file-deployer] is still running on host [10.20.2.12]: stderr: [], stdout: [] \r\n"]
[579.436332, "o", "\u001b[36mINFO\u001b[0m[0010] Waiting for [file-deployer] container to exit on host [10.20.2.12] \r\n"]
[579.439238, "o", "\u001b[36mINFO\u001b[0m[0010] Removing container [file-deployer] on host [10.20.2.12], try #1 \r\n"]
[579.454126, "o", "\u001b[36mINFO\u001b[0m[0010] [remove/file-deployer] Successfully removed container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0010] [/etc/kubernetes/audit-policy.yaml] Successfully deployed audit policy file to Cluster control nodes \r\n\u001b[36mINFO\u001b[0m[0010] [reconcile] Reconciling cluster state        \r\n"]
[579.455486, "o", "\u001b[36mINFO\u001b[0m[0010] [reconcile] Check etcd hosts to be deleted   \r\n"]
[579.45638, "o", "\u001b[36mINFO\u001b[0m[0010] [reconcile] Check etcd hosts to be added     \r\n"]
[579.457168, "o", "\u001b[36mINFO\u001b[0m[0010] [reconcile] Rebuilding and updating local kube config \r\n"]
[579.457307, "o", "\u001b[36mINFO\u001b[0m[0010] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_c1.yml] \r\n"]
[579.460062, "o", "\u001b[36mINFO\u001b[0m[0010] [reconcile] host [10.20.2.10] is active master on the cluster \r\n\u001b[36mINFO\u001b[0m[0010] [reconcile] Reconciled cluster state successfully \r\n\u001b[36mINFO\u001b[0m[0010] max_unavailable_worker got rounded down to 0, resetting to 1 \r\n\u001b[36mINFO\u001b[0m[0010] Setting maxUnavailable for worker nodes to: 1 \r\n\u001b[36mINFO\u001b[0m[0010] Setting maxUnavailable for controlplane nodes to: 1 \r\n\u001b[36mINFO\u001b[0m[0010] Pre-pulling kubernetes images                \r\n"]
[579.461353, "o", "\u001b[36mINFO\u001b[0m[0010] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.20.2.12], try #1 \r\n\u001b[36mINFO\u001b[0m[0010] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.20.2.11], try #1 \r\n"]
[579.461552, "o", "\u001b[36mINFO\u001b[0m[0010] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.20.2.21], try #1 \r\n\u001b[36mINFO\u001b[0m[0010] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.20.2.10], try #1 \r\n"]
[579.463344, "o", "\u001b[36mINFO\u001b[0m[0010] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.20.2.22], try #1 \r\n\u001b[36mINFO\u001b[0m[0010] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.20.2.20], try #1 \r\n"]
[625.941776, "o", "\u001b[36mINFO\u001b[0m[0057] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.21] \r\n"]
[625.943027, "o", "\u001b[36mINFO\u001b[0m[0057] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.22] \r\n"]
[627.068121, "o", "\u001b[36mINFO\u001b[0m[0058] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.20] \r\n"]
[628.010137, "o", "\u001b[36mINFO\u001b[0m[0059] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.11] \r\n"]
[628.892431, "o", "\u001b[36mINFO\u001b[0m[0060] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.12] \r\n"]
[629.759249, "o", "\u001b[36mINFO\u001b[0m[0061] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0061] Kubernetes images pulled successfully        \r\n"]
[629.760407, "o", "\u001b[36mINFO\u001b[0m[0061] [etcd] Building up etcd plane..              \r\n"]
[629.765438, "o", "\u001b[36mINFO\u001b[0m[0061] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[630.162254, "o", "\u001b[36mINFO\u001b[0m[0061] Starting container [etcd-fix-perm] on host [10.20.2.10], try #1 \r\n"]
[630.477087, "o", "\u001b[36mINFO\u001b[0m[0061] Successfully started [etcd-fix-perm] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0061] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0061] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.10] \r\n"]
[630.480492, "o", "\u001b[36mINFO\u001b[0m[0061] Container [etcd-fix-perm] is still running on host [10.20.2.10]: stderr: [], stdout: [] \r\n"]
[631.480811, "o", "\u001b[36mINFO\u001b[0m[0062] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.10] \r\n"]
[631.487279, "o", "\u001b[36mINFO\u001b[0m[0062] Removing container [etcd-fix-perm] on host [10.20.2.10], try #1 \r\n"]
[631.52174, "o", "\u001b[36mINFO\u001b[0m[0062] [remove/etcd-fix-perm] Successfully removed container on host [10.20.2.10] \r\n"]
[631.527142, "o", "\u001b[36mINFO\u001b[0m[0062] Checking if container [etcd] is running on host [10.20.2.10], try #1 \r\n"]
[631.531011, "o", "\u001b[36mINFO\u001b[0m[0062] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] on host [10.20.2.10], try #1 \r\n"]
[632.427799, "o", "\u001b[36mINFO\u001b[0m[0063] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] exists on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0063] Checking if container [old-etcd] is running on host [10.20.2.10], try #1 \r\n"]
[632.432588, "o", "\u001b[36mINFO\u001b[0m[0063] Stopping container [etcd] on host [10.20.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[634.246332, "o", "\u001b[36mINFO\u001b[0m[0065] Waiting for [etcd] container to exit on host [10.20.2.10] \r\n"]
[634.248348, "o", "\u001b[36mINFO\u001b[0m[0065] Renaming container [etcd] to [old-etcd] on host [10.20.2.10], try #1 \r\n"]
[634.288695, "o", "\u001b[36mINFO\u001b[0m[0065] Starting container [etcd] on host [10.20.2.10], try #1 \r\n"]
[634.427735, "o", "\u001b[36mINFO\u001b[0m[0065] [etcd] Successfully updated [etcd] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0065] Removing container [old-etcd] on host [10.20.2.10], try #1 \r\n"]
[634.458379, "o", "\u001b[36mINFO\u001b[0m[0065] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.20.2.10] \r\n"]
[634.462745, "o", "\u001b[36mINFO\u001b[0m[0065] Removing container [etcd-rolling-snapshots] on host [10.20.2.10], try #1 \r\n"]
[634.675532, "o", "\u001b[36mINFO\u001b[0m[0065] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.20.2.10] \r\n"]
[634.67686, "o", "\u001b[36mINFO\u001b[0m[0065] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[634.790324, "o", "\u001b[36mINFO\u001b[0m[0066] Starting container [etcd-rolling-snapshots] on host [10.20.2.10], try #1 \r\n"]
[635.096485, "o", "\u001b[36mINFO\u001b[0m[0066] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.20.2.10] \r\n"]
[640.110919, "o", "\u001b[36mINFO\u001b[0m[0071] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[640.294652, "o", "\u001b[36mINFO\u001b[0m[0071] Starting container [rke-bundle-cert] on host [10.20.2.10], try #1 \r\n"]
[640.624675, "o", "\u001b[36mINFO\u001b[0m[0071] [certificates] Successfully started [rke-bundle-cert] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0071] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.10] \r\n"]
[640.628503, "o", "\u001b[36mINFO\u001b[0m[0071] Container [rke-bundle-cert] is still running on host [10.20.2.10]: stderr: [], stdout: [] \r\n"]
[641.628784, "o", "\u001b[36mINFO\u001b[0m[0072] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.10] \r\n"]
[641.631522, "o", "\u001b[36mINFO\u001b[0m[0072] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0072] Removing container [rke-bundle-cert] on host [10.20.2.10], try #1 \r\n"]
[641.661556, "o", "\u001b[36mINFO\u001b[0m[0072] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[641.787633, "o", "\u001b[36mINFO\u001b[0m[0073] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[641.998882, "o", "\u001b[36mINFO\u001b[0m[0073] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[642.000273, "o", "\u001b[36mINFO\u001b[0m[0073] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[642.123265, "o", "\u001b[36mINFO\u001b[0m[0073] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[642.126503, "o", "\u001b[36mINFO\u001b[0m[0073] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[642.271457, "o", "\u001b[36mINFO\u001b[0m[0073] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[642.484572, "o", "\u001b[36mINFO\u001b[0m[0073] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[642.485226, "o", "\u001b[36mINFO\u001b[0m[0073] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[642.633966, "o", "\u001b[36mINFO\u001b[0m[0073] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[642.641699, "o", "\u001b[36mINFO\u001b[0m[0073] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[642.824219, "o", "\u001b[36mINFO\u001b[0m[0074] Starting container [etcd-fix-perm] on host [10.20.2.11], try #1 \r\n"]
[643.117478, "o", "\u001b[36mINFO\u001b[0m[0074] Successfully started [etcd-fix-perm] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0074] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0074] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.11] \r\n"]
[643.120439, "o", "\u001b[36mINFO\u001b[0m[0074] Container [etcd-fix-perm] is still running on host [10.20.2.11]: stderr: [], stdout: [] \r\n"]
[644.12065, "o", "\u001b[36mINFO\u001b[0m[0075] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.11] \r\n"]
[644.124771, "o", "\u001b[36mINFO\u001b[0m[0075] Removing container [etcd-fix-perm] on host [10.20.2.11], try #1 \r\n"]
[644.143037, "o", "\u001b[36mINFO\u001b[0m[0075] [remove/etcd-fix-perm] Successfully removed container on host [10.20.2.11] \r\n"]
[644.149795, "o", "\u001b[36mINFO\u001b[0m[0075] Checking if container [etcd] is running on host [10.20.2.11], try #1 \r\n"]
[644.157362, "o", "\u001b[36mINFO\u001b[0m[0075] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] on host [10.20.2.11], try #1 \r\n"]
[645.033726, "o", "\u001b[36mINFO\u001b[0m[0076] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] exists on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0076] Checking if container [old-etcd] is running on host [10.20.2.11], try #1 \r\n"]
[645.038131, "o", "\u001b[36mINFO\u001b[0m[0076] Stopping container [etcd] on host [10.20.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[646.851571, "o", "\u001b[36mINFO\u001b[0m[0078] Waiting for [etcd] container to exit on host [10.20.2.11] \r\n"]
[646.853106, "o", "\u001b[36mINFO\u001b[0m[0078] Renaming container [etcd] to [old-etcd] on host [10.20.2.11], try #1 \r\n"]
[646.886848, "o", "\u001b[36mINFO\u001b[0m[0078] Starting container [etcd] on host [10.20.2.11], try #1 \r\n"]
[647.02594, "o", "\u001b[36mINFO\u001b[0m[0078] [etcd] Successfully updated [etcd] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0078] Removing container [old-etcd] on host [10.20.2.11], try #1 \r\n"]
[647.042579, "o", "\u001b[36mINFO\u001b[0m[0078] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.20.2.11] \r\n"]
[647.043915, "o", "\u001b[36mINFO\u001b[0m[0078] Removing container [etcd-rolling-snapshots] on host [10.20.2.11], try #1 \r\n"]
[647.170242, "o", "\u001b[36mINFO\u001b[0m[0078] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.20.2.11] \r\n"]
[647.171767, "o", "\u001b[36mINFO\u001b[0m[0078] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[647.284369, "o", "\u001b[36mINFO\u001b[0m[0078] Starting container [etcd-rolling-snapshots] on host [10.20.2.11], try #1 \r\n"]
[647.621436, "o", "\u001b[36mINFO\u001b[0m[0078] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.20.2.11] \r\n"]
[652.639363, "o", "\u001b[36mINFO\u001b[0m[0083] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[652.825726, "o", "\u001b[36mINFO\u001b[0m[0084] Starting container [rke-bundle-cert] on host [10.20.2.11], try #1 \r\n"]
[653.064259, "o", "\u001b[36mINFO\u001b[0m[0084] [certificates] Successfully started [rke-bundle-cert] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0084] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.11] \r\n"]
[653.06643, "o", "\u001b[36mINFO\u001b[0m[0084] Container [rke-bundle-cert] is still running on host [10.20.2.11]: stderr: [], stdout: [] \r\n"]
[654.066591, "o", "\u001b[36mINFO\u001b[0m[0085] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.11] \r\n"]
[654.067583, "o", "\u001b[36mINFO\u001b[0m[0085] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0085] Removing container [rke-bundle-cert] on host [10.20.2.11], try #1 \r\n"]
[654.081042, "o", "\u001b[36mINFO\u001b[0m[0085] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[654.192347, "o", "\u001b[36mINFO\u001b[0m[0085] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[654.39168, "o", "\u001b[36mINFO\u001b[0m[0085] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[654.392484, "o", "\u001b[36mINFO\u001b[0m[0085] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[654.546201, "o", "\u001b[36mINFO\u001b[0m[0085] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n"]
[654.549131, "o", "\u001b[36mINFO\u001b[0m[0085] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[654.656239, "o", "\u001b[36mINFO\u001b[0m[0085] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[654.935935, "o", "\u001b[36mINFO\u001b[0m[0086] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[654.937784, "o", "\u001b[36mINFO\u001b[0m[0086] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[655.169712, "o", "\u001b[36mINFO\u001b[0m[0086] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n"]
[655.178403, "o", "\u001b[36mINFO\u001b[0m[0086] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[655.420923, "o", "\u001b[36mINFO\u001b[0m[0086] Starting container [etcd-fix-perm] on host [10.20.2.12], try #1 \r\n"]
[655.858382, "o", "\u001b[36mINFO\u001b[0m[0087] Successfully started [etcd-fix-perm] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0087] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0087] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.12] \r\n"]
[655.861638, "o", "\u001b[36mINFO\u001b[0m[0087] Container [etcd-fix-perm] is still running on host [10.20.2.12]: stderr: [], stdout: [] \r\n"]
[656.861702, "o", "\u001b[36mINFO\u001b[0m[0088] Waiting for [etcd-fix-perm] container to exit on host [10.20.2.12] \r\n"]
[656.86433, "o", "\u001b[36mINFO\u001b[0m[0088] Removing container [etcd-fix-perm] on host [10.20.2.12], try #1 \r\n"]
[656.88021, "o", "\u001b[36mINFO\u001b[0m[0088] [remove/etcd-fix-perm] Successfully removed container on host [10.20.2.12] \r\n"]
[656.886229, "o", "\u001b[36mINFO\u001b[0m[0088] Checking if container [etcd] is running on host [10.20.2.12], try #1 \r\n"]
[656.892044, "o", "\u001b[36mINFO\u001b[0m[0088] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] on host [10.20.2.12], try #1 \r\n"]
[657.82929, "o", "\u001b[36mINFO\u001b[0m[0089] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] exists on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0089] Checking if container [old-etcd] is running on host [10.20.2.12], try #1 \r\n"]
[657.833791, "o", "\u001b[36mINFO\u001b[0m[0089] Stopping container [etcd] on host [10.20.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[659.611427, "o", "\u001b[36mINFO\u001b[0m[0090] Waiting for [etcd] container to exit on host [10.20.2.12] \r\n"]
[659.612067, "o", "\u001b[36mINFO\u001b[0m[0090] Renaming container [etcd] to [old-etcd] on host [10.20.2.12], try #1 \r\n"]
[659.644241, "o", "\u001b[36mINFO\u001b[0m[0090] Starting container [etcd] on host [10.20.2.12], try #1 \r\n"]
[659.778405, "o", "\u001b[36mINFO\u001b[0m[0091] [etcd] Successfully updated [etcd] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0091] Removing container [old-etcd] on host [10.20.2.12], try #1 \r\n"]
[659.810134, "o", "\u001b[36mINFO\u001b[0m[0091] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.20.2.12] \r\n"]
[659.811526, "o", "\u001b[36mINFO\u001b[0m[0091] Removing container [etcd-rolling-snapshots] on host [10.20.2.12], try #1 \r\n"]
[659.919408, "o", "\u001b[36mINFO\u001b[0m[0091] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.20.2.12] \r\n"]
[659.923393, "o", "\u001b[36mINFO\u001b[0m[0091] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[660.026196, "o", "\u001b[36mINFO\u001b[0m[0091] Starting container [etcd-rolling-snapshots] on host [10.20.2.12], try #1 \r\n"]
[660.141288, "o", "\u001b[36mINFO\u001b[0m[0091] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.20.2.12] \r\n"]
[665.149769, "o", "\u001b[36mINFO\u001b[0m[0096] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[665.270008, "o", "\u001b[36mINFO\u001b[0m[0096] Starting container [rke-bundle-cert] on host [10.20.2.12], try #1 \r\n"]
[665.526891, "o", "\u001b[36mINFO\u001b[0m[0096] [certificates] Successfully started [rke-bundle-cert] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0096] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.12] \r\n"]
[665.529175, "o", "\u001b[36mINFO\u001b[0m[0096] Container [rke-bundle-cert] is still running on host [10.20.2.12]: stderr: [], stdout: [] \r\n"]
[666.529459, "o", "\u001b[36mINFO\u001b[0m[0097] Waiting for [rke-bundle-cert] container to exit on host [10.20.2.12] \r\n"]
[666.53038, "o", "\u001b[36mINFO\u001b[0m[0097] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0097] Removing container [rke-bundle-cert] on host [10.20.2.12], try #1 \r\n"]
[666.543545, "o", "\u001b[36mINFO\u001b[0m[0097] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[666.646676, "o", "\u001b[36mINFO\u001b[0m[0097] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[666.845078, "o", "\u001b[36mINFO\u001b[0m[0098] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[666.846764, "o", "\u001b[36mINFO\u001b[0m[0098] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[666.975256, "o", "\u001b[36mINFO\u001b[0m[0098] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n"]
[666.978042, "o", "\u001b[36mINFO\u001b[0m[0098] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[667.096258, "o", "\u001b[36mINFO\u001b[0m[0098] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[667.324129, "o", "\u001b[36mINFO\u001b[0m[0098] [etcd] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[667.326682, "o", "\u001b[36mINFO\u001b[0m[0098] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[667.48307, "o", "\u001b[36mINFO\u001b[0m[0098] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0098] [etcd] Successfully started etcd plane.. Checking etcd cluster health \r\n"]
[667.971612, "o", "\u001b[36mINFO\u001b[0m[0099] [etcd] etcd host [10.20.2.10] reported healthy=true \r\n"]
[667.972157, "o", "\u001b[36mINFO\u001b[0m[0099] [controlplane] Now checking status of node 10.20.2.10, try #1 \r\n"]
[667.977597, "o", "\u001b[36mINFO\u001b[0m[0099] [controlplane] Now checking status of node 10.20.2.11, try #1 \r\n"]
[667.983513, "o", "\u001b[36mINFO\u001b[0m[0099] [controlplane] Now checking status of node 10.20.2.12, try #1 \r\n"]
[667.98799, "o", "\u001b[36mINFO\u001b[0m[0099] [controlplane] Processing controlplane hosts for upgrade 1 at a time \r\n"]
[667.988148, "o", "\u001b[36mINFO\u001b[0m[0099] Processing controlplane host 10.20.2.10      \r\n\u001b[36mINFO\u001b[0m[0099] [controlplane] Now checking status of node 10.20.2.10, try #1 \r\n"]
[667.995653, "o", "\u001b[36mINFO\u001b[0m[0099] [controlplane] Getting list of nodes for upgrade \r\n"]
[668.029374, "o", "\u001b[36mINFO\u001b[0m[0099] Upgrading controlplane components for control host 10.20.2.10 \r\n\u001b[36mINFO\u001b[0m[0099] Checking if container [service-sidekick] is running on host [10.20.2.10], try #1 \r\n"]
[668.040127, "o", "\u001b[36mINFO\u001b[0m[0099] [sidekick] Sidekick container already created on host [10.20.2.10] \r\n"]
[668.042426, "o", "\u001b[36mINFO\u001b[0m[0099] Checking if container [kube-apiserver] is running on host [10.20.2.10], try #1 \r\n"]
[668.047693, "o", "\u001b[36mINFO\u001b[0m[0099] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0099] Checking if container [old-kube-apiserver] is running on host [10.20.2.10], try #1 \r\n"]
[668.058918, "o", "\u001b[36mINFO\u001b[0m[0099] Stopping container [kube-apiserver] on host [10.20.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[668.759648, "o", "\u001b[36mINFO\u001b[0m[0100] Waiting for [kube-apiserver] container to exit on host [10.20.2.10] \r\n"]
[668.760505, "o", "\u001b[36mINFO\u001b[0m[0100] Renaming container [kube-apiserver] to [old-kube-apiserver] on host [10.20.2.10], try #1 \r\n"]
[668.798374, "o", "\u001b[36mINFO\u001b[0m[0100] Starting container [kube-apiserver] on host [10.20.2.10], try #1 \r\n"]
[668.984033, "o", "\u001b[36mINFO\u001b[0m[0100] [controlplane] Successfully updated [kube-apiserver] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0100] Removing container [old-kube-apiserver] on host [10.20.2.10], try #1 \r\n"]
[669.001703, "o", "\u001b[36mINFO\u001b[0m[0100] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.20.2.10] \r\n"]
[677.516565, "o", "\u001b[36mINFO\u001b[0m[0108] [healthcheck] service [kube-apiserver] on host [10.20.2.10] is healthy \r\n"]
[677.520252, "o", "\u001b[36mINFO\u001b[0m[0108] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[677.649658, "o", "\u001b[36mINFO\u001b[0m[0108] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[677.88032, "o", "\u001b[36mINFO\u001b[0m[0109] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[677.882008, "o", "\u001b[36mINFO\u001b[0m[0109] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[678.034606, "o", "\u001b[36mINFO\u001b[0m[0109] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[678.039155, "o", "\u001b[36mINFO\u001b[0m[0109] Checking if container [kube-controller-manager] is running on host [10.20.2.10], try #1 \r\n"]
[678.043326, "o", "\u001b[36mINFO\u001b[0m[0109] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0109] Checking if container [old-kube-controller-manager] is running on host [10.20.2.10], try #1 \r\n"]
[678.049145, "o", "\u001b[36mINFO\u001b[0m[0109] Stopping container [kube-controller-manager] on host [10.20.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[678.157773, "o", "\u001b[36mINFO\u001b[0m[0109] Waiting for [kube-controller-manager] container to exit on host [10.20.2.10] \r\n"]
[678.158383, "o", "\u001b[36mINFO\u001b[0m[0109] Renaming container [kube-controller-manager] to [old-kube-controller-manager] on host [10.20.2.10], try #1 \r\n"]
[678.181221, "o", "\u001b[36mINFO\u001b[0m[0109] Starting container [kube-controller-manager] on host [10.20.2.10], try #1 \r\n"]
[678.313407, "o", "\u001b[36mINFO\u001b[0m[0109] [controlplane] Successfully updated [kube-controller-manager] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0109] Removing container [old-kube-controller-manager] on host [10.20.2.10], try #1 \r\n"]
[678.333756, "o", "\u001b[36mINFO\u001b[0m[0109] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.20.2.10] \r\n"]
[683.873818, "o", "\u001b[36mINFO\u001b[0m[0115] [healthcheck] service [kube-controller-manager] on host [10.20.2.10] is healthy \r\n"]
[683.886995, "o", "\u001b[36mINFO\u001b[0m[0115] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[684.030783, "o", "\u001b[36mINFO\u001b[0m[0115] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[684.307712, "o", "\u001b[36mINFO\u001b[0m[0115] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[684.309998, "o", "\u001b[36mINFO\u001b[0m[0115] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[684.44171, "o", "\u001b[36mINFO\u001b[0m[0115] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[684.444443, "o", "\u001b[36mINFO\u001b[0m[0115] Checking if container [kube-scheduler] is running on host [10.20.2.10], try #1 \r\n"]
[684.448159, "o", "\u001b[36mINFO\u001b[0m[0115] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0115] Checking if container [old-kube-scheduler] is running on host [10.20.2.10], try #1 \r\n"]
[684.452849, "o", "\u001b[36mINFO\u001b[0m[0115] Stopping container [kube-scheduler] on host [10.20.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[684.559673, "o", "\u001b[36mINFO\u001b[0m[0115] Waiting for [kube-scheduler] container to exit on host [10.20.2.10] \r\n"]
[684.560481, "o", "\u001b[36mINFO\u001b[0m[0115] Renaming container [kube-scheduler] to [old-kube-scheduler] on host [10.20.2.10], try #1 \r\n"]
[684.585921, "o", "\u001b[36mINFO\u001b[0m[0115] Starting container [kube-scheduler] on host [10.20.2.10], try #1 \r\n"]
[684.72509, "o", "\u001b[36mINFO\u001b[0m[0115] [controlplane] Successfully updated [kube-scheduler] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0115] Removing container [old-kube-scheduler] on host [10.20.2.10], try #1 \r\n"]
[684.742191, "o", "\u001b[36mINFO\u001b[0m[0116] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.20.2.10] \r\n"]
[690.417457, "o", "\u001b[36mINFO\u001b[0m[0121] [healthcheck] service [kube-scheduler] on host [10.20.2.10] is healthy \r\n"]
[690.422304, "o", "\u001b[36mINFO\u001b[0m[0121] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[690.670003, "o", "\u001b[36mINFO\u001b[0m[0121] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[690.948255, "o", "\u001b[36mINFO\u001b[0m[0122] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[690.950993, "o", "\u001b[36mINFO\u001b[0m[0122] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[691.08891, "o", "\u001b[36mINFO\u001b[0m[0122] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0122] Upgrading workerplane components for control host 10.20.2.10 \r\n\u001b[36mINFO\u001b[0m[0122] Checking if container [service-sidekick] is running on host [10.20.2.10], try #1 \r\n"]
[691.096736, "o", "\u001b[36mINFO\u001b[0m[0122] [sidekick] Sidekick container already created on host [10.20.2.10] \r\n"]
[691.099856, "o", "\u001b[36mINFO\u001b[0m[0122] Checking if container [kubelet] is running on host [10.20.2.10], try #1 \r\n"]
[691.105081, "o", "\u001b[36mINFO\u001b[0m[0122] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0122] Checking if container [old-kubelet] is running on host [10.20.2.10], try #1 \r\n"]
[691.110295, "o", "\u001b[36mINFO\u001b[0m[0122] Stopping container [kubelet] on host [10.20.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[691.219345, "o", "\u001b[36mINFO\u001b[0m[0122] Waiting for [kubelet] container to exit on host [10.20.2.10] \r\n"]
[691.221293, "o", "\u001b[36mINFO\u001b[0m[0122] Renaming container [kubelet] to [old-kubelet] on host [10.20.2.10], try #1 \r\n"]
[691.247118, "o", "\u001b[36mINFO\u001b[0m[0122] Starting container [kubelet] on host [10.20.2.10], try #1 \r\n"]
[691.375997, "o", "\u001b[36mINFO\u001b[0m[0122] [worker] Successfully updated [kubelet] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0122] Removing container [old-kubelet] on host [10.20.2.10], try #1 \r\n"]
[691.386988, "o", "\u001b[36mINFO\u001b[0m[0122] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.10] \r\n"]
[702.168355, "o", "\u001b[36mINFO\u001b[0m[0133] [healthcheck] service [kubelet] on host [10.20.2.10] is healthy \r\n"]
[702.187609, "o", "\u001b[36mINFO\u001b[0m[0133] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[702.32664, "o", "\u001b[36mINFO\u001b[0m[0133] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[702.564297, "o", "\u001b[36mINFO\u001b[0m[0133] [worker] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[702.567163, "o", "\u001b[36mINFO\u001b[0m[0133] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[702.699378, "o", "\u001b[36mINFO\u001b[0m[0133] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n"]
[702.702583, "o", "\u001b[36mINFO\u001b[0m[0133] Checking if container [kube-proxy] is running on host [10.20.2.10], try #1 \r\n"]
[702.709904, "o", "\u001b[36mINFO\u001b[0m[0133] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0133] Checking if container [old-kube-proxy] is running on host [10.20.2.10], try #1 \r\n"]
[702.715507, "o", "\u001b[36mINFO\u001b[0m[0133] Stopping container [kube-proxy] on host [10.20.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[702.822389, "o", "\u001b[36mINFO\u001b[0m[0134] Waiting for [kube-proxy] container to exit on host [10.20.2.10] \r\n"]
[702.824029, "o", "\u001b[36mINFO\u001b[0m[0134] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.20.2.10], try #1 \r\n"]
[702.853004, "o", "\u001b[36mINFO\u001b[0m[0134] Starting container [kube-proxy] on host [10.20.2.10], try #1 \r\n"]
[702.939756, "o", "\u001b[36mINFO\u001b[0m[0134] [worker] Successfully updated [kube-proxy] container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0134] Removing container [old-kube-proxy] on host [10.20.2.10], try #1 \r\n"]
[702.947392, "o", "\u001b[36mINFO\u001b[0m[0134] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.10] \r\n"]
[703.208812, "o", "\u001b[36mINFO\u001b[0m[0134] [healthcheck] service [kube-proxy] on host [10.20.2.10] is healthy \r\n"]
[703.212457, "o", "\u001b[36mINFO\u001b[0m[0134] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[703.313176, "o", "\u001b[36mINFO\u001b[0m[0134] Starting container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[703.580359, "o", "\u001b[36mINFO\u001b[0m[0134] [worker] Successfully started [rke-log-linker] container on host [10.20.2.10] \r\n"]
[703.584281, "o", "\u001b[36mINFO\u001b[0m[0134] Removing container [rke-log-linker] on host [10.20.2.10], try #1 \r\n"]
[703.742193, "o", "\u001b[36mINFO\u001b[0m[0135] [remove/rke-log-linker] Successfully removed container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0135] [controlplane] Now checking status of node 10.20.2.10, try #1 \r\n"]
[703.780542, "o", "\u001b[36mINFO\u001b[0m[0135] Processing controlplane host 10.20.2.11      \r\n\u001b[36mINFO\u001b[0m[0135] [controlplane] Now checking status of node 10.20.2.11, try #1 \r\n"]
[703.791695, "o", "\u001b[36mINFO\u001b[0m[0135] [controlplane] Getting list of nodes for upgrade \r\n"]
[703.822687, "o", "\u001b[36mINFO\u001b[0m[0135] Upgrading controlplane components for control host 10.20.2.11 \r\n\u001b[36mINFO\u001b[0m[0135] Checking if container [service-sidekick] is running on host [10.20.2.11], try #1 \r\n"]
[703.830945, "o", "\u001b[36mINFO\u001b[0m[0135] [sidekick] Sidekick container already created on host [10.20.2.11] \r\n"]
[703.833423, "o", "\u001b[36mINFO\u001b[0m[0135] Checking if container [kube-apiserver] is running on host [10.20.2.11], try #1 \r\n"]
[703.837298, "o", "\u001b[36mINFO\u001b[0m[0135] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0135] Checking if container [old-kube-apiserver] is running on host [10.20.2.11], try #1 \r\n"]
[703.843983, "o", "\u001b[36mINFO\u001b[0m[0135] Stopping container [kube-apiserver] on host [10.20.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[704.061734, "o", "\u001b[36mINFO\u001b[0m[0135] Waiting for [kube-apiserver] container to exit on host [10.20.2.11] \r\n"]
[704.062488, "o", "\u001b[36mINFO\u001b[0m[0135] Renaming container [kube-apiserver] to [old-kube-apiserver] on host [10.20.2.11], try #1 \r\n"]
[704.093438, "o", "\u001b[36mINFO\u001b[0m[0135] Starting container [kube-apiserver] on host [10.20.2.11], try #1 \r\n"]
[704.222573, "o", "\u001b[36mINFO\u001b[0m[0135] [controlplane] Successfully updated [kube-apiserver] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0135] Removing container [old-kube-apiserver] on host [10.20.2.11], try #1 \r\n"]
[704.234589, "o", "\u001b[36mINFO\u001b[0m[0135] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.20.2.11] \r\n"]
[712.441258, "o", "\u001b[36mINFO\u001b[0m[0143] [healthcheck] service [kube-apiserver] on host [10.20.2.11] is healthy \r\n"]
[712.444639, "o", "\u001b[36mINFO\u001b[0m[0143] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[712.562379, "o", "\u001b[36mINFO\u001b[0m[0143] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[712.846067, "o", "\u001b[36mINFO\u001b[0m[0144] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[712.846912, "o", "\u001b[36mINFO\u001b[0m[0144] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[712.984338, "o", "\u001b[36mINFO\u001b[0m[0144] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n"]
[712.989351, "o", "\u001b[36mINFO\u001b[0m[0144] Checking if container [kube-controller-manager] is running on host [10.20.2.11], try #1 \r\n"]
[712.993745, "o", "\u001b[36mINFO\u001b[0m[0144] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0144] Checking if container [old-kube-controller-manager] is running on host [10.20.2.11], try #1 \r\n"]
[712.999048, "o", "\u001b[36mINFO\u001b[0m[0144] Stopping container [kube-controller-manager] on host [10.20.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[713.103906, "o", "\u001b[36mINFO\u001b[0m[0144] Waiting for [kube-controller-manager] container to exit on host [10.20.2.11] \r\n"]
[713.104579, "o", "\u001b[36mINFO\u001b[0m[0144] Renaming container [kube-controller-manager] to [old-kube-controller-manager] on host [10.20.2.11], try #1 \r\n"]
[713.131094, "o", "\u001b[36mINFO\u001b[0m[0144] Starting container [kube-controller-manager] on host [10.20.2.11], try #1 \r\n"]
[713.247944, "o", "\u001b[36mINFO\u001b[0m[0144] [controlplane] Successfully updated [kube-controller-manager] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0144] Removing container [old-kube-controller-manager] on host [10.20.2.11], try #1 \r\n"]
[713.2553, "o", "\u001b[36mINFO\u001b[0m[0144] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.20.2.11] \r\n"]
[718.757816, "o", "\u001b[36mINFO\u001b[0m[0150] [healthcheck] service [kube-controller-manager] on host [10.20.2.11] is healthy \r\n"]
[718.761316, "o", "\u001b[36mINFO\u001b[0m[0150] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[718.864797, "o", "\u001b[36mINFO\u001b[0m[0150] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[719.130142, "o", "\u001b[36mINFO\u001b[0m[0150] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[719.131364, "o", "\u001b[36mINFO\u001b[0m[0150] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[719.257091, "o", "\u001b[36mINFO\u001b[0m[0150] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n"]
[719.260226, "o", "\u001b[36mINFO\u001b[0m[0150] Checking if container [kube-scheduler] is running on host [10.20.2.11], try #1 \r\n"]
[719.263961, "o", "\u001b[36mINFO\u001b[0m[0150] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0150] Checking if container [old-kube-scheduler] is running on host [10.20.2.11], try #1 \r\n"]
[719.272102, "o", "\u001b[36mINFO\u001b[0m[0150] Stopping container [kube-scheduler] on host [10.20.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[719.370047, "o", "\u001b[36mINFO\u001b[0m[0150] Waiting for [kube-scheduler] container to exit on host [10.20.2.11] \r\n"]
[719.370854, "o", "\u001b[36mINFO\u001b[0m[0150] Renaming container [kube-scheduler] to [old-kube-scheduler] on host [10.20.2.11], try #1 \r\n"]
[719.398803, "o", "\u001b[36mINFO\u001b[0m[0150] Starting container [kube-scheduler] on host [10.20.2.11], try #1 \r\n"]
[719.511915, "o", "\u001b[36mINFO\u001b[0m[0150] [controlplane] Successfully updated [kube-scheduler] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0150] Removing container [old-kube-scheduler] on host [10.20.2.11], try #1 \r\n"]
[719.520656, "o", "\u001b[36mINFO\u001b[0m[0150] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.20.2.11] \r\n"]
[725.040493, "o", "\u001b[36mINFO\u001b[0m[0156] [healthcheck] service [kube-scheduler] on host [10.20.2.11] is healthy \r\n"]
[725.044673, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[725.161042, "o", "\u001b[36mINFO\u001b[0m[0156] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[725.425938, "o", "\u001b[36mINFO\u001b[0m[0156] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[725.426602, "o", "\u001b[36mINFO\u001b[0m[0156] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[725.561633, "o", "\u001b[36mINFO\u001b[0m[0156] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0156] Upgrading workerplane components for control host 10.20.2.11 \r\n\u001b[36mINFO\u001b[0m[0156] Checking if container [service-sidekick] is running on host [10.20.2.11], try #1 \r\n"]
[725.569675, "o", "\u001b[36mINFO\u001b[0m[0156] [sidekick] Sidekick container already created on host [10.20.2.11] \r\n"]
[725.573704, "o", "\u001b[36mINFO\u001b[0m[0156] Checking if container [kubelet] is running on host [10.20.2.11], try #1 \r\n"]
[725.577608, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0156] Checking if container [old-kubelet] is running on host [10.20.2.11], try #1 \r\n"]
[725.582997, "o", "\u001b[36mINFO\u001b[0m[0156] Stopping container [kubelet] on host [10.20.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[725.705849, "o", "\u001b[36mINFO\u001b[0m[0156] Waiting for [kubelet] container to exit on host [10.20.2.11] \r\n"]
[725.706632, "o", "\u001b[36mINFO\u001b[0m[0156] Renaming container [kubelet] to [old-kubelet] on host [10.20.2.11], try #1 \r\n"]
[725.730292, "o", "\u001b[36mINFO\u001b[0m[0156] Starting container [kubelet] on host [10.20.2.11], try #1 \r\n"]
[725.834632, "o", "\u001b[36mINFO\u001b[0m[0157] [worker] Successfully updated [kubelet] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0157] Removing container [old-kubelet] on host [10.20.2.11], try #1 \r\n"]
[725.869395, "o", "\u001b[36mINFO\u001b[0m[0157] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.11] \r\n"]
[736.742232, "o", "\u001b[36mINFO\u001b[0m[0168] [healthcheck] service [kubelet] on host [10.20.2.11] is healthy \r\n"]
[736.747001, "o", "\u001b[36mINFO\u001b[0m[0168] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[736.865944, "o", "\u001b[36mINFO\u001b[0m[0168] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[737.125572, "o", "\u001b[36mINFO\u001b[0m[0168] [worker] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[737.126407, "o", "\u001b[36mINFO\u001b[0m[0168] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[737.250895, "o", "\u001b[36mINFO\u001b[0m[0168] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n"]
[737.254001, "o", "\u001b[36mINFO\u001b[0m[0168] Checking if container [kube-proxy] is running on host [10.20.2.11], try #1 \r\n"]
[737.258436, "o", "\u001b[36mINFO\u001b[0m[0168] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0168] Checking if container [old-kube-proxy] is running on host [10.20.2.11], try #1 \r\n"]
[737.262521, "o", "\u001b[36mINFO\u001b[0m[0168] Stopping container [kube-proxy] on host [10.20.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[737.375175, "o", "\u001b[36mINFO\u001b[0m[0168] Waiting for [kube-proxy] container to exit on host [10.20.2.11] \r\n"]
[737.375919, "o", "\u001b[36mINFO\u001b[0m[0168] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.20.2.11], try #1 \r\n"]
[737.39949, "o", "\u001b[36mINFO\u001b[0m[0168] Starting container [kube-proxy] on host [10.20.2.11], try #1 \r\n"]
[737.522908, "o", "\u001b[36mINFO\u001b[0m[0168] [worker] Successfully updated [kube-proxy] container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0168] Removing container [old-kube-proxy] on host [10.20.2.11], try #1 \r\n"]
[737.5412, "o", "\u001b[36mINFO\u001b[0m[0168] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.11] \r\n"]
[737.804689, "o", "\u001b[36mINFO\u001b[0m[0169] [healthcheck] service [kube-proxy] on host [10.20.2.11] is healthy \r\n"]
[737.810427, "o", "\u001b[36mINFO\u001b[0m[0169] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[737.928038, "o", "\u001b[36mINFO\u001b[0m[0169] Starting container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[738.198602, "o", "\u001b[36mINFO\u001b[0m[0169] [worker] Successfully started [rke-log-linker] container on host [10.20.2.11] \r\n"]
[738.20242, "o", "\u001b[36mINFO\u001b[0m[0169] Removing container [rke-log-linker] on host [10.20.2.11], try #1 \r\n"]
[738.337052, "o", "\u001b[36mINFO\u001b[0m[0169] [remove/rke-log-linker] Successfully removed container on host [10.20.2.11] \r\n\u001b[36mINFO\u001b[0m[0169] [controlplane] Now checking status of node 10.20.2.11, try #1 \r\n"]
[738.360759, "o", "\u001b[36mINFO\u001b[0m[0169] Processing controlplane host 10.20.2.12      \r\n\u001b[36mINFO\u001b[0m[0169] [controlplane] Now checking status of node 10.20.2.12, try #1 \r\n"]
[738.370818, "o", "\u001b[36mINFO\u001b[0m[0169] [controlplane] Getting list of nodes for upgrade \r\n"]
[738.797081, "o", "\u001b[36mINFO\u001b[0m[0170] Upgrading controlplane components for control host 10.20.2.12 \r\n\u001b[36mINFO\u001b[0m[0170] Checking if container [service-sidekick] is running on host [10.20.2.12], try #1 \r\n"]
[738.804253, "o", "\u001b[36mINFO\u001b[0m[0170] [sidekick] Sidekick container already created on host [10.20.2.12] \r\n"]
[738.806698, "o", "\u001b[36mINFO\u001b[0m[0170] Checking if container [kube-apiserver] is running on host [10.20.2.12], try #1 \r\n"]
[738.810257, "o", "\u001b[36mINFO\u001b[0m[0170] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0170] Checking if container [old-kube-apiserver] is running on host [10.20.2.12], try #1 \r\n"]
[738.816015, "o", "\u001b[36mINFO\u001b[0m[0170] Stopping container [kube-apiserver] on host [10.20.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[739.149237, "o", "\u001b[36mINFO\u001b[0m[0170] Waiting for [kube-apiserver] container to exit on host [10.20.2.12] \r\n"]
[739.150413, "o", "\u001b[36mINFO\u001b[0m[0170] Renaming container [kube-apiserver] to [old-kube-apiserver] on host [10.20.2.12], try #1 \r\n"]
[739.186234, "o", "\u001b[36mINFO\u001b[0m[0170] Starting container [kube-apiserver] on host [10.20.2.12], try #1 \r\n"]
[739.333006, "o", "\u001b[36mINFO\u001b[0m[0170] [controlplane] Successfully updated [kube-apiserver] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0170] Removing container [old-kube-apiserver] on host [10.20.2.12], try #1 \r\n"]
[739.340404, "o", "\u001b[36mINFO\u001b[0m[0170] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.20.2.12] \r\n"]
[747.549301, "o", "\u001b[36mINFO\u001b[0m[0178] [healthcheck] service [kube-apiserver] on host [10.20.2.12] is healthy \r\n"]
[747.554339, "o", "\u001b[36mINFO\u001b[0m[0178] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[747.690624, "o", "\u001b[36mINFO\u001b[0m[0178] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[747.933409, "o", "\u001b[36mINFO\u001b[0m[0179] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[747.934094, "o", "\u001b[36mINFO\u001b[0m[0179] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[748.076044, "o", "\u001b[36mINFO\u001b[0m[0179] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n"]
[748.07896, "o", "\u001b[36mINFO\u001b[0m[0179] Checking if container [kube-controller-manager] is running on host [10.20.2.12], try #1 \r\n"]
[748.082973, "o", "\u001b[36mINFO\u001b[0m[0179] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0179] Checking if container [old-kube-controller-manager] is running on host [10.20.2.12], try #1 \r\n"]
[748.08732, "o", "\u001b[36mINFO\u001b[0m[0179] Stopping container [kube-controller-manager] on host [10.20.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[748.181894, "o", "\u001b[36mINFO\u001b[0m[0179] Waiting for [kube-controller-manager] container to exit on host [10.20.2.12] \r\n"]
[748.182737, "o", "\u001b[36mINFO\u001b[0m[0179] Renaming container [kube-controller-manager] to [old-kube-controller-manager] on host [10.20.2.12], try #1 \r\n"]
[748.209083, "o", "\u001b[36mINFO\u001b[0m[0179] Starting container [kube-controller-manager] on host [10.20.2.12], try #1 \r\n"]
[748.337972, "o", "\u001b[36mINFO\u001b[0m[0179] [controlplane] Successfully updated [kube-controller-manager] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0179] Removing container [old-kube-controller-manager] on host [10.20.2.12], try #1 \r\n"]
[748.349563, "o", "\u001b[36mINFO\u001b[0m[0179] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.20.2.12] \r\n"]
[753.855544, "o", "\u001b[36mINFO\u001b[0m[0185] [healthcheck] service [kube-controller-manager] on host [10.20.2.12] is healthy \r\n"]
[753.860065, "o", "\u001b[36mINFO\u001b[0m[0185] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[753.983815, "o", "\u001b[36mINFO\u001b[0m[0185] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[754.218712, "o", "\u001b[36mINFO\u001b[0m[0185] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[754.219676, "o", "\u001b[36mINFO\u001b[0m[0185] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[754.357255, "o", "\u001b[36mINFO\u001b[0m[0185] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n"]
[754.361158, "o", "\u001b[36mINFO\u001b[0m[0185] Checking if container [kube-scheduler] is running on host [10.20.2.12], try #1 \r\n"]
[754.364687, "o", "\u001b[36mINFO\u001b[0m[0185] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0185] Checking if container [old-kube-scheduler] is running on host [10.20.2.12], try #1 \r\n"]
[754.368442, "o", "\u001b[36mINFO\u001b[0m[0185] Stopping container [kube-scheduler] on host [10.20.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[754.448989, "o", "\u001b[36mINFO\u001b[0m[0185] Waiting for [kube-scheduler] container to exit on host [10.20.2.12] \r\n"]
[754.449763, "o", "\u001b[36mINFO\u001b[0m[0185] Renaming container [kube-scheduler] to [old-kube-scheduler] on host [10.20.2.12], try #1 \r\n"]
[754.474374, "o", "\u001b[36mINFO\u001b[0m[0185] Starting container [kube-scheduler] on host [10.20.2.12], try #1 \r\n"]
[754.600579, "o", "\u001b[36mINFO\u001b[0m[0185] [controlplane] Successfully updated [kube-scheduler] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0185] Removing container [old-kube-scheduler] on host [10.20.2.12], try #1 \r\n"]
[754.609915, "o", "\u001b[36mINFO\u001b[0m[0185] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.20.2.12] \r\n"]
[760.186367, "o", "\u001b[36mINFO\u001b[0m[0191] [healthcheck] service [kube-scheduler] on host [10.20.2.12] is healthy \r\n"]
[760.190452, "o", "\u001b[36mINFO\u001b[0m[0191] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[760.308869, "o", "\u001b[36mINFO\u001b[0m[0191] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[760.560159, "o", "\u001b[36mINFO\u001b[0m[0191] [controlplane] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[760.562262, "o", "\u001b[36mINFO\u001b[0m[0191] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[760.703688, "o", "\u001b[36mINFO\u001b[0m[0191] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0191] Upgrading workerplane components for control host 10.20.2.12 \r\n\u001b[36mINFO\u001b[0m[0191] Checking if container [service-sidekick] is running on host [10.20.2.12], try #1 \r\n"]
[760.713282, "o", "\u001b[36mINFO\u001b[0m[0191] [sidekick] Sidekick container already created on host [10.20.2.12] \r\n"]
[760.717747, "o", "\u001b[36mINFO\u001b[0m[0191] Checking if container [kubelet] is running on host [10.20.2.12], try #1 \r\n"]
[760.721952, "o", "\u001b[36mINFO\u001b[0m[0191] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0191] Checking if container [old-kubelet] is running on host [10.20.2.12], try #1 \r\n"]
[760.726807, "o", "\u001b[36mINFO\u001b[0m[0191] Stopping container [kubelet] on host [10.20.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[760.836495, "o", "\u001b[36mINFO\u001b[0m[0192] Waiting for [kubelet] container to exit on host [10.20.2.12] \r\n"]
[760.83733, "o", "\u001b[36mINFO\u001b[0m[0192] Renaming container [kubelet] to [old-kubelet] on host [10.20.2.12], try #1 \r\n"]
[760.869887, "o", "\u001b[36mINFO\u001b[0m[0192] Starting container [kubelet] on host [10.20.2.12], try #1 \r\n"]
[761.010233, "o", "\u001b[36mINFO\u001b[0m[0192] [worker] Successfully updated [kubelet] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0192] Removing container [old-kubelet] on host [10.20.2.12], try #1 \r\n"]
[761.024121, "o", "\u001b[36mINFO\u001b[0m[0192] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.12] \r\n"]
[771.90342, "o", "\u001b[36mINFO\u001b[0m[0203] [healthcheck] service [kubelet] on host [10.20.2.12] is healthy \r\n"]
[771.907915, "o", "\u001b[36mINFO\u001b[0m[0203] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[772.012445, "o", "\u001b[36mINFO\u001b[0m[0203] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[772.274873, "o", "\u001b[36mINFO\u001b[0m[0203] [worker] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[772.275666, "o", "\u001b[36mINFO\u001b[0m[0203] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[772.409968, "o", "\u001b[36mINFO\u001b[0m[0203] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n"]
[772.413821, "o", "\u001b[36mINFO\u001b[0m[0203] Checking if container [kube-proxy] is running on host [10.20.2.12], try #1 \r\n"]
[772.417911, "o", "\u001b[36mINFO\u001b[0m[0203] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0203] Checking if container [old-kube-proxy] is running on host [10.20.2.12], try #1 \r\n"]
[772.422079, "o", "\u001b[36mINFO\u001b[0m[0203] Stopping container [kube-proxy] on host [10.20.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[772.52811, "o", "\u001b[36mINFO\u001b[0m[0203] Waiting for [kube-proxy] container to exit on host [10.20.2.12] \r\n"]
[772.528929, "o", "\u001b[36mINFO\u001b[0m[0203] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.20.2.12], try #1 \r\n"]
[772.559296, "o", "\u001b[36mINFO\u001b[0m[0203] Starting container [kube-proxy] on host [10.20.2.12], try #1 \r\n"]
[772.648914, "o", "\u001b[36mINFO\u001b[0m[0203] [worker] Successfully updated [kube-proxy] container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0203] Removing container [old-kube-proxy] on host [10.20.2.12], try #1 \r\n"]
[772.668377, "o", "\u001b[36mINFO\u001b[0m[0203] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.12] \r\n"]
[772.91152, "o", "\u001b[36mINFO\u001b[0m[0204] [healthcheck] service [kube-proxy] on host [10.20.2.12] is healthy \r\n"]
[772.915334, "o", "\u001b[36mINFO\u001b[0m[0204] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[773.026989, "o", "\u001b[36mINFO\u001b[0m[0204] Starting container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[773.271429, "o", "\u001b[36mINFO\u001b[0m[0204] [worker] Successfully started [rke-log-linker] container on host [10.20.2.12] \r\n"]
[773.272452, "o", "\u001b[36mINFO\u001b[0m[0204] Removing container [rke-log-linker] on host [10.20.2.12], try #1 \r\n"]
[773.403392, "o", "\u001b[36mINFO\u001b[0m[0204] [remove/rke-log-linker] Successfully removed container on host [10.20.2.12] \r\n\u001b[36mINFO\u001b[0m[0204] [controlplane] Now checking status of node 10.20.2.12, try #1 \r\n"]
[773.4375, "o", "\u001b[36mINFO\u001b[0m[0204] [controlplane] Successfully upgraded Controller Plane.. \r\n"]
[773.437685, "o", "\u001b[36mINFO\u001b[0m[0204] [authz] Creating rke-job-deployer ServiceAccount \r\n"]
[773.470466, "o", "\u001b[36mINFO\u001b[0m[0204] [authz] rke-job-deployer ServiceAccount created successfully \r\n\u001b[36mINFO\u001b[0m[0204] [authz] Creating system:node ClusterRoleBinding \r\n"]
[773.491385, "o", "\u001b[36mINFO\u001b[0m[0204] [authz] system:node ClusterRoleBinding created successfully \r\n\u001b[36mINFO\u001b[0m[0204] [authz] Creating kube-apiserver proxy ClusterRole and ClusterRoleBinding \r\n"]
[773.523601, "o", "\u001b[36mINFO\u001b[0m[0204] [authz] kube-apiserver proxy ClusterRole and ClusterRoleBinding created successfully \r\n"]
[773.526227, "o", "\u001b[36mINFO\u001b[0m[0204] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/c1.rkestate] \r\n"]
[773.527313, "o", "\u001b[36mINFO\u001b[0m[0204] [state] Saving full cluster state to Kubernetes \r\n"]
[773.575964, "o", "\u001b[36mINFO\u001b[0m[0204] [state] Successfully Saved full cluster state to Kubernetes ConfigMap: full-cluster-state \r\n"]
[773.577661, "o", "\u001b[36mINFO\u001b[0m[0204] [worker] Now checking status of node 10.20.2.20, try #1 \r\n"]
[773.589027, "o", "\u001b[36mINFO\u001b[0m[0204] [worker] Now checking status of node 10.20.2.21, try #1 \r\n"]
[773.59482, "o", "\u001b[36mINFO\u001b[0m[0204] [worker] Now checking status of node 10.20.2.22, try #1 \r\n"]
[773.599673, "o", "\u001b[36mINFO\u001b[0m[0204] [worker] Upgrading Worker Plane..            \r\n"]
[773.613945, "o", "\u001b[36mINFO\u001b[0m[0204] Now checking and upgrading worker components on nodes with only worker role 1 at a time \r\n\u001b[36mINFO\u001b[0m[0204] [workerplane] Processing host 10.20.2.20     \r\n"]
[773.614074, "o", "\u001b[36mINFO\u001b[0m[0204] [worker] Now checking status of node 10.20.2.20, try #1 \r\n"]
[773.621712, "o", "\u001b[36mINFO\u001b[0m[0204] [worker] Getting list of nodes for upgrade   \r\n"]
[773.885037, "o", "\u001b[36mINFO\u001b[0m[0205] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[774.003077, "o", "\u001b[36mINFO\u001b[0m[0205] Starting container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[774.25459, "o", "\u001b[36mINFO\u001b[0m[0205] [worker] Successfully started [rke-log-linker] container on host [10.20.2.20] \r\n"]
[774.256157, "o", "\u001b[36mINFO\u001b[0m[0205] Removing container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[774.383165, "o", "\u001b[36mINFO\u001b[0m[0205] [remove/rke-log-linker] Successfully removed container on host [10.20.2.20] \r\n\u001b[36mINFO\u001b[0m[0205] Checking if container [service-sidekick] is running on host [10.20.2.20], try #1 \r\n"]
[774.388785, "o", "\u001b[36mINFO\u001b[0m[0205] [sidekick] Sidekick container already created on host [10.20.2.20] \r\n"]
[774.392731, "o", "\u001b[36mINFO\u001b[0m[0205] Checking if container [kubelet] is running on host [10.20.2.20], try #1 \r\n"]
[774.39729, "o", "\u001b[36mINFO\u001b[0m[0205] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.20] \r\n\u001b[36mINFO\u001b[0m[0205] Checking if container [old-kubelet] is running on host [10.20.2.20], try #1 \r\n"]
[774.401026, "o", "\u001b[36mINFO\u001b[0m[0205] Stopping container [kubelet] on host [10.20.2.20] with stopTimeoutDuration [5s], try #1 \r\n"]
[774.507956, "o", "\u001b[36mINFO\u001b[0m[0205] Waiting for [kubelet] container to exit on host [10.20.2.20] \r\n"]
[774.508849, "o", "\u001b[36mINFO\u001b[0m[0205] Renaming container [kubelet] to [old-kubelet] on host [10.20.2.20], try #1 \r\n"]
[774.560857, "o", "\u001b[36mINFO\u001b[0m[0205] Starting container [kubelet] on host [10.20.2.20], try #1 \r\n"]
[774.657954, "o", "\u001b[36mINFO\u001b[0m[0205] [worker] Successfully updated [kubelet] container on host [10.20.2.20] \r\n\u001b[36mINFO\u001b[0m[0205] Removing container [old-kubelet] on host [10.20.2.20], try #1 \r\n"]
[774.701938, "o", "\u001b[36mINFO\u001b[0m[0205] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.20] \r\n"]
[785.48082, "o", "\u001b[36mINFO\u001b[0m[0216] [healthcheck] service [kubelet] on host [10.20.2.20] is healthy \r\n"]
[785.492111, "o", "\u001b[36mINFO\u001b[0m[0216] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[785.610144, "o", "\u001b[36mINFO\u001b[0m[0216] Starting container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[785.832957, "o", "\u001b[36mINFO\u001b[0m[0217] [worker] Successfully started [rke-log-linker] container on host [10.20.2.20] \r\n"]
[785.83565, "o", "\u001b[36mINFO\u001b[0m[0217] Removing container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[785.951409, "o", "\u001b[36mINFO\u001b[0m[0217] [remove/rke-log-linker] Successfully removed container on host [10.20.2.20] \r\n"]
[785.953498, "o", "\u001b[36mINFO\u001b[0m[0217] Checking if container [kube-proxy] is running on host [10.20.2.20], try #1 \r\n"]
[785.956853, "o", "\u001b[36mINFO\u001b[0m[0217] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.20] \r\n\u001b[36mINFO\u001b[0m[0217] Checking if container [old-kube-proxy] is running on host [10.20.2.20], try #1 \r\n"]
[785.960375, "o", "\u001b[36mINFO\u001b[0m[0217] Stopping container [kube-proxy] on host [10.20.2.20] with stopTimeoutDuration [5s], try #1 \r\n"]
[786.06208, "o", "\u001b[36mINFO\u001b[0m[0217] Waiting for [kube-proxy] container to exit on host [10.20.2.20] \r\n"]
[786.062719, "o", "\u001b[36mINFO\u001b[0m[0217] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.20.2.20], try #1 \r\n"]
[786.086243, "o", "\u001b[36mINFO\u001b[0m[0217] Starting container [kube-proxy] on host [10.20.2.20], try #1 \r\n"]
[786.192433, "o", "\u001b[36mINFO\u001b[0m[0217] [worker] Successfully updated [kube-proxy] container on host [10.20.2.20] \r\n\u001b[36mINFO\u001b[0m[0217] Removing container [old-kube-proxy] on host [10.20.2.20], try #1 \r\n"]
[786.221782, "o", "\u001b[36mINFO\u001b[0m[0217] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.20] \r\n"]
[786.46827, "o", "\u001b[36mINFO\u001b[0m[0217] [healthcheck] service [kube-proxy] on host [10.20.2.20] is healthy \r\n"]
[786.471808, "o", "\u001b[36mINFO\u001b[0m[0217] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[786.587597, "o", "\u001b[36mINFO\u001b[0m[0217] Starting container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[786.817945, "o", "\u001b[36mINFO\u001b[0m[0218] [worker] Successfully started [rke-log-linker] container on host [10.20.2.20] \r\n"]
[786.821121, "o", "\u001b[36mINFO\u001b[0m[0218] Removing container [rke-log-linker] on host [10.20.2.20], try #1 \r\n"]
[786.946252, "o", "\u001b[36mINFO\u001b[0m[0218] [remove/rke-log-linker] Successfully removed container on host [10.20.2.20] \r\n\u001b[36mINFO\u001b[0m[0218] [worker] Now checking status of node 10.20.2.20, try #1 \r\n"]
[786.972453, "o", "\u001b[36mINFO\u001b[0m[0218] [workerplane] Processing host 10.20.2.21     \r\n\u001b[36mINFO\u001b[0m[0218] [worker] Now checking status of node 10.20.2.21, try #1 \r\n"]
[786.984142, "o", "\u001b[36mINFO\u001b[0m[0218] [worker] Getting list of nodes for upgrade   \r\n"]
[787.250627, "o", "\u001b[36mINFO\u001b[0m[0218] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[787.371417, "o", "\u001b[36mINFO\u001b[0m[0218] Starting container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[787.604934, "o", "\u001b[36mINFO\u001b[0m[0218] [worker] Successfully started [rke-log-linker] container on host [10.20.2.21] \r\n"]
[787.606034, "o", "\u001b[36mINFO\u001b[0m[0218] Removing container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[787.761243, "o", "\u001b[36mINFO\u001b[0m[0219] [remove/rke-log-linker] Successfully removed container on host [10.20.2.21] \r\n\u001b[36mINFO\u001b[0m[0219] Checking if container [service-sidekick] is running on host [10.20.2.21], try #1 \r\n"]
[787.767071, "o", "\u001b[36mINFO\u001b[0m[0219] [sidekick] Sidekick container already created on host [10.20.2.21] \r\n"]
[787.769077, "o", "\u001b[36mINFO\u001b[0m[0219] Checking if container [kubelet] is running on host [10.20.2.21], try #1 \r\n"]
[787.773459, "o", "\u001b[36mINFO\u001b[0m[0219] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.21] \r\n\u001b[36mINFO\u001b[0m[0219] Checking if container [old-kubelet] is running on host [10.20.2.21], try #1 \r\n"]
[787.777926, "o", "\u001b[36mINFO\u001b[0m[0219] Stopping container [kubelet] on host [10.20.2.21] with stopTimeoutDuration [5s], try #1 \r\n"]
[787.894411, "o", "\u001b[36mINFO\u001b[0m[0219] Waiting for [kubelet] container to exit on host [10.20.2.21] \r\n"]
[787.8952, "o", "\u001b[36mINFO\u001b[0m[0219] Renaming container [kubelet] to [old-kubelet] on host [10.20.2.21], try #1 \r\n"]
[787.939823, "o", "\u001b[36mINFO\u001b[0m[0219] Starting container [kubelet] on host [10.20.2.21], try #1 \r\n"]
[788.029787, "o", "\u001b[36mINFO\u001b[0m[0219] [worker] Successfully updated [kubelet] container on host [10.20.2.21] \r\n\u001b[36mINFO\u001b[0m[0219] Removing container [old-kubelet] on host [10.20.2.21], try #1 \r\n"]
[788.063445, "o", "\u001b[36mINFO\u001b[0m[0219] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.21] \r\n"]
[798.859291, "o", "\u001b[36mINFO\u001b[0m[0230] [healthcheck] service [kubelet] on host [10.20.2.21] is healthy \r\n"]
[798.865368, "o", "\u001b[36mINFO\u001b[0m[0230] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[798.978396, "o", "\u001b[36mINFO\u001b[0m[0230] Starting container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[799.219448, "o", "\u001b[36mINFO\u001b[0m[0230] [worker] Successfully started [rke-log-linker] container on host [10.20.2.21] \r\n"]
[799.22169, "o", "\u001b[36mINFO\u001b[0m[0230] Removing container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[799.348484, "o", "\u001b[36mINFO\u001b[0m[0230] [remove/rke-log-linker] Successfully removed container on host [10.20.2.21] \r\n"]
[799.351355, "o", "\u001b[36mINFO\u001b[0m[0230] Checking if container [kube-proxy] is running on host [10.20.2.21], try #1 \r\n"]
[799.355457, "o", "\u001b[36mINFO\u001b[0m[0230] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.21] \r\n\u001b[36mINFO\u001b[0m[0230] Checking if container [old-kube-proxy] is running on host [10.20.2.21], try #1 \r\n"]
[799.359947, "o", "\u001b[36mINFO\u001b[0m[0230] Stopping container [kube-proxy] on host [10.20.2.21] with stopTimeoutDuration [5s], try #1 \r\n"]
[799.463679, "o", "\u001b[36mINFO\u001b[0m[0230] Waiting for [kube-proxy] container to exit on host [10.20.2.21] \r\n"]
[799.464595, "o", "\u001b[36mINFO\u001b[0m[0230] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.20.2.21], try #1 \r\n"]
[799.489131, "o", "\u001b[36mINFO\u001b[0m[0230] Starting container [kube-proxy] on host [10.20.2.21], try #1 \r\n"]
[799.589304, "o", "\u001b[36mINFO\u001b[0m[0230] [worker] Successfully updated [kube-proxy] container on host [10.20.2.21] \r\n\u001b[36mINFO\u001b[0m[0230] Removing container [old-kube-proxy] on host [10.20.2.21], try #1 \r\n"]
[799.605307, "o", "\u001b[36mINFO\u001b[0m[0230] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.21] \r\n"]
[799.850501, "o", "\u001b[36mINFO\u001b[0m[0231] [healthcheck] service [kube-proxy] on host [10.20.2.21] is healthy \r\n"]
[799.854513, "o", "\u001b[36mINFO\u001b[0m[0231] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[799.963721, "o", "\u001b[36mINFO\u001b[0m[0231] Starting container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[800.186354, "o", "\u001b[36mINFO\u001b[0m[0231] [worker] Successfully started [rke-log-linker] container on host [10.20.2.21] \r\n"]
[800.187043, "o", "\u001b[36mINFO\u001b[0m[0231] Removing container [rke-log-linker] on host [10.20.2.21], try #1 \r\n"]
[800.312675, "o", "\u001b[36mINFO\u001b[0m[0231] [remove/rke-log-linker] Successfully removed container on host [10.20.2.21] \r\n\u001b[36mINFO\u001b[0m[0231] [worker] Now checking status of node 10.20.2.21, try #1 \r\n"]
[800.34355, "o", "\u001b[36mINFO\u001b[0m[0231] [workerplane] Processing host 10.20.2.22     \r\n\u001b[36mINFO\u001b[0m[0231] [worker] Now checking status of node 10.20.2.22, try #1 \r\n"]
[800.355984, "o", "\u001b[36mINFO\u001b[0m[0231] [worker] Getting list of nodes for upgrade   \r\n"]
[800.677397, "o", "\u001b[36mINFO\u001b[0m[0231] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[800.795795, "o", "\u001b[36mINFO\u001b[0m[0232] Starting container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[801.025433, "o", "\u001b[36mINFO\u001b[0m[0232] [worker] Successfully started [rke-log-linker] container on host [10.20.2.22] \r\n"]
[801.027537, "o", "\u001b[36mINFO\u001b[0m[0232] Removing container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[801.152494, "o", "\u001b[36mINFO\u001b[0m[0232] [remove/rke-log-linker] Successfully removed container on host [10.20.2.22] \r\n\u001b[36mINFO\u001b[0m[0232] Checking if container [service-sidekick] is running on host [10.20.2.22], try #1 \r\n"]
[801.159188, "o", "\u001b[36mINFO\u001b[0m[0232] [sidekick] Sidekick container already created on host [10.20.2.22] \r\n"]
[801.161801, "o", "\u001b[36mINFO\u001b[0m[0232] Checking if container [kubelet] is running on host [10.20.2.22], try #1 \r\n"]
[801.16522, "o", "\u001b[36mINFO\u001b[0m[0232] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.22] \r\n\u001b[36mINFO\u001b[0m[0232] Checking if container [old-kubelet] is running on host [10.20.2.22], try #1 \r\n"]
[801.169365, "o", "\u001b[36mINFO\u001b[0m[0232] Stopping container [kubelet] on host [10.20.2.22] with stopTimeoutDuration [5s], try #1 \r\n"]
[801.274321, "o", "\u001b[36mINFO\u001b[0m[0232] Waiting for [kubelet] container to exit on host [10.20.2.22] \r\n"]
[801.275247, "o", "\u001b[36mINFO\u001b[0m[0232] Renaming container [kubelet] to [old-kubelet] on host [10.20.2.22], try #1 \r\n"]
[801.321471, "o", "\u001b[36mINFO\u001b[0m[0232] Starting container [kubelet] on host [10.20.2.22], try #1 \r\n"]
[801.40978, "o", "\u001b[36mINFO\u001b[0m[0232] [worker] Successfully updated [kubelet] container on host [10.20.2.22] \r\n\u001b[36mINFO\u001b[0m[0232] Removing container [old-kubelet] on host [10.20.2.22], try #1 \r\n"]
[801.446818, "o", "\u001b[36mINFO\u001b[0m[0232] [healthcheck] Start Healthcheck on service [kubelet] on host [10.20.2.22] \r\n"]
[812.19974, "o", "\u001b[36mINFO\u001b[0m[0243] [healthcheck] service [kubelet] on host [10.20.2.22] is healthy \r\n"]
[812.203725, "o", "\u001b[36mINFO\u001b[0m[0243] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[812.325893, "o", "\u001b[36mINFO\u001b[0m[0243] Starting container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[812.559783, "o", "\u001b[36mINFO\u001b[0m[0243] [worker] Successfully started [rke-log-linker] container on host [10.20.2.22] \r\n"]
[812.561512, "o", "\u001b[36mINFO\u001b[0m[0243] Removing container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[812.697245, "o", "\u001b[36mINFO\u001b[0m[0243] [remove/rke-log-linker] Successfully removed container on host [10.20.2.22] \r\n"]
[812.69949, "o", "\u001b[36mINFO\u001b[0m[0243] Checking if container [kube-proxy] is running on host [10.20.2.22], try #1 \r\n"]
[812.702938, "o", "\u001b[36mINFO\u001b[0m[0243] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.20.2.22] \r\n\u001b[36mINFO\u001b[0m[0243] Checking if container [old-kube-proxy] is running on host [10.20.2.22], try #1 \r\n"]
[812.706745, "o", "\u001b[36mINFO\u001b[0m[0243] Stopping container [kube-proxy] on host [10.20.2.22] with stopTimeoutDuration [5s], try #1 \r\n"]
[812.801451, "o", "\u001b[36mINFO\u001b[0m[0244] Waiting for [kube-proxy] container to exit on host [10.20.2.22] \r\n"]
[812.802207, "o", "\u001b[36mINFO\u001b[0m[0244] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.20.2.22], try #1 \r\n"]
[812.825316, "o", "\u001b[36mINFO\u001b[0m[0244] Starting container [kube-proxy] on host [10.20.2.22], try #1 \r\n"]
[812.934105, "o", "\u001b[36mINFO\u001b[0m[0244] [worker] Successfully updated [kube-proxy] container on host [10.20.2.22] \r\n\u001b[36mINFO\u001b[0m[0244] Removing container [old-kube-proxy] on host [10.20.2.22], try #1 \r\n"]
[812.941376, "o", "\u001b[36mINFO\u001b[0m[0244] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.20.2.22] \r\n"]
[813.189382, "o", "\u001b[36mINFO\u001b[0m[0244] [healthcheck] service [kube-proxy] on host [10.20.2.22] is healthy \r\n"]
[813.193895, "o", "\u001b[36mINFO\u001b[0m[0244] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[813.304332, "o", "\u001b[36mINFO\u001b[0m[0244] Starting container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[813.541733, "o", "\u001b[36mINFO\u001b[0m[0244] [worker] Successfully started [rke-log-linker] container on host [10.20.2.22] \r\n"]
[813.543279, "o", "\u001b[36mINFO\u001b[0m[0244] Removing container [rke-log-linker] on host [10.20.2.22], try #1 \r\n"]
[813.675169, "o", "\u001b[36mINFO\u001b[0m[0244] [remove/rke-log-linker] Successfully removed container on host [10.20.2.22] \r\n\u001b[36mINFO\u001b[0m[0244] [worker] Now checking status of node 10.20.2.22, try #1 \r\n"]
[813.700234, "o", "\u001b[36mINFO\u001b[0m[0244] [worker] Successfully upgraded Worker Plane.. \r\n"]
[813.703303, "o", "\u001b[36mINFO\u001b[0m[0244] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.21] \r\n"]
[813.70425, "o", "\u001b[36mINFO\u001b[0m[0244] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.20] \r\n"]
[813.704993, "o", "\u001b[36mINFO\u001b[0m[0244] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.12] \r\n"]
[813.713969, "o", "\u001b[36mINFO\u001b[0m[0244] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.22] \r\n"]
[813.994569, "o", "\u001b[36mINFO\u001b[0m[0245] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.11] \r\n"]
[814.006286, "o", "\u001b[36mINFO\u001b[0m[0245] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.20.2.10] \r\n"]
[814.152181, "o", "\u001b[36mINFO\u001b[0m[0245] Starting container [rke-log-cleaner] on host [10.20.2.20], try #1 \r\n\u001b[36mINFO\u001b[0m[0245] Starting container [rke-log-cleaner] on host [10.20.2.21], try #1 \r\n"]
[814.159321, "o", "\u001b[36mINFO\u001b[0m[0245] Starting container [rke-log-cleaner] on host [10.20.2.22], try #1 \r\n\u001b[36mINFO\u001b[0m[0245] Starting container [rke-log-cleaner] on host [10.20.2.12], try #1 \r\n"]
[814.434018, "o", "\u001b[36mINFO\u001b[0m[0245] Starting container [rke-log-cleaner] on host [10.20.2.11], try #1 \r\n"]
[814.439857, "o", "\u001b[36mINFO\u001b[0m[0245] Starting container [rke-log-cleaner] on host [10.20.2.10], try #1 \r\n"]
[814.73036, "o", "\u001b[36mINFO\u001b[0m[0245] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.21] \r\n"]
[814.732107, "o", "\u001b[36mINFO\u001b[0m[0245] Removing container [rke-log-cleaner] on host [10.20.2.21], try #1 \r\n"]
[814.756558, "o", "\u001b[36mINFO\u001b[0m[0246] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.22] \r\n"]
[814.759451, "o", "\u001b[36mINFO\u001b[0m[0246] Removing container [rke-log-cleaner] on host [10.20.2.22], try #1 \r\n"]
[814.761849, "o", "\u001b[36mINFO\u001b[0m[0246] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.20] \r\n"]
[814.765909, "o", "\u001b[36mINFO\u001b[0m[0246] Removing container [rke-log-cleaner] on host [10.20.2.20], try #1 \r\n"]
[814.783991, "o", "\u001b[36mINFO\u001b[0m[0246] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.12] \r\n"]
[814.78912, "o", "\u001b[36mINFO\u001b[0m[0246] Removing container [rke-log-cleaner] on host [10.20.2.12], try #1 \r\n"]
[814.851121, "o", "\u001b[36mINFO\u001b[0m[0246] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.11] \r\n"]
[814.853781, "o", "\u001b[36mINFO\u001b[0m[0246] Removing container [rke-log-cleaner] on host [10.20.2.11], try #1 \r\n"]
[814.876298, "o", "\u001b[36mINFO\u001b[0m[0246] [cleanup] Successfully started [rke-log-cleaner] container on host [10.20.2.10] \r\n"]
[814.879718, "o", "\u001b[36mINFO\u001b[0m[0246] Removing container [rke-log-cleaner] on host [10.20.2.10], try #1 \r\n"]
[814.914194, "o", "\u001b[36mINFO\u001b[0m[0246] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.21] \r\n"]
[814.929746, "o", "\u001b[36mINFO\u001b[0m[0246] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.20] \r\n"]
[814.932837, "o", "\u001b[36mINFO\u001b[0m[0246] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.22] \r\n"]
[814.963233, "o", "\u001b[36mINFO\u001b[0m[0246] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.12] \r\n"]
[815.025207, "o", "\u001b[36mINFO\u001b[0m[0246] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.11] \r\n"]
[815.064858, "o", "\u001b[36mINFO\u001b[0m[0246] [remove/rke-log-cleaner] Successfully removed container on host [10.20.2.10] \r\n\u001b[36mINFO\u001b[0m[0246] [sync] Syncing nodes Labels and Taints       \r\n"]
[815.082196, "o", "\u001b[36mINFO\u001b[0m[0246] [sync] Successfully synced nodes Labels and Taints \r\n"]
[815.082307, "o", "\u001b[36mINFO\u001b[0m[0246] [network] Setting up network plugin: calico  \r\n"]
[815.083108, "o", "\u001b[36mINFO\u001b[0m[0246] [addons] Saving ConfigMap for addon rke-network-plugin to Kubernetes \r\n"]
[815.114116, "o", "\u001b[36mINFO\u001b[0m[0246] [addons] Successfully saved ConfigMap for addon rke-network-plugin to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0246] [addons] Executing deploy job rke-network-plugin \r\n"]
[830.174226, "o", "\u001b[36mINFO\u001b[0m[0261] [addons] Setting up coredns                  \r\n"]
[830.174606, "o", "\u001b[36mINFO\u001b[0m[0261] [addons] Saving ConfigMap for addon rke-coredns-addon to Kubernetes \r\n"]
[830.187614, "o", "\u001b[36mINFO\u001b[0m[0261] [addons] Successfully saved ConfigMap for addon rke-coredns-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0261] [addons] Executing deploy job rke-coredns-addon \r\n"]
[840.228865, "o", "\u001b[36mINFO\u001b[0m[0271] [addons] CoreDNS deployed successfully       \r\n\u001b[36mINFO\u001b[0m[0271] [dns] DNS provider coredns deployed successfully \r\n"]
[840.23295, "o", "\u001b[36mINFO\u001b[0m[0271] [addons] Setting up Metrics Server           \r\n"]
[840.233223, "o", "\u001b[36mINFO\u001b[0m[0271] [addons] Saving ConfigMap for addon rke-metrics-addon to Kubernetes \r\n"]
[840.237255, "o", "\u001b[36mINFO\u001b[0m[0271] [addons] Successfully saved ConfigMap for addon rke-metrics-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0271] [addons] Executing deploy job rke-metrics-addon \r\n"]
[840.261465, "o", "\u001b[36mINFO\u001b[0m[0271] [addons] Metrics Server deployed successfully \r\n\u001b[36mINFO\u001b[0m[0271] [ingress] Setting up nginx ingress controller \r\n"]
[840.262201, "o", "\u001b[36mINFO\u001b[0m[0271] [addons] Saving ConfigMap for addon rke-ingress-controller to Kubernetes \r\n"]
[840.266521, "o", "\u001b[36mINFO\u001b[0m[0271] [addons] Successfully saved ConfigMap for addon rke-ingress-controller to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0271] [addons] Executing deploy job rke-ingress-controller \r\n"]
[840.285302, "o", "\u001b[36mINFO\u001b[0m[0271] [ingress] ingress controller nginx deployed successfully \r\n\u001b[36mINFO\u001b[0m[0271] [addons] Setting up user addons              \r\n"]
[840.299042, "o", "\u001b[36mINFO\u001b[0m[0271] [addons] no user addons defined              \r\n\u001b[36mINFO\u001b[0m[0271] Finished building Kubernetes cluster successfully \r\n"]
[840.302649, "o", "\r\nreal\t4m31.851s\r\nuser\t0m1.407s\r\nsys\t0m0.488s\r\n"]
[870.841881, "o", "NAME              STATUS   ROLES               AGE     VERSION\r\nnode/10.20.2.10   Ready    controlplane,etcd   6m11s   v1.19.4\r\nnode/10.20.2.11   Ready    controlplane,etcd   6m11s   v1.19.4\r\nnode/10.20.2.12   Ready    controlplane,etcd   6m12s   v1.19.4\r\nnode/10.20.2.20   Ready    worker              6m10s   v1.19.4\r\n"]
[870.842021, "o", "node/10.20.2.21   Ready    worker              6m11s   v1.19.4\r\nnode/10.20.2.22   Ready    worker              6m10s   v1.19.4\r\n"]
[870.848871, "o", "\r\nNAMESPACE       NAME                                           READY   STATUS              RESTARTS   AGE\r\ningress-nginx   pod/default-http-backend-5b564dd459-kpngl      1/1     Running             0          5m36s\r\ningress-nginx   pod/nginx-ingress-controller-hdcbp             1/1     Running             0          5m36s\r\ningress-nginx   pod/nginx-ingress-controller-rw6vk             1/1     Running             0          5m36s\r\ningress-nginx   pod/nginx-ingress-controller-zvh4c             1/1     Running             0          5m36s\r\nkube-system     pod/calico-kube-controllers-7dcd869879-sk45x   0/1     ContainerCreating   0          35s"]
[870.849, "o", "\r\nkube-system     pod/calico-node-4ttvf                          1/1     Running             0          43s\r\nkube-system     pod/calico-node-b2ccc                          0/1     Running             0          13s\r\nkube-system     pod/calico-node-klvbb                          1/1     Running             0          5m55s\r\nkube-system     pod/calico-node-m7lf2                          1/1     Running             0          5m55s\r\nkube-system     pod/calico-node-mlc8z                          1/1     Running             0          5m55s\r\nkube-system     pod/calico-node-zzwl9                          1/1     Running             0          5m55s\r\nkube-system     pod/coredns-5d44d4c69-6rr5v                    0/1     Running        "]
[870.849084, "o", "     0          34s\r\nkube-system     pod/coredns-5d44d4c69-m74tg                    0/1     ContainerCreating   0          25s\r\nkube-system     pod/coredns-autoscaler-557f965569-bgfvj        0/1     Terminating         0          5m51s\r\nkube-system     pod/coredns-autoscaler-8df879c6f-tbjs7         1/1     Running             0          35s\r\nkube-system     pod/metrics-server-77956db857-69jkz            1/1     Running             0          5m43s\r\nkube-system     pod/rke-coredns-addon-deploy-job-jdv9k         0/1     "]
[870.849152, "o", "Completed           0          36s\r\nkube-system     pod/rke-ingress-controller-deploy-job-2mwl2    0/1     Completed           0          5m38s\r\nkube-system     pod/rke-metrics-addon-deploy-job-99gpv         0/1     Completed           0          5m48s\r\nkube-system     pod/rke-network-plugin-deploy-job-dc6kk        0/1     Completed           0          51s\r\n"]
