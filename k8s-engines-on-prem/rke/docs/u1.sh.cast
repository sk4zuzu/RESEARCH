{"version": 2, "width": 191, "height": 48, "timestamp": 1607172716, "env": {"SHELL": "/run/current-system/sw/bin/bash", "TERM": "xterm-256color"}}
[0.198734, "o", "cd /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ && ansible-playbook -vv --inventory=/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/u1.ini prereq.yml\r\n"]
[2.420049, "o", "\u001b[0;34mansible-playbook 2.10.3\u001b[0m\r\n\u001b[0;34m  config file = /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ansible.cfg\u001b[0m\r\n\u001b[0;34m  configured module search path = ['/home/asd/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']\u001b[0m\r\n\u001b[0;34m  ansible python module location = /home/asd/3.7/lib/python3.7/site-packages/ansible\u001b[0m\r\n\u001b[0;34m  executable location = /home/asd/3.7/bin/ansible-playbook\u001b[0m\r\n\u001b[0;34m  python version = 3.7.8 (default, Jun 27 2020, 09:38:56) [GCC 9.3.0]\u001b[0m\r\n\u001b[0;34mUsing /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ansible.cfg as config file\u001b[0m\r\n"]
[2.943903, "o", "\u001b[0;34mredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\u001b[0m\r\n"]
[3.037794, "o", "\u001b[0;34mredirecting (type: callback) ansible.builtin.yaml to community.general.yaml\u001b[0m\r\n"]
[3.07541, "o", "\u001b[0;34mSkipping callback 'default', as we already have a stdout callback.\u001b[0m\r\n\u001b[0;34mSkipping callback 'minimal', as we already have a stdout callback.\u001b[0m\r\n"]
[3.075461, "o", "\u001b[0;34mSkipping callback 'oneline', as we already have a stdout callback.\u001b[0m\r\n\r\nPLAYBOOK: prereq.yml **************************************************************************************************************************************************************************\r\n"]
[3.075576, "o", "\u001b[0;34m1 plays in prereq.yml\u001b[0m\r\n"]
[3.078161, "o", "\r\nPLAY [m:n] ************************************************************************************************************************************************************************************\r\n"]
[3.093245, "o", "\r\nTASK [Gathering Facts] ************************************************************************************************************************************************************************\r\n"]
[3.093349, "o", "\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/prereq.yml:2\u001b[0m\r\n"]
[6.704038, "o", "\u001b[0;32mok: [u1a3]\u001b[0m\r\n"]
[6.977452, "o", "\u001b[0;32mok: [u1b2]\u001b[0m\r\n"]
[7.160074, "o", "\u001b[0;32mok: [u1b1]\u001b[0m\r\n"]
[7.200382, "o", "\u001b[0;32mok: [u1a1]\u001b[0m\r\n"]
[7.341267, "o", "\u001b[0;32mok: [u1a2]\u001b[0m\r\n"]
[8.757603, "o", "\u001b[0;32mok: [u1b3]\u001b[0m\r\n"]
[8.768497, "o", "\u001b[0;34mMETA: ran handlers\u001b[0m\r\n"]
[8.782163, "o", "\r\nTASK [modprobe : shell] ***********************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/modprobe/tasks/main.yml:2\u001b[0m\r\n"]
[9.777773, "o", "\u001b[0;33mchanged: [u1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.443583'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:05.463347'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:05.019764'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[10.345226, "o", "\u001b[0;33mchanged: [u1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.331019'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:06.712587'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:06.381568'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[11.308555, "o", "\u001b[0;33mchanged: [u1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:01.965288'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:07.670091'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:05.704803'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[11.326272, "o", "\u001b[0;33mchanged: [u1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:02.007823'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:07.688203'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:05.680380'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[11.333283, "o", "\u001b[0;33mchanged: [u1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:01.995279'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:07.694820'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:05.699541'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[11.334442, "o", "\u001b[0;33mchanged: [u1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 modprobe\u001b[0m\r\n\u001b[0;33m  delta: '0:00:01.986777'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:07.694387'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:05.707610'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: ''\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[11.355237, "o", "\r\nTASK [sysctl : shell] *************************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/sysctl/tasks/main.yml:2\u001b[0m\r\n"]
[11.621141, "o", "\u001b[0;33mchanged: [u1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.003930'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:07.974014'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:07.970084'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[11.624444, "o", "\u001b[0;33mchanged: [u1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.003386'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:07.978607'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:07.975221'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[11.627447, "o", "\u001b[0;33mchanged: [u1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.005848'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:07.988272'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:07.982424'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[11.641103, "o", "\u001b[0;33mchanged: [u1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.003824'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:07.334072'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:07.330248'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[11.656449, "o", "\u001b[0;33mchanged: [u1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.004736'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:08.021839'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:08.017103'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[11.817888, "o", "\u001b[0;33mchanged: [u1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  cmd: |-\u001b[0m\r\n\u001b[0;33m    xargs --no-run-if-empty -n1 sysctl -w\u001b[0m\r\n\u001b[0;33m  delta: '0:00:00.003843'\u001b[0m\r\n\u001b[0;33m  end: '2020-12-05 12:52:08.188493'\u001b[0m\r\n\u001b[0;33m  rc: 0\u001b[0m\r\n\u001b[0;33m  start: '2020-12-05 12:52:08.184650'\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: net.bridge.bridge-nf-call-iptables = 1\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[11.824034, "o", "\r\nTASK [docker : include_tasks] *****************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/main.yml:2\u001b[0m\r\n"]
[11.923757, "o", "\u001b[0;36mincluded: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/debian.yml for u1a1, u1a2, u1a3, u1b1, u1b2, u1b3\u001b[0m\r\n"]
[11.934134, "o", "\r\nTASK [docker : apt_key] ***********************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/debian.yml:2\u001b[0m\r\n"]
[13.220111, "o", "\u001b[0;33mchanged: [u1a1] => changed=true\u001b[0m\r\n"]
[13.22076, "o", "\u001b[0;33mchanged: [u1a3] => changed=true\u001b[0m\r\n"]
[13.221059, "o", "\u001b[0;33mchanged: [u1a2] => changed=true\u001b[0m\r\n"]
[13.221318, "o", "\u001b[0;33mchanged: [u1b1] => changed=true\u001b[0m\r\n"]
[13.231493, "o", "\u001b[0;33mchanged: [u1b2] => changed=true\u001b[0m\r\n"]
[14.110696, "o", "\u001b[0;33mchanged: [u1b3] => changed=true\u001b[0m\r\n"]
[14.116876, "o", "\r\nTASK [docker : apt_repository] ****************************************************************************************************************************************************************\r\n"]
[14.116988, "o", "\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/debian.yml:6\u001b[0m\r\n"]
[18.984496, "o", "\u001b[0;33mchanged: [u1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[19.180176, "o", "\u001b[0;33mchanged: [u1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[19.189682, "o", "\u001b[0;33mchanged: [u1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[19.269694, "o", "\u001b[0;33mchanged: [u1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[19.270361, "o", "\u001b[0;33mchanged: [u1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[22.692061, "o", "\u001b[0;33mchanged: [u1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n"]
[22.698238, "o", "\r\nTASK [docker : apt] ***************************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/debian.yml:14\u001b[0m\r\n"]
[203.53792, "o", "\u001b[1;30mFAILED - RETRYING: docker : apt (8 retries left).\u001b[0m\r\n"]
[203.54794, "o", "\u001b[0;33mchanged: [u1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  cache_update_time: 1607172740\u001b[0m\r\n\u001b[0;33m  cache_updated: true\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: |-\u001b[0m\r\n\u001b[0;33m    Reading package lists...\u001b[0m\r\n\u001b[0;33m    Building dependency tree...\u001b[0m\r\n\u001b[0;33m    Reading state information...\u001b[0m\r\n\u001b[0;33m    The following package was automatically installed and is no longer required:\u001b[0m\r\n\u001b[0;33m      grub-pc-bin\u001b[0m\r\n\u001b[0;33m    Use 'sudo apt autoremove' to remove it.\u001b[0m\r\n\u001b[0;33m    The following additional packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount libltdl7 pigz\u001b[0m\r\n\u001b[0;33m    The following NEW packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount containerd.io docker-ce docker-ce-cli libltdl7\u001b[0m\r\n\u001b[0;33m      pigz\u001b[0m\r\n\u001b[0;33m    0 upgraded, 7 newly installed, 0 to remove and 48 not upgraded.\u001b[0m\r\n\u001b[0;33m    Need to get 91.3 MB of archives.\u001b[0m\r\n\u001b[0;33m    After this operation, 410 MB of add"]
[203.548084, "o", "itional disk space will be used.\u001b[0m\r\n\u001b[0;33m    Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pigz amd64 2.4-1 [57.4 kB]\u001b[0m\r\n\u001b[0;33m    Get:2 https://download.docker.com/linux/ubuntu bionic/stable amd64 containerd.io amd64 1.3.7-1 [24.4 MB]\u001b[0m\r\n\u001b[0;33m    Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 aufs-tools amd64 1:4.9+20170918-1ubuntu1 [104 kB]\u001b[0m\r\n\u001b[0;33m    Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cgroupfs-mount all 1.4 [6320 B]\u001b[0m\r\n\u001b[0;33m    Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl7 amd64 2.4.6-2 [38.8 kB]\u001b[0m\r\n\u001b[0;33m    Get:6 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-cli amd64 5:19.03.13~3-0~ubuntu-bionic [44.2 MB]\u001b[0m\r\n\u001b[0;33m    Get:7 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce amd64 5:19.03.13~3-0~ubuntu-bionic [22.5 MB]\u001b[0m\r\n\u001b[0;33m    Fetched 91.3 MB in 2min 44s (557 kB/s)\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package pigz.\u001b[0m\r\n\u001b[0;33m    (Reading da"]
[203.548173, "o", "tabase ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 61374 files and directories currently installed.)\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking pigz (2.4-1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package aufs-tools.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../1-aufs-tools_1%3a4.9+20170918-1ubuntu1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking aufs-tools (1:4.9+20170918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package cgroupfs-mount.\u001b[0m\r\n\u001b[0;33m    Preparing to un"]
[203.548234, "o", "pack .../2-cgroupfs-mount_1.4_all.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package containerd.io.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../3-containerd.io_1.3.7-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce-cli.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../4-docker-ce-cli_5%3a19.03.13~3-0~ubuntu-bionic_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../5-docker-ce_5%3a19.03.13~3-0~ubuntu-bionic_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package libltdl7:amd64.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../6-libltdl7_2.4.6-2_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up aufs-tools (1:4.9+2"]
[203.548307, "o", "0170918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Setting up containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service -> /lib/systemd/system/containerd.service.\u001b[0m\r\n\u001b[0;33m    Setting up cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Setting up libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Setting up pigz (2.4-1) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/docker.service -> /lib/systemd/system/docker.service.\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/sockets.target.wants/docker.socket -> /lib/systemd/system/docker.socket.\u001b[0m\r\n\u001b[0;33m    Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for systemd (237-3ubuntu10.42) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for ur"]
[203.548372, "o", "eadahead (0.100.0-21) ...\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[207.66285, "o", "\u001b[0;33mchanged: [u1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  cache_update_time: 1607172740\u001b[0m\r\n\u001b[0;33m  cache_updated: true\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: |-\u001b[0m\r\n\u001b[0;33m    Reading package lists...\u001b[0m\r\n\u001b[0;33m    Building dependency tree...\u001b[0m\r\n\u001b[0;33m    Reading state information...\u001b[0m\r\n\u001b[0;33m    The following package was automatically installed and is no longer required:\u001b[0m\r\n\u001b[0;33m      grub-pc-bin\u001b[0m\r\n\u001b[0;33m    Use 'sudo apt autoremove' to remove it.\u001b[0m\r\n\u001b[0;33m    The following additional packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount libltdl7 pigz\u001b[0m\r\n\u001b[0;33m    The following NEW packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount containerd.io docker-ce docker-ce-cli libltdl7\u001b[0m\r\n\u001b[0;33m      pigz\u001b[0m\r\n\u001b[0;33m    0 upgraded, 7 newly installed, 0 to remove and 48 not upgraded.\u001b[0m\r\n\u001b[0;33m    Need to get 91.3 MB of archives.\u001b[0m\r\n\u001b[0;33m    After this operation, 410 MB of add"]
[207.662906, "o", "itional disk space will be used.\u001b[0m\r\n\u001b[0;33m    Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pigz amd64 2.4-1 [57.4 kB]\u001b[0m\r\n\u001b[0;33m    Get:2 https://download.docker.com/linux/ubuntu bionic/stable amd64 containerd.io amd64 1.3.7-1 [24.4 MB]\u001b[0m\r\n\u001b[0;33m    Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 aufs-tools amd64 1:4.9+20170918-1ubuntu1 [104 kB]\u001b[0m\r\n\u001b[0;33m    Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cgroupfs-mount all 1.4 [6320 B]\u001b[0m\r\n\u001b[0;33m    Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl7 amd64 2.4.6-2 [38.8 kB]\u001b[0m\r\n\u001b[0;33m    Get:6 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-cli amd64 5:19.03.13~3-0~ubuntu-bionic [44.2 MB]\u001b[0m\r\n\u001b[0;33m    Get:7 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce amd64 5:19.03.13~3-0~ubuntu-bionic [22.5 MB]\u001b[0m\r\n\u001b[0;33m    Fetched 91.3 MB in 2min 48s (543 kB/s)\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package pigz.\u001b[0m\r\n\u001b[0;33m    (Reading da"]
[207.662917, "o", "tabase ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 61374 files and directories currently installed.)\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking pigz (2.4-1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package aufs-tools.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../1-aufs-tools_1%3a4.9+20170918-1ubuntu1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking aufs-tools (1:4.9+20170918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package cgroupfs-mount.\u001b[0m\r\n\u001b[0;33m    Preparing to un"]
[207.663044, "o", "pack .../2-cgroupfs-mount_1.4_all.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package containerd.io.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../3-containerd.io_1.3.7-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce-cli.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../4-docker-ce-cli_5%3a19.03.13~3-0~ubuntu-bionic_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../5-docker-ce_5%3a19.03.13~3-0~ubuntu-bionic_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package libltdl7:amd64.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../6-libltdl7_2.4.6-2_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up aufs-tools (1:4.9+2"]
[207.663065, "o", "0170918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Setting up containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service -> /lib/systemd/system/containerd.service.\u001b[0m\r\n\u001b[0;33m    Setting up cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Setting up libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Setting up pigz (2.4-1) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/docker.service -> /lib/systemd/system/docker.service.\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/sockets.target.wants/docker.socket -> /lib/systemd/system/docker.socket.\u001b[0m\r\n\u001b[0;33m    Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for systemd (237-3ubuntu10.42) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for ur"]
[207.663073, "o", "eadahead (0.100.0-21) ...\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[211.620919, "o", "\u001b[0;33mchanged: [u1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  cache_update_time: 1607172740\u001b[0m\r\n\u001b[0;33m  cache_updated: true\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: |-\u001b[0m\r\n\u001b[0;33m    Reading package lists...\u001b[0m\r\n\u001b[0;33m    Building dependency tree...\u001b[0m\r\n\u001b[0;33m    Reading state information...\u001b[0m\r\n\u001b[0;33m    The following package was automatically installed and is no longer required:\u001b[0m\r\n\u001b[0;33m      grub-pc-bin\u001b[0m\r\n\u001b[0;33m    Use 'sudo apt autoremove' to remove it.\u001b[0m\r\n\u001b[0;33m    The following additional packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount libltdl7 pigz\u001b[0m\r\n\u001b[0;33m    The following NEW packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount containerd.io docker-ce docker-ce-cli libltdl7\u001b[0m\r\n\u001b[0;33m      pigz\u001b[0m\r\n\u001b[0;33m    0 upgraded, 7 newly installed, 0 to remove and 48 not upgraded.\u001b[0m\r\n\u001b[0;33m    Need to get 91.3 MB of archives.\u001b[0m\r\n\u001b[0;33m    After this operation, 410 MB of add"]
[211.621087, "o", "itional disk space will be used.\u001b[0m\r\n\u001b[0;33m    Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pigz amd64 2.4-1 [57.4 kB]\u001b[0m\r\n\u001b[0;33m    Get:2 https://download.docker.com/linux/ubuntu bionic/stable amd64 containerd.io amd64 1.3.7-1 [24.4 MB]\u001b[0m\r\n\u001b[0;33m    Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 aufs-tools amd64 1:4.9+20170918-1ubuntu1 [104 kB]\u001b[0m\r\n\u001b[0;33m    Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cgroupfs-mount all 1.4 [6320 B]\u001b[0m\r\n\u001b[0;33m    Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl7 amd64 2.4.6-2 [38.8 kB]\u001b[0m\r\n\u001b[0;33m    Get:6 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-cli amd64 5:19.03.13~3-0~ubuntu-bionic [44.2 MB]\u001b[0m\r\n\u001b[0;33m    Get:7 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce amd64 5:19.03.13~3-0~ubuntu-bionic [22.5 MB]\u001b[0m\r\n\u001b[0;33m    Fetched 91.3 MB in 2min 52s (532 kB/s)\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package pigz.\u001b[0m\r\n\u001b[0;33m    (Reading da"]
[211.621239, "o", "tabase ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 61374 files and directories currently installed.)\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking pigz (2.4-1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package aufs-tools.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../1-aufs-tools_1%3a4.9+20170918-1ubuntu1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking aufs-tools (1:4.9+20170918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package cgroupfs-mount.\u001b[0m\r\n\u001b[0;33m    Preparing to un"]
[211.621387, "o", "pack .../2-cgroupfs-mount_1.4_all.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package containerd.io.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../3-containerd.io_1.3.7-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce-cli.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../4-docker-ce-cli_5%3a19.03.13~3-0~ubuntu-bionic_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../5-docker-ce_5%3a19.03.13~3-0~ubuntu-bionic_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package libltdl7:amd64.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../6-libltdl7_2.4.6-2_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up aufs-tools (1:4.9+2"]
[211.62151, "o", "0170918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Setting up containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service -> /lib/systemd/system/containerd.service.\u001b[0m\r\n\u001b[0;33m    Setting up cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Setting up libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Setting up pigz (2.4-1) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/docker.service -> /lib/systemd/system/docker.service.\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/sockets.target.wants/docker.socket -> /lib/systemd/system/docker.socket.\u001b[0m\r\n\u001b[0;33m    Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for systemd (237-3ubuntu10.42) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for ur"]
[211.62157, "o", "eadahead (0.100.0-21) ...\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[212.454254, "o", "\u001b[0;33mchanged: [u1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  cache_update_time: 1607172740\u001b[0m\r\n\u001b[0;33m  cache_updated: true\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: |-\u001b[0m\r\n\u001b[0;33m    Reading package lists...\u001b[0m\r\n\u001b[0;33m    Building dependency tree...\u001b[0m\r\n\u001b[0;33m    Reading state information...\u001b[0m\r\n\u001b[0;33m    The following package was automatically installed and is no longer required:\u001b[0m\r\n\u001b[0;33m      grub-pc-bin\u001b[0m\r\n\u001b[0;33m    Use 'sudo apt autoremove' to remove it.\u001b[0m\r\n\u001b[0;33m    The following additional packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount libltdl7 pigz\u001b[0m\r\n\u001b[0;33m    The following NEW packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount containerd.io docker-ce docker-ce-cli libltdl7\u001b[0m\r\n\u001b[0;33m      pigz\u001b[0m\r\n\u001b[0;33m    0 upgraded, 7 newly installed, 0 to remove and 48 not upgraded.\u001b[0m\r\n\u001b[0;33m    Need to get 91.3 MB of archives.\u001b[0m\r\n\u001b[0;33m    After this operation, 410 MB of add"]
[212.454287, "o", "itional disk space will be used.\u001b[0m\r\n\u001b[0;33m    Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pigz amd64 2.4-1 [57.4 kB]\u001b[0m\r\n\u001b[0;33m    Get:2 https://download.docker.com/linux/ubuntu bionic/stable amd64 containerd.io amd64 1.3.7-1 [24.4 MB]\u001b[0m\r\n\u001b[0;33m    Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 aufs-tools amd64 1:4.9+20170918-1ubuntu1 [104 kB]\u001b[0m\r\n\u001b[0;33m    Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cgroupfs-mount all 1.4 [6320 B]\u001b[0m\r\n\u001b[0;33m    Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl7 amd64 2.4.6-2 [38.8 kB]\u001b[0m\r\n\u001b[0;33m    Get:6 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-cli amd64 5:19.03.13~3-0~ubuntu-bionic [44.2 MB]\u001b[0m\r\n\u001b[0;33m    Get:7 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce amd64 5:19.03.13~3-0~ubuntu-bionic [22.5 MB]\u001b[0m\r\n\u001b[0;33m    Fetched 91.3 MB in 3min 5s (494 kB/s)\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package pigz.\u001b[0m\r\n\u001b[0;33m    (Reading dat"]
[212.454299, "o", "abase ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 61374 files and directories currently installed.)\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking pigz (2.4-1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package aufs-tools.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../1-aufs-tools_1%3a4.9+20170918-1ubuntu1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking aufs-tools (1:4.9+20170918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package cgroupfs-mount.\u001b[0m\r\n\u001b[0;33m    Preparing to unp"]
[212.454306, "o", "ack .../2-cgroupfs-mount_1.4_all.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package containerd.io.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../3-containerd.io_1.3.7-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce-cli.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../4-docker-ce-cli_5%3a19.03.13~3-0~ubuntu-bionic_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../5-docker-ce_5%3a19.03.13~3-0~ubuntu-bionic_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package libltdl7:amd64.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../6-libltdl7_2.4.6-2_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up aufs-tools (1:4.9+20"]
[212.454312, "o", "170918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Setting up containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service -> /lib/systemd/system/containerd.service.\u001b[0m\r\n\u001b[0;33m    Setting up cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Setting up libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Setting up pigz (2.4-1) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/docker.service -> /lib/systemd/system/docker.service.\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/sockets.target.wants/docker.socket -> /lib/systemd/system/docker.socket.\u001b[0m\r\n\u001b[0;33m    Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for systemd (237-3ubuntu10.42) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for ure"]
[212.454467, "o", "adahead (0.100.0-21) ...\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[258.585892, "o", "\u001b[0;33mchanged: [u1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 2\u001b[0m\r\n\u001b[0;33m  cache_update_time: 1607172918\u001b[0m\r\n\u001b[0;33m  cache_updated: true\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: |-\u001b[0m\r\n\u001b[0;33m    Reading package lists...\u001b[0m\r\n\u001b[0;33m    Building dependency tree...\u001b[0m\r\n\u001b[0;33m    Reading state information...\u001b[0m\r\n\u001b[0;33m    The following package was automatically installed and is no longer required:\u001b[0m\r\n\u001b[0;33m      grub-pc-bin\u001b[0m\r\n\u001b[0;33m    Use 'sudo apt autoremove' to remove it.\u001b[0m\r\n\u001b[0;33m    The following additional packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount libltdl7 pigz\u001b[0m\r\n\u001b[0;33m    The following NEW packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount containerd.io docker-ce docker-ce-cli libltdl7\u001b[0m\r\n\u001b[0;33m      pigz\u001b[0m\r\n\u001b[0;33m    0 upgraded, 7 newly installed, 0 to remove and 48 not upgraded.\u001b[0m\r\n\u001b[0;33m    Need to get 44.2 MB/91.3 MB of archives.\u001b[0m\r\n\u001b[0;33m    After this operation, 410 M"]
[258.585981, "o", "B of additional disk space will be used.\u001b[0m\r\n\u001b[0;33m    Get:1 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-cli amd64 5:19.03.13~3-0~ubuntu-bionic [44.2 MB]\u001b[0m\r\n\u001b[0;33m    Fetched 44.2 MB in 42s (1046 kB/s)\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package pigz.\u001b[0m\r\n\u001b[0;33m    (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 61374 files and directories currently installed.)\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking pigz (2.4-1) ...\u001b[0m\r\n\u001b[0"]
[258.586165, "o", ";33m    Selecting previously unselected package aufs-tools.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../1-aufs-tools_1%3a4.9+20170918-1ubuntu1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking aufs-tools (1:4.9+20170918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package cgroupfs-mount.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../2-cgroupfs-mount_1.4_all.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package containerd.io.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../3-containerd.io_1.3.7-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce-cli.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../4-docker-ce-cli_5%3a19.03.13~3-0~ubuntu-bionic_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../5-docker-ce_5%3a19.03.13~3-0~ubuntu-bionic_amd64.de"]
[258.586209, "o", "b ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package libltdl7:amd64.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../6-libltdl7_2.4.6-2_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up aufs-tools (1:4.9+20170918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Setting up containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service -> /lib/systemd/system/containerd.service.\u001b[0m\r\n\u001b[0;33m    Setting up cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Setting up libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Setting up pigz (2.4-1) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/docker.service -> /lib/systemd/system/docker.service.\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/sock"]
[258.586298, "o", "ets.target.wants/docker.socket -> /lib/systemd/system/docker.socket.\u001b[0m\r\n\u001b[0;33m    Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for systemd (237-3ubuntu10.42) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for ureadahead (0.100.0-21) ...\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[278.667479, "o", "\u001b[0;33mchanged: [u1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  attempts: 1\u001b[0m\r\n\u001b[0;33m  cache_update_time: 1607172922\u001b[0m\r\n\u001b[0;33m  cache_updated: true\u001b[0m\r\n\u001b[0;33m  stderr: ''\u001b[0m\r\n\u001b[0;33m  stderr_lines: <omitted>\u001b[0m\r\n\u001b[0;33m  stdout: |-\u001b[0m\r\n\u001b[0;33m    Reading package lists...\u001b[0m\r\n\u001b[0;33m    Building dependency tree...\u001b[0m\r\n\u001b[0;33m    Reading state information...\u001b[0m\r\n\u001b[0;33m    The following package was automatically installed and is no longer required:\u001b[0m\r\n\u001b[0;33m      grub-pc-bin\u001b[0m\r\n\u001b[0;33m    Use 'sudo apt autoremove' to remove it.\u001b[0m\r\n\u001b[0;33m    The following additional packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount libltdl7 pigz\u001b[0m\r\n\u001b[0;33m    The following NEW packages will be installed:\u001b[0m\r\n\u001b[0;33m      aufs-tools cgroupfs-mount containerd.io docker-ce docker-ce-cli libltdl7\u001b[0m\r\n\u001b[0;33m      pigz\u001b[0m\r\n\u001b[0;33m    0 upgraded, 7 newly installed, 0 to remove and 48 not upgraded.\u001b[0m\r\n\u001b[0;33m    Need to get 91.3 MB of archives.\u001b[0m\r\n\u001b[0;33m    After this operation, 410 MB of add"]
[278.667523, "o", "itional disk space will be used.\u001b[0m\r\n\u001b[0;33m    Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pigz amd64 2.4-1 [57.4 kB]\u001b[0m\r\n\u001b[0;33m    Get:2 https://download.docker.com/linux/ubuntu bionic/stable amd64 containerd.io amd64 1.3.7-1 [24.4 MB]\u001b[0m\r\n\u001b[0;33m    Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 aufs-tools amd64 1:4.9+20170918-1ubuntu1 [104 kB]\u001b[0m\r\n\u001b[0;33m    Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 cgroupfs-mount all 1.4 [6320 B]\u001b[0m\r\n\u001b[0;33m    Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl7 amd64 2.4.6-2 [38.8 kB]\u001b[0m\r\n\u001b[0;33m    Get:6 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-cli amd64 5:19.03.13~3-0~ubuntu-bionic [44.2 MB]\u001b[0m\r\n\u001b[0;33m    Get:7 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce amd64 5:19.03.13~3-0~ubuntu-bionic [22.5 MB]\u001b[0m\r\n\u001b[0;33m    Fetched 91.3 MB in 59s (1557 kB/s)\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package pigz.\u001b[0m\r\n\u001b[0;33m    (Reading databa"]
[278.667534, "o", "se ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 61374 files and directories currently installed.)\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking pigz (2.4-1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package aufs-tools.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../1-aufs-tools_1%3a4.9+20170918-1ubuntu1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking aufs-tools (1:4.9+20170918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package cgroupfs-mount.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack"]
[278.667603, "o", " .../2-cgroupfs-mount_1.4_all.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package containerd.io.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../3-containerd.io_1.3.7-1_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce-cli.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../4-docker-ce-cli_5%3a19.03.13~3-0~ubuntu-bionic_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package docker-ce.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../5-docker-ce_5%3a19.03.13~3-0~ubuntu-bionic_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Selecting previously unselected package libltdl7:amd64.\u001b[0m\r\n\u001b[0;33m    Preparing to unpack .../6-libltdl7_2.4.6-2_amd64.deb ...\u001b[0m\r\n\u001b[0;33m    Unpacking libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up aufs-tools (1:4.9+20170"]
[278.667675, "o", "918-1ubuntu1) ...\u001b[0m\r\n\u001b[0;33m    Setting up containerd.io (1.3.7-1) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service -> /lib/systemd/system/containerd.service.\u001b[0m\r\n\u001b[0;33m    Setting up cgroupfs-mount (1.4) ...\u001b[0m\r\n\u001b[0;33m    Setting up libltdl7:amd64 (2.4.6-2) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce-cli (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Setting up pigz (2.4-1) ...\u001b[0m\r\n\u001b[0;33m    Setting up docker-ce (5:19.03.13~3-0~ubuntu-bionic) ...\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/multi-user.target.wants/docker.service -> /lib/systemd/system/docker.service.\u001b[0m\r\n\u001b[0;33m    Created symlink /etc/systemd/system/sockets.target.wants/docker.socket -> /lib/systemd/system/docker.socket.\u001b[0m\r\n\u001b[0;33m    Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for systemd (237-3ubuntu10.42) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\u001b[0m\r\n\u001b[0;33m    Processing triggers for ureada"]
[278.667722, "o", "head (0.100.0-21) ...\u001b[0m\r\n\u001b[0;33m  stdout_lines: <omitted>\u001b[0m\r\n"]
[278.673469, "o", "\r\nTASK [docker : user] **************************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/debian.yml:26\u001b[0m\r\n"]
[279.217066, "o", "\u001b[0;33mchanged: [u1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Ubuntu\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/ubuntu\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: ubuntu\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[279.506798, "o", "\u001b[0;33mchanged: [u1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Ubuntu\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/ubuntu\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: ubuntu\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[279.710373, "o", "\u001b[0;33mchanged: [u1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Ubuntu\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/ubuntu\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: ubuntu\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[279.741065, "o", "\u001b[0;33mchanged: [u1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Ubuntu\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/ubuntu\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: ubuntu\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[279.758753, "o", "\u001b[0;33mchanged: [u1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Ubuntu\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/ubuntu\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: ubuntu\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[279.773866, "o", "\u001b[0;33mchanged: [u1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  append: true\u001b[0m\r\n\u001b[0;33m  comment: Ubuntu\u001b[0m\r\n\u001b[0;33m  group: 1000\u001b[0m\r\n\u001b[0;33m  groups: docker\u001b[0m\r\n\u001b[0;33m  home: /home/ubuntu\u001b[0m\r\n\u001b[0;33m  move_home: false\u001b[0m\r\n\u001b[0;33m  name: ubuntu\u001b[0m\r\n\u001b[0;33m  shell: /bin/bash\u001b[0m\r\n\u001b[0;33m  state: present\u001b[0m\r\n\u001b[0;33m  uid: 1000\u001b[0m\r\n"]
[279.780663, "o", "\r\nTASK [docker : file] **************************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/main.yml:4\u001b[0m\r\n"]
[280.153583, "o", "\u001b[0;32mok: [u1b1] => changed=false \u001b[0m\r\n\u001b[0;32m  gid: 0\u001b[0m\r\n\u001b[0;32m  group: root\u001b[0m\r\n\u001b[0;32m  mode: '0755'\u001b[0m\r\n\u001b[0;32m  owner: root\u001b[0m\r\n\u001b[0;32m  path: /etc/docker/\u001b[0m\r\n\u001b[0;32m  size: 4096\u001b[0m\r\n\u001b[0;32m  state: directory\u001b[0m\r\n\u001b[0;32m  uid: 0\u001b[0m\r\n"]
[280.154998, "o", "\u001b[0;32mok: [u1a3] => changed=false \u001b[0m\r\n\u001b[0;32m  gid: 0\u001b[0m\r\n\u001b[0;32m  group: root\u001b[0m\r\n\u001b[0;32m  mode: '0755'\u001b[0m\r\n\u001b[0;32m  owner: root\u001b[0m\r\n\u001b[0;32m  path: /etc/docker/\u001b[0m\r\n\u001b[0;32m  size: 4096\u001b[0m\r\n\u001b[0;32m  state: directory\u001b[0m\r\n\u001b[0;32m  uid: 0\u001b[0m\r\n"]
[280.156414, "o", "\u001b[0;32mok: [u1b2] => changed=false \u001b[0m\r\n\u001b[0;32m  gid: 0\u001b[0m\r\n\u001b[0;32m  group: root\u001b[0m\r\n\u001b[0;32m  mode: '0755'\u001b[0m\r\n\u001b[0;32m  owner: root\u001b[0m\r\n\u001b[0;32m  path: /etc/docker/\u001b[0m\r\n\u001b[0;32m  size: 4096\u001b[0m\r\n\u001b[0;32m  state: directory\u001b[0m\r\n\u001b[0;32m  uid: 0\u001b[0m\r\n"]
[280.168869, "o", "\u001b[0;32mok: [u1a1] => changed=false \u001b[0m\r\n\u001b[0;32m  gid: 0\u001b[0m\r\n\u001b[0;32m  group: root\u001b[0m\r\n\u001b[0;32m  mode: '0755'\u001b[0m\r\n\u001b[0;32m  owner: root\u001b[0m\r\n\u001b[0;32m  path: /etc/docker/\u001b[0m\r\n\u001b[0;32m  size: 4096\u001b[0m\r\n\u001b[0;32m  state: directory\u001b[0m\r\n\u001b[0;32m  uid: 0\u001b[0m\r\n"]
[280.169734, "o", "\u001b[0;32mok: [u1a2] => changed=false \u001b[0m\r\n\u001b[0;32m  gid: 0\u001b[0m\r\n\u001b[0;32m  group: root\u001b[0m\r\n\u001b[0;32m  mode: '0755'\u001b[0m\r\n\u001b[0;32m  owner: root\u001b[0m\r\n\u001b[0;32m  path: /etc/docker/\u001b[0m\r\n\u001b[0;32m  size: 4096\u001b[0m\r\n\u001b[0;32m  state: directory\u001b[0m\r\n\u001b[0;32m  uid: 0\u001b[0m\r\n"]
[280.346755, "o", "\u001b[0;32mok: [u1b3] => changed=false \u001b[0m\r\n\u001b[0;32m  gid: 0\u001b[0m\r\n\u001b[0;32m  group: root\u001b[0m\r\n\u001b[0;32m  mode: '0755'\u001b[0m\r\n\u001b[0;32m  owner: root\u001b[0m\r\n\u001b[0;32m  path: /etc/docker/\u001b[0m\r\n\u001b[0;32m  size: 4096\u001b[0m\r\n\u001b[0;32m  state: directory\u001b[0m\r\n\u001b[0;32m  uid: 0\u001b[0m\r\n"]
[280.354117, "o", "\r\nTASK [docker : copy] **************************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/main.yml:8\u001b[0m\r\n"]
[281.082898, "o", "\u001b[0;33mchanged: [u1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/ubuntu/.ansible/tmp/ansible-tmp-1607172996.7931163-22217-170539735992268/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[281.084455, "o", "\u001b[0;33mchanged: [u1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/ubuntu/.ansible/tmp/ansible-tmp-1607172996.7725065-22215-89819840464096/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[281.085699, "o", "\u001b[0;33mchanged: [u1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/ubuntu/.ansible/tmp/ansible-tmp-1607172996.8167787-22222-167897737804228/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[281.13437, "o", "\u001b[0;33mchanged: [u1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/ubuntu/.ansible/tmp/ansible-tmp-1607172996.8042982-22219-47383961127459/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[281.135682, "o", "\u001b[0;33mchanged: [u1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/ubuntu/.ansible/tmp/ansible-tmp-1607172996.78266-22216-172945732061406/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[281.457285, "o", "\u001b[0;33mchanged: [u1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  checksum: 5899f09ade7799a1f283574094a948e635327510\u001b[0m\r\n\u001b[0;33m  dest: /etc/docker/daemon.json\u001b[0m\r\n\u001b[0;33m  gid: 0\u001b[0m\r\n\u001b[0;33m  group: root\u001b[0m\r\n\u001b[0;33m  md5sum: 645412fd87ff87db3194e194499464ad\u001b[0m\r\n\u001b[0;33m  mode: '0644'\u001b[0m\r\n\u001b[0;33m  owner: root\u001b[0m\r\n\u001b[0;33m  size: 181\u001b[0m\r\n\u001b[0;33m  src: /home/ubuntu/.ansible/tmp/ansible-tmp-1607172997.4994822-22291-276611718734277/source\u001b[0m\r\n\u001b[0;33m  state: file\u001b[0m\r\n\u001b[0;33m  uid: 0\u001b[0m\r\n"]
[281.463665, "o", "\r\nTASK [docker : systemd] ***********************************************************************************************************************************************************************\r\n\u001b[1;30mtask path: /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/roles/docker/tasks/main.yml:21\u001b[0m\r\n"]
[282.112237, "o", "\u001b[0;33mchanged: [u1a2] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestamp: Sat 2020-12-05 12:55:24 UTC\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '237276641'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: active\u001b[0m\r\n\u001b[0;33m    After: containerd.service network-online.target systemd-journald.socket sysinit.target system.slice docker.socket firewalld.service basic.target\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: ''\u001b[0m\r\n\u001b[0;33m    AssertResult: 'yes'\u001b[0m\r\n\u001b[0;33m    AssertTimestamp: Sat 2020-12-05 12:55:24 UTC\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '236817807'\u001b[0m\r\n\u001b[0;33m    Before: shutdown.target multi-user.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b["]
[282.11236, "o", "0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUUsageNSec: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CacheDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    CanIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend\u001b[0m\r\n\u001b[0;33m    CollectMode: inactive\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'yes'\u001b"]
[282.112409, "o", "[0m\r\n\u001b[0;33m    ConditionTimestamp: Sat 2020-12-05 12:55:24 UTC\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '236817806'\u001b[0m\r\n\u001b[0;33m    ConfigurationDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlGroup: /system.slice/docker.service\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    DelegateControllers: cpu cpuacct io blkio memory devices pids\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    DynamicUser: 'no'\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '3483'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestamp: Sat 2020-12-05 12:55:24 UTC\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '236818460'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ;"]
[282.112451, "o", " argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GID: '[not set]'\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IOSchedulingClass: '0'\u001b[0m\r\n\u001b[0;33m    IOSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    IOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    IPAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IPEgressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPEgressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;3"]
[282.112489, "o", "3m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestamp: Sat 2020-12-05 12:55:24 UTC\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '236818498'\u001b[0m\r\n\u001b[0;33m    InvocationID: 83eaf2f00a23467bbc00c09a5a55b772\u001b[0m\r\n\u001b[0;33m    JobRunningTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    KeyringMode: private\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: infinity\u001b[0m\r\n\u001b[0;33m    LimitASSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORE: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPU: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPUSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATA: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATASoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZE: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK: "]
[282.11253, "o", "'16777216'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCKSoft: '16777216'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUESoft: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNICESoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: infinity\u001b[0m\r\n\u001b[0;33m    LimitNOFILESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROC: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROCSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSS: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIOSoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTTIMESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '7869'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDINGSoft: '7869'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: infinity\u001b[0m\r\n\u001b[0;33m    LimitSTACKSoft: '8388608'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    LockPersonality: 'no'\u001b[0m\r\n\u001b[0;33m    LogLevelMax: '-1'\u001b[0m\r\n\u001b[0;33m    LogsDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    MainPID: '3483'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '[not set]'\u001b[0m\r\n\u001b[0;33"]
[282.112566, "o", "m    MemoryDenyWriteExecute: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryHigh: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLimit: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLow: '0'\u001b[0m\r\n\u001b[0;33m    MemoryMax: infinity\u001b[0m\r\n\u001b[0;33m    MemorySwapMax: infinity\u001b[0m\r\n\u001b[0;33m    MountAPIVFS: 'no'\u001b[0m\r\n\u001b[0;33m    MountFlags: ''\u001b[0m\r\n\u001b[0;33m    NFileDescriptorStore: '0'\u001b[0m\r\n\u001b[0;33m    NRestarts: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailureJobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    Perpetual: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateUsers: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectControlGroups: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelModules: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelTunables: 'no'\u001b["]
[282.112638, "o", "0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    RemoveIPC: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: sysinit.target docker.socket system.slice\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    RestrictNamespaces: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictRealtime: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictSUIDSGID: 'no'\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryPreserve: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeMaxUSec: infinity\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardInputData: ''\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0;3"]
[282.11271, "o", "3m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitIntervalUSec: 1min\u001b[0m\r\n\u001b[0;33m    StartupBlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StateChangeTimestamp: Sat 2020-12-05 12:55:24 UTC\u001b[0m\r\n\u001b[0;33m    StateChangeTimestampMonotonic: '237276641'\u001b[0m\r\n\u001b[0;33m    StateDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: running\u001b[0m\r\n\u001b[0;33m    SuccessAction: none\u001b[0m\r\n\u001b[0;33m    SyslogFacility: '3'\u001b[0m\r\n\u001b[0;33m    SyslogLevel: '6'\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'yes'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '9'\u001b[0m\r\n\u001b[0;33m    TasksMax: infinity\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: infinity\u001b[0m\r\n\u001b[0;33"]
[282.112734, "o", "m    TimeoutStopUSec: infinity\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UID: '[not set]'\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: enabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: enabled\u001b[0m\r\n\u001b[0;33m    UtmpMode: init\u001b[0m\r\n\u001b[0;33m    WantedBy: multi-user.target\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestamp: Sat 2020-12-05 12:55:24 UTC\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '237276637'\u001b[0m\r\n\u001b[0;33m    WatchdogUSec: '0'\u001b[0m\r\n"]
[282.130066, "o", "\u001b[0;33mchanged: [u1a3] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestamp: Sat 2020-12-05 12:56:11 UTC\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '284541546'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: active\u001b[0m\r\n\u001b[0;33m    After: containerd.service sysinit.target system.slice basic.target network-online.target docker.socket systemd-journald.socket firewalld.service\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: ''\u001b[0m\r\n\u001b[0;33m    AssertResult: 'yes'\u001b[0m\r\n\u001b[0;33m    AssertTimestamp: Sat 2020-12-05 12:56:11 UTC\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '284085754'\u001b[0m\r\n\u001b[0;33m    Before: multi-user.target shutdown.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b["]
[282.130111, "o", "0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUUsageNSec: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CacheDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    CanIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend\u001b[0m\r\n\u001b[0;33m    CollectMode: inactive\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'yes'\u001b"]
[282.130125, "o", "[0m\r\n\u001b[0;33m    ConditionTimestamp: Sat 2020-12-05 12:56:11 UTC\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '284085754'\u001b[0m\r\n\u001b[0;33m    ConfigurationDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlGroup: /system.slice/docker.service\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    DelegateControllers: cpu cpuacct io blkio memory devices pids\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    DynamicUser: 'no'\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '4008'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestamp: Sat 2020-12-05 12:56:11 UTC\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '284086310'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ;"]
[282.130233, "o", " argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GID: '[not set]'\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IOSchedulingClass: '0'\u001b[0m\r\n\u001b[0;33m    IOSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    IOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    IPAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IPEgressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPEgressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;3"]
[282.130313, "o", "3m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestamp: Sat 2020-12-05 12:56:11 UTC\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '284086342'\u001b[0m\r\n\u001b[0;33m    InvocationID: a2e4200e27d145d39a3fa1f54d8aee95\u001b[0m\r\n\u001b[0;33m    JobRunningTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    KeyringMode: private\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: infinity\u001b[0m\r\n\u001b[0;33m    LimitASSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORE: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPU: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPUSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATA: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATASoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZE: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK: "]
[282.130333, "o", "'16777216'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCKSoft: '16777216'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUESoft: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNICESoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: infinity\u001b[0m\r\n\u001b[0;33m    LimitNOFILESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROC: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROCSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSS: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIOSoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTTIMESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '7869'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDINGSoft: '7869'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: infinity\u001b[0m\r\n\u001b[0;33m    LimitSTACKSoft: '8388608'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    LockPersonality: 'no'\u001b[0m\r\n\u001b[0;33m    LogLevelMax: '-1'\u001b[0m\r\n\u001b[0;33m    LogsDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    MainPID: '4008'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '[not set]'\u001b[0m\r\n\u001b[0;33"]
[282.13043, "o", "m    MemoryDenyWriteExecute: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryHigh: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLimit: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLow: '0'\u001b[0m\r\n\u001b[0;33m    MemoryMax: infinity\u001b[0m\r\n\u001b[0;33m    MemorySwapMax: infinity\u001b[0m\r\n\u001b[0;33m    MountAPIVFS: 'no'\u001b[0m\r\n\u001b[0;33m    MountFlags: ''\u001b[0m\r\n\u001b[0;33m    NFileDescriptorStore: '0'\u001b[0m\r\n\u001b[0;33m    NRestarts: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailureJobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    Perpetual: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateUsers: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectControlGroups: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelModules: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelTunables: 'no'\u001b["]
[282.130484, "o", "0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    RemoveIPC: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: docker.socket system.slice sysinit.target\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    RestrictNamespaces: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictRealtime: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictSUIDSGID: 'no'\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryPreserve: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeMaxUSec: infinity\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardInputData: ''\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0;3"]
[282.130525, "o", "3m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitIntervalUSec: 1min\u001b[0m\r\n\u001b[0;33m    StartupBlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StateChangeTimestamp: Sat 2020-12-05 12:56:11 UTC\u001b[0m\r\n\u001b[0;33m    StateChangeTimestampMonotonic: '284541546'\u001b[0m\r\n\u001b[0;33m    StateDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: running\u001b[0m\r\n\u001b[0;33m    SuccessAction: none\u001b[0m\r\n\u001b[0;33m    SyslogFacility: '3'\u001b[0m\r\n\u001b[0;33m    SyslogLevel: '6'\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'yes'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '10'\u001b[0m\r\n\u001b[0;33m    TasksMax: infinity\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: infinity\u001b[0m\r\n\u001b[0;3"]
[282.130564, "o", "3m    TimeoutStopUSec: infinity\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UID: '[not set]'\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: enabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: enabled\u001b[0m\r\n\u001b[0;33m    UtmpMode: init\u001b[0m\r\n\u001b[0;33m    WantedBy: multi-user.target\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestamp: Sat 2020-12-05 12:56:11 UTC\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '284541543'\u001b[0m\r\n\u001b[0;33m    WatchdogUSec: '0'\u001b[0m\r\n"]
[282.147496, "o", "\u001b[0;33mchanged: [u1a1] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestamp: Sat 2020-12-05 12:55:20 UTC\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '233149301'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: active\u001b[0m\r\n\u001b[0;33m    After: basic.target sysinit.target system.slice firewalld.service network-online.target containerd.service docker.socket systemd-journald.socket\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: ''\u001b[0m\r\n\u001b[0;33m    AssertResult: 'yes'\u001b[0m\r\n\u001b[0;33m    AssertTimestamp: Sat 2020-12-05 12:55:19 UTC\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '232405115'\u001b[0m\r\n\u001b[0;33m    Before: multi-user.target shutdown.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b["]
[282.147624, "o", "0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUUsageNSec: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CacheDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    CanIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend\u001b[0m\r\n\u001b[0;33m    CollectMode: inactive\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'yes'\u001b"]
[282.147638, "o", "[0m\r\n\u001b[0;33m    ConditionTimestamp: Sat 2020-12-05 12:55:19 UTC\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '232405114'\u001b[0m\r\n\u001b[0;33m    ConfigurationDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlGroup: /system.slice/docker.service\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    DelegateControllers: cpu cpuacct io blkio memory devices pids\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    DynamicUser: 'no'\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '3472'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestamp: Sat 2020-12-05 12:55:19 UTC\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '232405728'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ;"]
[282.147648, "o", " argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GID: '[not set]'\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IOSchedulingClass: '0'\u001b[0m\r\n\u001b[0;33m    IOSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    IOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    IPAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IPEgressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPEgressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;3"]
[282.147655, "o", "3m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestamp: Sat 2020-12-05 12:55:19 UTC\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '232405756'\u001b[0m\r\n\u001b[0;33m    InvocationID: 6b9515c661914156b1686655ed09e501\u001b[0m\r\n\u001b[0;33m    JobRunningTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    KeyringMode: private\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: infinity\u001b[0m\r\n\u001b[0;33m    LimitASSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORE: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPU: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPUSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATA: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATASoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZE: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK:"]
[282.147746, "o", " '16777216'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCKSoft: '16777216'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUESoft: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNICESoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: infinity\u001b[0m\r\n\u001b[0;33m    LimitNOFILESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROC: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROCSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSS: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIOSoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTTIMESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '7869'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDINGSoft: '7869'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: infinity\u001b[0m\r\n\u001b[0;33m    LimitSTACKSoft: '8388608'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    LockPersonality: 'no'\u001b[0m\r\n\u001b[0;33m    LogLevelMax: '-1'\u001b[0m\r\n\u001b[0;33m    LogsDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    MainPID: '3472'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '[not set]'\u001b[0m\r\n\u001b[0;3"]
[282.147788, "o", "3m    MemoryDenyWriteExecute: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryHigh: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLimit: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLow: '0'\u001b[0m\r\n\u001b[0;33m    MemoryMax: infinity\u001b[0m\r\n\u001b[0;33m    MemorySwapMax: infinity\u001b[0m\r\n\u001b[0;33m    MountAPIVFS: 'no'\u001b[0m\r\n\u001b[0;33m    MountFlags: ''\u001b[0m\r\n\u001b[0;33m    NFileDescriptorStore: '0'\u001b[0m\r\n\u001b[0;33m    NRestarts: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailureJobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    Perpetual: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateUsers: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectControlGroups: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelModules: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelTunables: 'no'\u001b"]
[282.147836, "o", "[0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    RemoveIPC: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: docker.socket system.slice sysinit.target\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    RestrictNamespaces: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictRealtime: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictSUIDSGID: 'no'\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryPreserve: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeMaxUSec: infinity\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardInputData: ''\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0;"]
[282.147874, "o", "33m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitIntervalUSec: 1min\u001b[0m\r\n\u001b[0;33m    StartupBlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StateChangeTimestamp: Sat 2020-12-05 12:55:20 UTC\u001b[0m\r\n\u001b[0;33m    StateChangeTimestampMonotonic: '233149301'\u001b[0m\r\n\u001b[0;33m    StateDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: running\u001b[0m\r\n\u001b[0;33m    SuccessAction: none\u001b[0m\r\n\u001b[0;33m    SyslogFacility: '3'\u001b[0m\r\n\u001b[0;33m    SyslogLevel: '6'\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'yes'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '10'\u001b[0m\r\n\u001b[0;33m    TasksMax: infinity\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: infinity\u001b[0m\r\n\u001b[0;"]
[282.14791, "o", "33m    TimeoutStopUSec: infinity\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UID: '[not set]'\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: enabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: enabled\u001b[0m\r\n\u001b[0;33m    UtmpMode: init\u001b[0m\r\n\u001b[0;33m    WantedBy: multi-user.target\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestamp: Sat 2020-12-05 12:55:20 UTC\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '233149298'\u001b[0m\r\n\u001b[0;33m    WatchdogUSec: '0'\u001b[0m\r\n"]
[282.162056, "o", "\u001b[0;33mchanged: [u1b1] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestamp: Sat 2020-12-05 12:55:25 UTC\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '237124972'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: active\u001b[0m\r\n\u001b[0;33m    After: network-online.target containerd.service system.slice systemd-journald.socket firewalld.service basic.target docker.socket sysinit.target\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: ''\u001b[0m\r\n\u001b[0;33m    AssertResult: 'yes'\u001b[0m\r\n\u001b[0;33m    AssertTimestamp: Sat 2020-12-05 12:55:25 UTC\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '236688614'\u001b[0m\r\n\u001b[0;33m    Before: shutdown.target multi-user.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b["]
[282.16219, "o", "0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUUsageNSec: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CacheDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    CanIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend\u001b[0m\r\n\u001b[0;33m    CollectMode: inactive\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'yes'\u001b"]
[282.162213, "o", "[0m\r\n\u001b[0;33m    ConditionTimestamp: Sat 2020-12-05 12:55:25 UTC\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '236688613'\u001b[0m\r\n\u001b[0;33m    ConfigurationDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlGroup: /system.slice/docker.service\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    DelegateControllers: cpu cpuacct io blkio memory devices pids\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    DynamicUser: 'no'\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '3457'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestamp: Sat 2020-12-05 12:55:25 UTC\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '236689500'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ;"]
[282.162223, "o", " argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GID: '[not set]'\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IOSchedulingClass: '0'\u001b[0m\r\n\u001b[0;33m    IOSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    IOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    IPAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IPEgressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPEgressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;3"]
[282.162232, "o", "3m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestamp: Sat 2020-12-05 12:55:25 UTC\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '236689532'\u001b[0m\r\n\u001b[0;33m    InvocationID: 2bf072c9defa4823b9294e8d9625349f\u001b[0m\r\n\u001b[0;33m    JobRunningTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    KeyringMode: private\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: infinity\u001b[0m\r\n\u001b[0;33m    LimitASSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORE: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPU: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPUSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATA: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATASoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZE: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK: "]
[282.162312, "o", "'16777216'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCKSoft: '16777216'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUESoft: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNICESoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: infinity\u001b[0m\r\n\u001b[0;33m    LimitNOFILESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROC: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROCSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSS: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIOSoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTTIMESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '11901'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDINGSoft: '11901'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: infinity\u001b[0m\r\n\u001b[0;33m    LimitSTACKSoft: '8388608'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    LockPersonality: 'no'\u001b[0m\r\n\u001b[0;33m    LogLevelMax: '-1'\u001b[0m\r\n\u001b[0;33m    LogsDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    MainPID: '3457'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '[not set]'\u001b[0m\r\n\u001b[0;"]
[282.162374, "o", "33m    MemoryDenyWriteExecute: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryHigh: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLimit: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLow: '0'\u001b[0m\r\n\u001b[0;33m    MemoryMax: infinity\u001b[0m\r\n\u001b[0;33m    MemorySwapMax: infinity\u001b[0m\r\n\u001b[0;33m    MountAPIVFS: 'no'\u001b[0m\r\n\u001b[0;33m    MountFlags: ''\u001b[0m\r\n\u001b[0;33m    NFileDescriptorStore: '0'\u001b[0m\r\n\u001b[0;33m    NRestarts: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailureJobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    Perpetual: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateUsers: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectControlGroups: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelModules: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelTunables: 'no'"]
[282.162425, "o", "\u001b[0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    RemoveIPC: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: system.slice docker.socket sysinit.target\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    RestrictNamespaces: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictRealtime: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictSUIDSGID: 'no'\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryPreserve: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeMaxUSec: infinity\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardInputData: ''\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0"]
[282.162465, "o", ";33m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitIntervalUSec: 1min\u001b[0m\r\n\u001b[0;33m    StartupBlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StateChangeTimestamp: Sat 2020-12-05 12:55:25 UTC\u001b[0m\r\n\u001b[0;33m    StateChangeTimestampMonotonic: '237124972'\u001b[0m\r\n\u001b[0;33m    StateDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: running\u001b[0m\r\n\u001b[0;33m    SuccessAction: none\u001b[0m\r\n\u001b[0;33m    SyslogFacility: '3'\u001b[0m\r\n\u001b[0;33m    SyslogLevel: '6'\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'yes'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '10'\u001b[0m\r\n\u001b[0;33m    TasksMax: infinity\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: infinity\u001b[0m\r\n\u001b[0"]
[282.162503, "o", ";33m    TimeoutStopUSec: infinity\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UID: '[not set]'\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: enabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: enabled\u001b[0m\r\n\u001b[0;33m    UtmpMode: init\u001b[0m\r\n\u001b[0;33m    WantedBy: multi-user.target\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestamp: Sat 2020-12-05 12:55:25 UTC\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '237124971'\u001b[0m\r\n\u001b[0;33m    WatchdogUSec: '0'\u001b[0m\r\n"]
[282.177092, "o", "\u001b[0;33mchanged: [u1b2] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestamp: Sat 2020-12-05 12:55:16 UTC\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '227816266'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: active\u001b[0m\r\n\u001b[0;33m    After: network-online.target system.slice docker.socket systemd-journald.socket containerd.service basic.target sysinit.target firewalld.service\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: ''\u001b[0m\r\n\u001b[0;33m    AssertResult: 'yes'\u001b[0m\r\n\u001b[0;33m    AssertTimestamp: Sat 2020-12-05 12:55:15 UTC\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '227273038'\u001b[0m\r\n\u001b[0;33m    Before: multi-user.target shutdown.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b["]
[282.177135, "o", "0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUUsageNSec: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CacheDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    CanIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend\u001b[0m\r\n\u001b[0;33m    CollectMode: inactive\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'yes'\u001b"]
[282.177145, "o", "[0m\r\n\u001b[0;33m    ConditionTimestamp: Sat 2020-12-05 12:55:15 UTC\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '227273037'\u001b[0m\r\n\u001b[0;33m    ConfigurationDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlGroup: /system.slice/docker.service\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    DelegateControllers: cpu cpuacct io blkio memory devices pids\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    DynamicUser: 'no'\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '3497'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestamp: Sat 2020-12-05 12:55:15 UTC\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '227273734'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ;"]
[282.177153, "o", " argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GID: '[not set]'\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IOSchedulingClass: '0'\u001b[0m\r\n\u001b[0;33m    IOSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    IOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    IPAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IPEgressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPEgressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;3"]
[282.177252, "o", "3m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestamp: Sat 2020-12-05 12:55:15 UTC\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '227273769'\u001b[0m\r\n\u001b[0;33m    InvocationID: 6c39b268c2ef43eb94768754bf2332eb\u001b[0m\r\n\u001b[0;33m    JobRunningTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    KeyringMode: private\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: infinity\u001b[0m\r\n\u001b[0;33m    LimitASSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORE: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPU: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPUSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATA: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATASoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZE: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK: "]
[282.177264, "o", "'16777216'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCKSoft: '16777216'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUESoft: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNICESoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: infinity\u001b[0m\r\n\u001b[0;33m    LimitNOFILESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROC: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROCSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSS: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIOSoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTTIMESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '11901'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDINGSoft: '11901'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: infinity\u001b[0m\r\n\u001b[0;33m    LimitSTACKSoft: '8388608'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    LockPersonality: 'no'\u001b[0m\r\n\u001b[0;33m    LogLevelMax: '-1'\u001b[0m\r\n\u001b[0;33m    LogsDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    MainPID: '3497'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '[not set]'\u001b[0m\r\n\u001b[0;"]
[282.177271, "o", "33m    MemoryDenyWriteExecute: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryHigh: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLimit: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLow: '0'\u001b[0m\r\n\u001b[0;33m    MemoryMax: infinity\u001b[0m\r\n\u001b[0;33m    MemorySwapMax: infinity\u001b[0m\r\n\u001b[0;33m    MountAPIVFS: 'no'\u001b[0m\r\n\u001b[0;33m    MountFlags: ''\u001b[0m\r\n\u001b[0;33m    NFileDescriptorStore: '0'\u001b[0m\r\n\u001b[0;33m    NRestarts: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailureJobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    Perpetual: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateUsers: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectControlGroups: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelModules: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelTunables: 'no'"]
[282.177277, "o", "\u001b[0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    RemoveIPC: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: docker.socket sysinit.target system.slice\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    RestrictNamespaces: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictRealtime: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictSUIDSGID: 'no'\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryPreserve: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeMaxUSec: infinity\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardInputData: ''\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0"]
[282.177286, "o", ";33m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitIntervalUSec: 1min\u001b[0m\r\n\u001b[0;33m    StartupBlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StateChangeTimestamp: Sat 2020-12-05 12:55:16 UTC\u001b[0m\r\n\u001b[0;33m    StateChangeTimestampMonotonic: '227816266'\u001b[0m\r\n\u001b[0;33m    StateDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: running\u001b[0m\r\n\u001b[0;33m    SuccessAction: none\u001b[0m\r\n\u001b[0;33m    SyslogFacility: '3'\u001b[0m\r\n\u001b[0;33m    SyslogLevel: '6'\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'yes'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '9'\u001b[0m\r\n\u001b[0;33m    TasksMax: infinity\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: infinity\u001b[0m\r\n\u001b[0;"]
[282.177294, "o", "33m    TimeoutStopUSec: infinity\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UID: '[not set]'\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: enabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: enabled\u001b[0m\r\n\u001b[0;33m    UtmpMode: init\u001b[0m\r\n\u001b[0;33m    WantedBy: multi-user.target\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestamp: Sat 2020-12-05 12:55:16 UTC\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '227816263'\u001b[0m\r\n\u001b[0;33m    WatchdogUSec: '0'\u001b[0m\r\n"]
[282.412749, "o", "\u001b[0;33mchanged: [u1b3] => changed=true \u001b[0m\r\n\u001b[0;33m  name: docker\u001b[0m\r\n\u001b[0;33m  state: started\u001b[0m\r\n\u001b[0;33m  status:\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestamp: Sat 2020-12-05 12:56:32 UTC\u001b[0m\r\n\u001b[0;33m    ActiveEnterTimestampMonotonic: '303636705'\u001b[0m\r\n\u001b[0;33m    ActiveExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ActiveState: active\u001b[0m\r\n\u001b[0;33m    After: network-online.target system.slice basic.target containerd.service sysinit.target docker.socket systemd-journald.socket firewalld.service\u001b[0m\r\n\u001b[0;33m    AllowIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    AmbientCapabilities: ''\u001b[0m\r\n\u001b[0;33m    AssertResult: 'yes'\u001b[0m\r\n\u001b[0;33m    AssertTimestamp: Sat 2020-12-05 12:56:31 UTC\u001b[0m\r\n\u001b[0;33m    AssertTimestampMonotonic: '303139593'\u001b[0m\r\n\u001b[0;33m    Before: multi-user.target shutdown.target\u001b[0m\r\n\u001b[0;33m    BindsTo: containerd.service\u001b[0m\r\n\u001b[0;33m    BlockIOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    BlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    CPUQuotaPerSecUSec: infinity\u001b[0m\r\n\u001b[0;33m    CPUSchedulingPolicy: '0'\u001b["]
[282.413012, "o", "0m\r\n\u001b[0;33m    CPUSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    CPUSchedulingResetOnFork: 'no'\u001b[0m\r\n\u001b[0;33m    CPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUUsageNSec: '[not set]'\u001b[0m\r\n\u001b[0;33m    CPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    CacheDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    CanIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    CanReload: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStart: 'yes'\u001b[0m\r\n\u001b[0;33m    CanStop: 'yes'\u001b[0m\r\n\u001b[0;33m    CapabilityBoundingSet: cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend\u001b[0m\r\n\u001b[0;33m    CollectMode: inactive\u001b[0m\r\n\u001b[0;33m    ConditionResult: 'yes'\u001b"]
[282.413191, "o", "[0m\r\n\u001b[0;33m    ConditionTimestamp: Sat 2020-12-05 12:56:31 UTC\u001b[0m\r\n\u001b[0;33m    ConditionTimestampMonotonic: '303139593'\u001b[0m\r\n\u001b[0;33m    ConfigurationDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    Conflicts: shutdown.target\u001b[0m\r\n\u001b[0;33m    ConsistsOf: docker.socket\u001b[0m\r\n\u001b[0;33m    ControlGroup: /system.slice/docker.service\u001b[0m\r\n\u001b[0;33m    ControlPID: '0'\u001b[0m\r\n\u001b[0;33m    DefaultDependencies: 'yes'\u001b[0m\r\n\u001b[0;33m    Delegate: 'yes'\u001b[0m\r\n\u001b[0;33m    DelegateControllers: cpu cpuacct io blkio memory devices pids\u001b[0m\r\n\u001b[0;33m    Description: Docker Application Container Engine\u001b[0m\r\n\u001b[0;33m    DevicePolicy: auto\u001b[0m\r\n\u001b[0;33m    Documentation: https://docs.docker.com\u001b[0m\r\n\u001b[0;33m    DynamicUser: 'no'\u001b[0m\r\n\u001b[0;33m    ExecMainCode: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainExitTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    ExecMainPID: '3600'\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestamp: Sat 2020-12-05 12:56:31 UTC\u001b[0m\r\n\u001b[0;33m    ExecMainStartTimestampMonotonic: '303140058'\u001b[0m\r\n\u001b[0;33m    ExecMainStatus: '0'\u001b[0m\r\n\u001b[0;33m    ExecReload: '{ path=/bin/kill ;"]
[282.413328, "o", " argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    ExecStart: '{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }'\u001b[0m\r\n\u001b[0;33m    FailureAction: none\u001b[0m\r\n\u001b[0;33m    FileDescriptorStoreMax: '0'\u001b[0m\r\n\u001b[0;33m    FragmentPath: /lib/systemd/system/docker.service\u001b[0m\r\n\u001b[0;33m    GID: '[not set]'\u001b[0m\r\n\u001b[0;33m    GuessMainPID: 'yes'\u001b[0m\r\n\u001b[0;33m    IOAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IOSchedulingClass: '0'\u001b[0m\r\n\u001b[0;33m    IOSchedulingPriority: '0'\u001b[0m\r\n\u001b[0;33m    IOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    IPAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    IPEgressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPEgressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressBytes: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    IPIngressPackets: '18446744073709551615'\u001b[0m\r\n\u001b[0;33m    Id: docker.service\u001b[0m\r\n\u001b[0;3"]
[282.41345, "o", "3m    IgnoreOnIsolate: 'no'\u001b[0m\r\n\u001b[0;33m    IgnoreSIGPIPE: 'yes'\u001b[0m\r\n\u001b[0;33m    InactiveEnterTimestampMonotonic: '0'\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestamp: Sat 2020-12-05 12:56:31 UTC\u001b[0m\r\n\u001b[0;33m    InactiveExitTimestampMonotonic: '303140083'\u001b[0m\r\n\u001b[0;33m    InvocationID: bfcf7b5323fd4c01b05decc533103bf9\u001b[0m\r\n\u001b[0;33m    JobRunningTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    JobTimeoutAction: none\u001b[0m\r\n\u001b[0;33m    JobTimeoutUSec: infinity\u001b[0m\r\n\u001b[0;33m    KeyringMode: private\u001b[0m\r\n\u001b[0;33m    KillMode: process\u001b[0m\r\n\u001b[0;33m    KillSignal: '15'\u001b[0m\r\n\u001b[0;33m    LimitAS: infinity\u001b[0m\r\n\u001b[0;33m    LimitASSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORE: infinity\u001b[0m\r\n\u001b[0;33m    LimitCORESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPU: infinity\u001b[0m\r\n\u001b[0;33m    LimitCPUSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATA: infinity\u001b[0m\r\n\u001b[0;33m    LimitDATASoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZE: infinity\u001b[0m\r\n\u001b[0;33m    LimitFSIZESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKS: infinity\u001b[0m\r\n\u001b[0;33m    LimitLOCKSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCK: "]
[282.413489, "o", "'16777216'\u001b[0m\r\n\u001b[0;33m    LimitMEMLOCKSoft: '16777216'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUE: '819200'\u001b[0m\r\n\u001b[0;33m    LimitMSGQUEUESoft: '819200'\u001b[0m\r\n\u001b[0;33m    LimitNICE: '0'\u001b[0m\r\n\u001b[0;33m    LimitNICESoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitNOFILE: infinity\u001b[0m\r\n\u001b[0;33m    LimitNOFILESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROC: infinity\u001b[0m\r\n\u001b[0;33m    LimitNPROCSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSS: infinity\u001b[0m\r\n\u001b[0;33m    LimitRSSSoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTPRIO: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTPRIOSoft: '0'\u001b[0m\r\n\u001b[0;33m    LimitRTTIME: infinity\u001b[0m\r\n\u001b[0;33m    LimitRTTIMESoft: infinity\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDING: '11901'\u001b[0m\r\n\u001b[0;33m    LimitSIGPENDINGSoft: '11901'\u001b[0m\r\n\u001b[0;33m    LimitSTACK: infinity\u001b[0m\r\n\u001b[0;33m    LimitSTACKSoft: '8388608'\u001b[0m\r\n\u001b[0;33m    LoadState: loaded\u001b[0m\r\n\u001b[0;33m    LockPersonality: 'no'\u001b[0m\r\n\u001b[0;33m    LogLevelMax: '-1'\u001b[0m\r\n\u001b[0;33m    LogsDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    MainPID: '3600'\u001b[0m\r\n\u001b[0;33m    MemoryAccounting: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryCurrent: '[not set]'\u001b[0m\r\n\u001b[0;"]
[282.413544, "o", "33m    MemoryDenyWriteExecute: 'no'\u001b[0m\r\n\u001b[0;33m    MemoryHigh: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLimit: infinity\u001b[0m\r\n\u001b[0;33m    MemoryLow: '0'\u001b[0m\r\n\u001b[0;33m    MemoryMax: infinity\u001b[0m\r\n\u001b[0;33m    MemorySwapMax: infinity\u001b[0m\r\n\u001b[0;33m    MountAPIVFS: 'no'\u001b[0m\r\n\u001b[0;33m    MountFlags: ''\u001b[0m\r\n\u001b[0;33m    NFileDescriptorStore: '0'\u001b[0m\r\n\u001b[0;33m    NRestarts: '0'\u001b[0m\r\n\u001b[0;33m    Names: docker.service\u001b[0m\r\n\u001b[0;33m    NeedDaemonReload: 'no'\u001b[0m\r\n\u001b[0;33m    Nice: '0'\u001b[0m\r\n\u001b[0;33m    NoNewPrivileges: 'no'\u001b[0m\r\n\u001b[0;33m    NonBlocking: 'no'\u001b[0m\r\n\u001b[0;33m    NotifyAccess: main\u001b[0m\r\n\u001b[0;33m    OOMScoreAdjust: '0'\u001b[0m\r\n\u001b[0;33m    OnFailureJobMode: replace\u001b[0m\r\n\u001b[0;33m    PermissionsStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    Perpetual: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateDevices: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateNetwork: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateTmp: 'no'\u001b[0m\r\n\u001b[0;33m    PrivateUsers: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectControlGroups: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectHome: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelModules: 'no'\u001b[0m\r\n\u001b[0;33m    ProtectKernelTunables: 'no'"]
[282.413578, "o", "\u001b[0m\r\n\u001b[0;33m    ProtectSystem: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStart: 'no'\u001b[0m\r\n\u001b[0;33m    RefuseManualStop: 'no'\u001b[0m\r\n\u001b[0;33m    RemainAfterExit: 'no'\u001b[0m\r\n\u001b[0;33m    RemoveIPC: 'no'\u001b[0m\r\n\u001b[0;33m    Requires: sysinit.target system.slice docker.socket\u001b[0m\r\n\u001b[0;33m    Restart: always\u001b[0m\r\n\u001b[0;33m    RestartUSec: 2s\u001b[0m\r\n\u001b[0;33m    RestrictNamespaces: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictRealtime: 'no'\u001b[0m\r\n\u001b[0;33m    RestrictSUIDSGID: 'no'\u001b[0m\r\n\u001b[0;33m    Result: success\u001b[0m\r\n\u001b[0;33m    RootDirectoryStartOnly: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    RuntimeDirectoryPreserve: 'no'\u001b[0m\r\n\u001b[0;33m    RuntimeMaxUSec: infinity\u001b[0m\r\n\u001b[0;33m    SameProcessGroup: 'no'\u001b[0m\r\n\u001b[0;33m    SecureBits: '0'\u001b[0m\r\n\u001b[0;33m    SendSIGHUP: 'no'\u001b[0m\r\n\u001b[0;33m    SendSIGKILL: 'yes'\u001b[0m\r\n\u001b[0;33m    Slice: system.slice\u001b[0m\r\n\u001b[0;33m    StandardError: inherit\u001b[0m\r\n\u001b[0;33m    StandardInput: 'null'\u001b[0m\r\n\u001b[0;33m    StandardInputData: ''\u001b[0m\r\n\u001b[0;33m    StandardOutput: journal\u001b[0m\r\n\u001b[0;33m    StartLimitAction: none\u001b[0m\r\n\u001b[0"]
[282.413603, "o", ";33m    StartLimitBurst: '3'\u001b[0m\r\n\u001b[0;33m    StartLimitIntervalUSec: 1min\u001b[0m\r\n\u001b[0;33m    StartupBlockIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUShares: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupCPUWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StartupIOWeight: '[not set]'\u001b[0m\r\n\u001b[0;33m    StateChangeTimestamp: Sat 2020-12-05 12:56:32 UTC\u001b[0m\r\n\u001b[0;33m    StateChangeTimestampMonotonic: '303636705'\u001b[0m\r\n\u001b[0;33m    StateDirectoryMode: '0755'\u001b[0m\r\n\u001b[0;33m    StatusErrno: '0'\u001b[0m\r\n\u001b[0;33m    StopWhenUnneeded: 'no'\u001b[0m\r\n\u001b[0;33m    SubState: running\u001b[0m\r\n\u001b[0;33m    SuccessAction: none\u001b[0m\r\n\u001b[0;33m    SyslogFacility: '3'\u001b[0m\r\n\u001b[0;33m    SyslogLevel: '6'\u001b[0m\r\n\u001b[0;33m    SyslogLevelPrefix: 'yes'\u001b[0m\r\n\u001b[0;33m    SyslogPriority: '30'\u001b[0m\r\n\u001b[0;33m    SystemCallErrorNumber: '0'\u001b[0m\r\n\u001b[0;33m    TTYReset: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVHangup: 'no'\u001b[0m\r\n\u001b[0;33m    TTYVTDisallocate: 'no'\u001b[0m\r\n\u001b[0;33m    TasksAccounting: 'yes'\u001b[0m\r\n\u001b[0;33m    TasksCurrent: '10'\u001b[0m\r\n\u001b[0;33m    TasksMax: infinity\u001b[0m\r\n\u001b[0;33m    TimeoutStartUSec: infinity\u001b[0m\r\n\u001b[0"]
[282.413611, "o", ";33m    TimeoutStopUSec: infinity\u001b[0m\r\n\u001b[0;33m    TimerSlackNSec: '50000'\u001b[0m\r\n\u001b[0;33m    Transient: 'no'\u001b[0m\r\n\u001b[0;33m    TriggeredBy: docker.socket\u001b[0m\r\n\u001b[0;33m    Type: notify\u001b[0m\r\n\u001b[0;33m    UID: '[not set]'\u001b[0m\r\n\u001b[0;33m    UMask: '0022'\u001b[0m\r\n\u001b[0;33m    UnitFilePreset: enabled\u001b[0m\r\n\u001b[0;33m    UnitFileState: enabled\u001b[0m\r\n\u001b[0;33m    UtmpMode: init\u001b[0m\r\n\u001b[0;33m    WantedBy: multi-user.target\u001b[0m\r\n\u001b[0;33m    Wants: network-online.target\u001b[0m\r\n\u001b[0;33m    WatchdogTimestamp: Sat 2020-12-05 12:56:32 UTC\u001b[0m\r\n\u001b[0;33m    WatchdogTimestampMonotonic: '303636703'\u001b[0m\r\n\u001b[0;33m    WatchdogUSec: '0'\u001b[0m\r\n"]
[282.419136, "o", "\u001b[0;34mMETA: ran handlers\u001b[0m\r\n"]
[282.425011, "o", "\u001b[0;34mMETA: ran handlers\u001b[0m\r\n"]
[282.425798, "o", "\r\nPLAY RECAP ************************************************************************************************************************************************************************************\r\n\u001b[0;33mu1a1\u001b[0m                       : \u001b[0;32mok=11  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n"]
[282.426, "o", "\u001b[0;33mu1a2\u001b[0m                       : \u001b[0;32mok=11  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n\u001b[0;33mu1a3\u001b[0m                       : \u001b[0;32mok=11  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n\u001b[0;33mu1b1\u001b[0m                       : \u001b[0;32mok=11  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n"]
[282.426171, "o", "\u001b[0;33mu1b2\u001b[0m                       : \u001b[0;32mok=11  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n\u001b[0;33mu1b3\u001b[0m                       : \u001b[0;32mok=11  \u001b[0m \u001b[0;33mchanged=8   \u001b[0m unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \r\n\r\n"]
[282.472863, "o", "\r\nreal\t4m42.450s\r\nuser\t0m56.785s\r\nsys\t0m13.896s\r\n"]
[282.641444, "o", "cd /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ && tee /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/u1.yml </home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/u1-v1.18.12-rancher1-1.yml \\\r\n            && ./files/rke up --config=/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/u1.yml\r\n"]
[282.727693, "o", "---\r\ncluster_name: u1\r\nkubernetes_version: v1.18.12-rancher1-1\r\n\r\nssh_agent_auth: true\r\n\r\nnodes:\r\n  - &m1\r\n    address: 10.50.2.10\r\n    user: ubuntu\r\n    role: [etcd, controlplane]\r\n  - <<: *m1\r\n    address: 10.50.2.11\r\n  - <<: *m1\r\n    address: 10.50.2.12\r\n\r\n  - &n1\r\n    address: 10.50.2.20\r\n    user: ubuntu\r\n    role: [worker]\r\n  - <<: *n1\r\n    address: 10.50.2.21\r\n  - <<: *n1\r\n    address: 10.50.2.22\r\n\r\nnetwork:\r\n  plugin: calico\r\n\r\nprivate_registries:\r\n  - url: 10.8.101.2:5000\r\n    is_default: true\r\n"]
[282.74911, "o", "\u001b[36mINFO\u001b[0m[0000] Running RKE version: v1.2.3                  \r\n"]
[282.749631, "o", "\u001b[36mINFO\u001b[0m[0000] Initiating Kubernetes cluster                \r\n"]
[282.768278, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.12]  \r\n\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.22]  \r\n\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.11]  \r\n"]
[282.768521, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.21]  \r\n\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.20]  \r\n"]
[282.768714, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.10]  \r\n"]
[283.584302, "o", "\u001b[36mINFO\u001b[0m[0000] Checking if container [cluster-state-deployer] is running on host [10.50.2.10], try #1 \r\n"]
[283.586615, "o", "\u001b[36mINFO\u001b[0m[0000] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.50.2.10], try #1 \r\n"]
[286.251659, "o", "\u001b[36mINFO\u001b[0m[0003] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[286.792263, "o", "\u001b[36mINFO\u001b[0m[0004] Starting container [cluster-state-deployer] on host [10.50.2.10], try #1 \r\n"]
[287.077426, "o", "\u001b[36mINFO\u001b[0m[0004] [state] Successfully started [cluster-state-deployer] container on host [10.50.2.10] \r\n"]
[287.159628, "o", "\u001b[36mINFO\u001b[0m[0004] Checking if container [cluster-state-deployer] is running on host [10.50.2.11], try #1 \r\n"]
[287.161512, "o", "\u001b[36mINFO\u001b[0m[0004] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.50.2.11], try #1 \r\n"]
[289.90007, "o", "\u001b[36mINFO\u001b[0m[0007] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[290.545032, "o", "\u001b[36mINFO\u001b[0m[0007] Starting container [cluster-state-deployer] on host [10.50.2.11], try #1 \r\n"]
[290.829046, "o", "\u001b[36mINFO\u001b[0m[0008] [state] Successfully started [cluster-state-deployer] container on host [10.50.2.11] \r\n"]
[290.91361, "o", "\u001b[36mINFO\u001b[0m[0008] Checking if container [cluster-state-deployer] is running on host [10.50.2.12], try #1 \r\n"]
[290.915849, "o", "\u001b[36mINFO\u001b[0m[0008] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.50.2.12], try #1 \r\n"]
[293.620169, "o", "\u001b[36mINFO\u001b[0m[0010] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[294.348282, "o", "\u001b[36mINFO\u001b[0m[0011] Starting container [cluster-state-deployer] on host [10.50.2.12], try #1 \r\n"]
[294.643045, "o", "\u001b[36mINFO\u001b[0m[0011] [state] Successfully started [cluster-state-deployer] container on host [10.50.2.12] \r\n"]
[294.736301, "o", "\u001b[36mINFO\u001b[0m[0012] Checking if container [cluster-state-deployer] is running on host [10.50.2.20], try #1 \r\n"]
[294.745353, "o", "\u001b[36mINFO\u001b[0m[0012] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.50.2.20], try #1 \r\n"]
[297.289789, "o", "\u001b[36mINFO\u001b[0m[0014] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[297.832305, "o", "\u001b[36mINFO\u001b[0m[0015] Starting container [cluster-state-deployer] on host [10.50.2.20], try #1 \r\n"]
[298.106058, "o", "\u001b[36mINFO\u001b[0m[0015] [state] Successfully started [cluster-state-deployer] container on host [10.50.2.20] \r\n"]
[298.194581, "o", "\u001b[36mINFO\u001b[0m[0015] Checking if container [cluster-state-deployer] is running on host [10.50.2.21], try #1 \r\n"]
[298.203395, "o", "\u001b[36mINFO\u001b[0m[0015] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.50.2.21], try #1 \r\n"]
[300.829187, "o", "\u001b[36mINFO\u001b[0m[0018] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[301.353336, "o", "\u001b[36mINFO\u001b[0m[0018] Starting container [cluster-state-deployer] on host [10.50.2.21], try #1 \r\n"]
[301.716106, "o", "\u001b[36mINFO\u001b[0m[0018] [state] Successfully started [cluster-state-deployer] container on host [10.50.2.21] \r\n"]
[301.801548, "o", "\u001b[36mINFO\u001b[0m[0019] Checking if container [cluster-state-deployer] is running on host [10.50.2.22], try #1 \r\n"]
[301.803688, "o", "\u001b[36mINFO\u001b[0m[0019] Pulling image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] on host [10.50.2.22], try #1 \r\n"]
[304.253981, "o", "\u001b[36mINFO\u001b[0m[0021] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[304.899174, "o", "\u001b[36mINFO\u001b[0m[0022] Starting container [cluster-state-deployer] on host [10.50.2.22], try #1 \r\n"]
[305.185451, "o", "\u001b[36mINFO\u001b[0m[0022] [state] Successfully started [cluster-state-deployer] container on host [10.50.2.22] \r\n"]
[305.263179, "o", "\u001b[36mINFO\u001b[0m[0022] [certificates] Generating CA kubernetes certificates \r\n"]
[305.392793, "o", "\u001b[36mINFO\u001b[0m[0022] [certificates] Generating Kubernetes API server aggregation layer requestheader client CA certificates \r\n"]
[305.708316, "o", "\u001b[36mINFO\u001b[0m[0022] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates \r\n\u001b[36mINFO\u001b[0m[0022] [certificates] Generating Kubernetes API server certificates \r\n"]
[305.881455, "o", "\u001b[36mINFO\u001b[0m[0023] [certificates] Generating Service account token key \r\n\u001b[36mINFO\u001b[0m[0023] [certificates] Generating Kube Controller certificates \r\n"]
[305.973823, "o", "\u001b[36mINFO\u001b[0m[0023] [certificates] Generating Kube Scheduler certificates \r\n"]
[306.114358, "o", "\u001b[36mINFO\u001b[0m[0023] [certificates] Generating Kube Proxy certificates \r\n"]
[306.229049, "o", "\u001b[36mINFO\u001b[0m[0023] [certificates] Generating Node certificate   \r\n"]
[306.230524, "o", "\u001b[36mINFO\u001b[0m[0023] [certificates] Generating admin certificates and kubeconfig \r\n"]
[306.349243, "o", "\u001b[36mINFO\u001b[0m[0023] [certificates] Generating Kubernetes API server proxy client certificates \r\n"]
[306.467878, "o", "\u001b[36mINFO\u001b[0m[0023] [certificates] Generating kube-etcd-10-50-2-10 certificate and key \r\n"]
[306.574215, "o", "\u001b[36mINFO\u001b[0m[0023] [certificates] Generating kube-etcd-10-50-2-11 certificate and key \r\n"]
[306.852286, "o", "\u001b[36mINFO\u001b[0m[0024] [certificates] Generating kube-etcd-10-50-2-12 certificate and key \r\n"]
[306.975296, "o", "\u001b[36mINFO\u001b[0m[0024] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/u1.rkestate] \r\n"]
[306.97735, "o", "\u001b[36mINFO\u001b[0m[0024] Building Kubernetes cluster                  \r\n\u001b[36mINFO\u001b[0m[0024] [dialer] Setup tunnel for host [10.50.2.22]  \r\n"]
[306.977526, "o", "\u001b[36mINFO\u001b[0m[0024] [dialer] Setup tunnel for host [10.50.2.12]  \r\n\u001b[36mINFO\u001b[0m[0024] [dialer] Setup tunnel for host [10.50.2.10]  \r\n\u001b[36mINFO\u001b[0m[0024] [dialer] Setup tunnel for host [10.50.2.21]  \r\n"]
[306.977583, "o", "\u001b[36mINFO\u001b[0m[0024] [dialer] Setup tunnel for host [10.50.2.20]  \r\n\u001b[36mINFO\u001b[0m[0024] [dialer] Setup tunnel for host [10.50.2.11]  \r\n"]
[307.666028, "o", "\u001b[36mINFO\u001b[0m[0024] [network] Deploying port listener containers \r\n"]
[307.668195, "o", "\u001b[36mINFO\u001b[0m[0024] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[307.668303, "o", "\u001b[36mINFO\u001b[0m[0024] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[307.668474, "o", "\u001b[36mINFO\u001b[0m[0024] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[307.981716, "o", "\u001b[36mINFO\u001b[0m[0025] Starting container [rke-etcd-port-listener] on host [10.50.2.11], try #1 \r\n"]
[307.983777, "o", "\u001b[36mINFO\u001b[0m[0025] Starting container [rke-etcd-port-listener] on host [10.50.2.10], try #1 \r\n"]
[307.984231, "o", "\u001b[36mINFO\u001b[0m[0025] Starting container [rke-etcd-port-listener] on host [10.50.2.12], try #1 \r\n"]
[308.431706, "o", "\u001b[36mINFO\u001b[0m[0025] [network] Successfully started [rke-etcd-port-listener] container on host [10.50.2.10] \r\n"]
[308.445891, "o", "\u001b[36mINFO\u001b[0m[0025] [network] Successfully started [rke-etcd-port-listener] container on host [10.50.2.11] \r\n"]
[308.448061, "o", "\u001b[36mINFO\u001b[0m[0025] [network] Successfully started [rke-etcd-port-listener] container on host [10.50.2.12] \r\n"]
[308.451483, "o", "\u001b[36mINFO\u001b[0m[0025] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[308.451863, "o", "\u001b[36mINFO\u001b[0m[0025] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[308.45281, "o", "\u001b[36mINFO\u001b[0m[0025] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[308.792785, "o", "\u001b[36mINFO\u001b[0m[0026] Starting container [rke-cp-port-listener] on host [10.50.2.11], try #1 \r\n"]
[308.794075, "o", "\u001b[36mINFO\u001b[0m[0026] Starting container [rke-cp-port-listener] on host [10.50.2.10], try #1 \r\n"]
[308.795074, "o", "\u001b[36mINFO\u001b[0m[0026] Starting container [rke-cp-port-listener] on host [10.50.2.12], try #1 \r\n"]
[309.193424, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Successfully started [rke-cp-port-listener] container on host [10.50.2.11] \r\n"]
[309.202515, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Successfully started [rke-cp-port-listener] container on host [10.50.2.10] \r\n"]
[309.211092, "o", "\u001b[36mINFO\u001b[0m[0026] [network] Successfully started [rke-cp-port-listener] container on host [10.50.2.12] \r\n"]
[309.213073, "o", "\u001b[36mINFO\u001b[0m[0026] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[309.213555, "o", "\u001b[36mINFO\u001b[0m[0026] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[309.213666, "o", "\u001b[36mINFO\u001b[0m[0026] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[309.476626, "o", "\u001b[36mINFO\u001b[0m[0026] Starting container [rke-worker-port-listener] on host [10.50.2.20], try #1 \r\n"]
[309.480369, "o", "\u001b[36mINFO\u001b[0m[0026] Starting container [rke-worker-port-listener] on host [10.50.2.22], try #1 \r\n"]
[309.480985, "o", "\u001b[36mINFO\u001b[0m[0026] Starting container [rke-worker-port-listener] on host [10.50.2.21], try #1 \r\n"]
[309.91214, "o", "\u001b[36mINFO\u001b[0m[0027] [network] Successfully started [rke-worker-port-listener] container on host [10.50.2.20] \r\n"]
[309.925068, "o", "\u001b[36mINFO\u001b[0m[0027] [network] Successfully started [rke-worker-port-listener] container on host [10.50.2.21] \r\n"]
[309.940752, "o", "\u001b[36mINFO\u001b[0m[0027] [network] Successfully started [rke-worker-port-listener] container on host [10.50.2.22] \r\n\u001b[36mINFO\u001b[0m[0027] [network] Port listener containers deployed successfully \r\n\u001b[36mINFO\u001b[0m[0027] [network] Running etcd <-> etcd port checks  \r\n"]
[309.943987, "o", "\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[309.944461, "o", "\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[309.944597, "o", "\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[310.211301, "o", "\u001b[36mINFO\u001b[0m[0027] Starting container [rke-port-checker] on host [10.50.2.12], try #1 \r\n"]
[310.215355, "o", "\u001b[36mINFO\u001b[0m[0027] Starting container [rke-port-checker] on host [10.50.2.11], try #1 \r\n"]
[310.224002, "o", "\u001b[36mINFO\u001b[0m[0027] Starting container [rke-port-checker] on host [10.50.2.10], try #1 \r\n"]
[310.493981, "o", "\u001b[36mINFO\u001b[0m[0027] [network] Successfully started [rke-port-checker] container on host [10.50.2.10] \r\n"]
[310.497125, "o", "\u001b[36mINFO\u001b[0m[0027] [network] Successfully started [rke-port-checker] container on host [10.50.2.12] \r\n"]
[310.524237, "o", "\u001b[36mINFO\u001b[0m[0027] [network] Successfully started [rke-port-checker] container on host [10.50.2.11] \r\n"]
[310.583951, "o", "\u001b[36mINFO\u001b[0m[0027] Removing container [rke-port-checker] on host [10.50.2.12], try #1 \r\n"]
[310.602703, "o", "\u001b[36mINFO\u001b[0m[0027] Removing container [rke-port-checker] on host [10.50.2.10], try #1 \r\n"]
[310.613733, "o", "\u001b[36mINFO\u001b[0m[0027] Removing container [rke-port-checker] on host [10.50.2.11], try #1 \r\n"]
[310.654586, "o", "\u001b[36mINFO\u001b[0m[0027] [network] Running control plane -> etcd port checks \r\n"]
[310.656438, "o", "\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[310.656607, "o", "\u001b[36mINFO\u001b[0m[0027] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[310.907662, "o", "\u001b[36mINFO\u001b[0m[0028] Starting container [rke-port-checker] on host [10.50.2.10], try #1 \r\n"]
[310.90997, "o", "\u001b[36mINFO\u001b[0m[0028] Starting container [rke-port-checker] on host [10.50.2.11], try #1 \r\n"]
[310.925342, "o", "\u001b[36mINFO\u001b[0m[0028] Starting container [rke-port-checker] on host [10.50.2.12], try #1 \r\n"]
[311.137012, "o", "\u001b[36mINFO\u001b[0m[0028] [network] Successfully started [rke-port-checker] container on host [10.50.2.11] \r\n"]
[311.144984, "o", "\u001b[36mINFO\u001b[0m[0028] [network] Successfully started [rke-port-checker] container on host [10.50.2.12] \r\n"]
[311.174284, "o", "\u001b[36mINFO\u001b[0m[0028] [network] Successfully started [rke-port-checker] container on host [10.50.2.10] \r\n"]
[311.205693, "o", "\u001b[36mINFO\u001b[0m[0028] Removing container [rke-port-checker] on host [10.50.2.11], try #1 \r\n"]
[311.21109, "o", "\u001b[36mINFO\u001b[0m[0028] Removing container [rke-port-checker] on host [10.50.2.12], try #1 \r\n"]
[311.24648, "o", "\u001b[36mINFO\u001b[0m[0028] Removing container [rke-port-checker] on host [10.50.2.10], try #1 \r\n"]
[311.284918, "o", "\u001b[36mINFO\u001b[0m[0028] [network] Running control plane -> worker port checks \r\n"]
[311.286779, "o", "\u001b[36mINFO\u001b[0m[0028] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[311.2869, "o", "\u001b[36mINFO\u001b[0m[0028] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0028] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[311.52474, "o", "\u001b[36mINFO\u001b[0m[0028] Starting container [rke-port-checker] on host [10.50.2.12], try #1 \r\n"]
[311.526886, "o", "\u001b[36mINFO\u001b[0m[0028] Starting container [rke-port-checker] on host [10.50.2.11], try #1 \r\n"]
[311.528244, "o", "\u001b[36mINFO\u001b[0m[0028] Starting container [rke-port-checker] on host [10.50.2.10], try #1 \r\n"]
[311.793347, "o", "\u001b[36mINFO\u001b[0m[0029] [network] Successfully started [rke-port-checker] container on host [10.50.2.12] \r\n"]
[311.806363, "o", "\u001b[36mINFO\u001b[0m[0029] [network] Successfully started [rke-port-checker] container on host [10.50.2.11] \r\n"]
[311.814919, "o", "\u001b[36mINFO\u001b[0m[0029] [network] Successfully started [rke-port-checker] container on host [10.50.2.10] \r\n"]
[311.891211, "o", "\u001b[36mINFO\u001b[0m[0029] Removing container [rke-port-checker] on host [10.50.2.12], try #1 \r\n"]
[311.914655, "o", "\u001b[36mINFO\u001b[0m[0029] Removing container [rke-port-checker] on host [10.50.2.10], try #1 \r\n"]
[311.915531, "o", "\u001b[36mINFO\u001b[0m[0029] Removing container [rke-port-checker] on host [10.50.2.11], try #1 \r\n"]
[311.965323, "o", "\u001b[36mINFO\u001b[0m[0029] [network] Running workers -> control plane port checks \r\n"]
[311.968946, "o", "\u001b[36mINFO\u001b[0m[0029] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[311.969139, "o", "\u001b[36mINFO\u001b[0m[0029] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[311.969589, "o", "\u001b[36mINFO\u001b[0m[0029] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[312.249477, "o", "\u001b[36mINFO\u001b[0m[0029] Starting container [rke-port-checker] on host [10.50.2.21], try #1 \r\n"]
[312.250087, "o", "\u001b[36mINFO\u001b[0m[0029] Starting container [rke-port-checker] on host [10.50.2.20], try #1 \r\n"]
[312.260412, "o", "\u001b[36mINFO\u001b[0m[0029] Starting container [rke-port-checker] on host [10.50.2.22], try #1 \r\n"]
[312.549327, "o", "\u001b[36mINFO\u001b[0m[0029] [network] Successfully started [rke-port-checker] container on host [10.50.2.22] \r\n"]
[312.565339, "o", "\u001b[36mINFO\u001b[0m[0029] [network] Successfully started [rke-port-checker] container on host [10.50.2.20] \r\n"]
[312.567326, "o", "\u001b[36mINFO\u001b[0m[0029] [network] Successfully started [rke-port-checker] container on host [10.50.2.21] \r\n"]
[312.647683, "o", "\u001b[36mINFO\u001b[0m[0029] Removing container [rke-port-checker] on host [10.50.2.21], try #1 \r\n"]
[312.648355, "o", "\u001b[36mINFO\u001b[0m[0029] Removing container [rke-port-checker] on host [10.50.2.20], try #1 \r\n"]
[312.654056, "o", "\u001b[36mINFO\u001b[0m[0029] Removing container [rke-port-checker] on host [10.50.2.22], try #1 \r\n"]
[312.708246, "o", "\u001b[36mINFO\u001b[0m[0029] [network] Checking KubeAPI port Control Plane hosts \r\n"]
[312.709533, "o", "\u001b[36mINFO\u001b[0m[0029] [network] Removing port listener containers  \r\n"]
[312.711639, "o", "\u001b[36mINFO\u001b[0m[0029] Removing container [rke-etcd-port-listener] on host [10.50.2.10], try #1 \r\n\u001b[36mINFO\u001b[0m[0029] Removing container [rke-etcd-port-listener] on host [10.50.2.12], try #1 \r\n"]
[312.711789, "o", "\u001b[36mINFO\u001b[0m[0029] Removing container [rke-etcd-port-listener] on host [10.50.2.11], try #1 \r\n"]
[312.985922, "o", "\u001b[36mINFO\u001b[0m[0030] [remove/rke-etcd-port-listener] Successfully removed container on host [10.50.2.12] \r\n"]
[312.988163, "o", "\u001b[36mINFO\u001b[0m[0030] [remove/rke-etcd-port-listener] Successfully removed container on host [10.50.2.10] \r\n"]
[312.989284, "o", "\u001b[36mINFO\u001b[0m[0030] [remove/rke-etcd-port-listener] Successfully removed container on host [10.50.2.11] \r\n"]
[312.990719, "o", "\u001b[36mINFO\u001b[0m[0030] Removing container [rke-cp-port-listener] on host [10.50.2.11], try #1 \r\n"]
[312.990858, "o", "\u001b[36mINFO\u001b[0m[0030] Removing container [rke-cp-port-listener] on host [10.50.2.10], try #1 \r\n"]
[312.990921, "o", "\u001b[36mINFO\u001b[0m[0030] Removing container [rke-cp-port-listener] on host [10.50.2.12], try #1 \r\n"]
[313.246432, "o", "\u001b[36mINFO\u001b[0m[0030] [remove/rke-cp-port-listener] Successfully removed container on host [10.50.2.11] \r\n"]
[313.254882, "o", "\u001b[36mINFO\u001b[0m[0030] [remove/rke-cp-port-listener] Successfully removed container on host [10.50.2.10] \r\n"]
[313.25507, "o", "\u001b[36mINFO\u001b[0m[0030] [remove/rke-cp-port-listener] Successfully removed container on host [10.50.2.12] \r\n"]
[313.256126, "o", "\u001b[36mINFO\u001b[0m[0030] Removing container [rke-worker-port-listener] on host [10.50.2.22], try #1 \r\n"]
[313.25623, "o", "\u001b[36mINFO\u001b[0m[0030] Removing container [rke-worker-port-listener] on host [10.50.2.20], try #1 \r\n"]
[313.256465, "o", "\u001b[36mINFO\u001b[0m[0030] Removing container [rke-worker-port-listener] on host [10.50.2.21], try #1 \r\n"]
[313.489816, "o", "\u001b[36mINFO\u001b[0m[0030] [remove/rke-worker-port-listener] Successfully removed container on host [10.50.2.22] \r\n"]
[313.491504, "o", "\u001b[36mINFO\u001b[0m[0030] [remove/rke-worker-port-listener] Successfully removed container on host [10.50.2.21] \r\n"]
[313.497838, "o", "\u001b[36mINFO\u001b[0m[0030] [remove/rke-worker-port-listener] Successfully removed container on host [10.50.2.20] \r\n\u001b[36mINFO\u001b[0m[0030] [network] Port listener containers removed successfully \r\n"]
[313.497969, "o", "\u001b[36mINFO\u001b[0m[0030] [certificates] Deploying kubernetes certificates to Cluster nodes \r\n\u001b[36mINFO\u001b[0m[0030] Checking if container [cert-deployer] is running on host [10.50.2.22], try #1 \r\n"]
[313.498199, "o", "\u001b[36mINFO\u001b[0m[0030] Checking if container [cert-deployer] is running on host [10.50.2.20], try #1 \r\n\u001b[36mINFO\u001b[0m[0030] Checking if container [cert-deployer] is running on host [10.50.2.21], try #1 \r\n"]
[313.498361, "o", "\u001b[36mINFO\u001b[0m[0030] Checking if container [cert-deployer] is running on host [10.50.2.10], try #1 \r\n"]
[313.499754, "o", "\u001b[36mINFO\u001b[0m[0030] Checking if container [cert-deployer] is running on host [10.50.2.11], try #1 \r\n\u001b[36mINFO\u001b[0m[0030] Checking if container [cert-deployer] is running on host [10.50.2.12], try #1 \r\n"]
[313.501154, "o", "\u001b[36mINFO\u001b[0m[0030] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[313.501788, "o", "\u001b[36mINFO\u001b[0m[0030] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0030] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0030] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[313.501888, "o", "\u001b[36mINFO\u001b[0m[0030] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[313.502484, "o", "\u001b[36mINFO\u001b[0m[0030] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[313.914322, "o", "\u001b[36mINFO\u001b[0m[0031] Starting container [cert-deployer] on host [10.50.2.21], try #1 \r\n"]
[313.915155, "o", "\u001b[36mINFO\u001b[0m[0031] Starting container [cert-deployer] on host [10.50.2.20], try #1 \r\n"]
[313.920802, "o", "\u001b[36mINFO\u001b[0m[0031] Starting container [cert-deployer] on host [10.50.2.22], try #1 \r\n"]
[313.921056, "o", "\u001b[36mINFO\u001b[0m[0031] Starting container [cert-deployer] on host [10.50.2.12], try #1 \r\n"]
[313.922945, "o", "\u001b[36mINFO\u001b[0m[0031] Starting container [cert-deployer] on host [10.50.2.11], try #1 \r\n"]
[313.92808, "o", "\u001b[36mINFO\u001b[0m[0031] Starting container [cert-deployer] on host [10.50.2.10], try #1 \r\n"]
[314.22893, "o", "\u001b[36mINFO\u001b[0m[0031] Checking if container [cert-deployer] is running on host [10.50.2.21], try #1 \r\n"]
[314.252538, "o", "\u001b[36mINFO\u001b[0m[0031] Checking if container [cert-deployer] is running on host [10.50.2.20], try #1 \r\n"]
[314.267782, "o", "\u001b[36mINFO\u001b[0m[0031] Checking if container [cert-deployer] is running on host [10.50.2.22], try #1 \r\n"]
[314.281762, "o", "\u001b[36mINFO\u001b[0m[0031] Checking if container [cert-deployer] is running on host [10.50.2.10], try #1 \r\n"]
[314.284994, "o", "\u001b[36mINFO\u001b[0m[0031] Checking if container [cert-deployer] is running on host [10.50.2.12], try #1 \r\n"]
[314.289747, "o", "\u001b[36mINFO\u001b[0m[0031] Checking if container [cert-deployer] is running on host [10.50.2.11], try #1 \r\n"]
[319.230641, "o", "\u001b[36mINFO\u001b[0m[0036] Checking if container [cert-deployer] is running on host [10.50.2.21], try #1 \r\n"]
[319.232573, "o", "\u001b[36mINFO\u001b[0m[0036] Removing container [cert-deployer] on host [10.50.2.21], try #1 \r\n"]
[319.254123, "o", "\u001b[36mINFO\u001b[0m[0036] Checking if container [cert-deployer] is running on host [10.50.2.20], try #1 \r\n"]
[319.255992, "o", "\u001b[36mINFO\u001b[0m[0036] Removing container [cert-deployer] on host [10.50.2.20], try #1 \r\n"]
[319.269076, "o", "\u001b[36mINFO\u001b[0m[0036] Checking if container [cert-deployer] is running on host [10.50.2.22], try #1 \r\n"]
[319.270827, "o", "\u001b[36mINFO\u001b[0m[0036] Removing container [cert-deployer] on host [10.50.2.22], try #1 \r\n"]
[319.283797, "o", "\u001b[36mINFO\u001b[0m[0036] Checking if container [cert-deployer] is running on host [10.50.2.10], try #1 \r\n"]
[319.285141, "o", "\u001b[36mINFO\u001b[0m[0036] Removing container [cert-deployer] on host [10.50.2.10], try #1 \r\n"]
[319.286287, "o", "\u001b[36mINFO\u001b[0m[0036] Checking if container [cert-deployer] is running on host [10.50.2.12], try #1 \r\n"]
[319.287394, "o", "\u001b[36mINFO\u001b[0m[0036] Removing container [cert-deployer] on host [10.50.2.12], try #1 \r\n"]
[319.292603, "o", "\u001b[36mINFO\u001b[0m[0036] Checking if container [cert-deployer] is running on host [10.50.2.11], try #1 \r\n"]
[319.293758, "o", "\u001b[36mINFO\u001b[0m[0036] Removing container [cert-deployer] on host [10.50.2.11], try #1 \r\n"]
[319.306365, "o", "\u001b[36mINFO\u001b[0m[0036] [reconcile] Rebuilding and updating local kube config \r\n"]
[319.306495, "o", "\u001b[36mINFO\u001b[0m[0036] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_u1.yml] \r\n"]
[319.308351, "o", "\u001b[36mINFO\u001b[0m[0036] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_u1.yml] \r\n"]
[319.309811, "o", "\u001b[36mINFO\u001b[0m[0036] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_u1.yml] \r\n"]
[319.310837, "o", "\u001b[36mINFO\u001b[0m[0036] [certificates] Successfully deployed kubernetes certificates to Cluster nodes \r\n"]
[319.310951, "o", "\u001b[36mINFO\u001b[0m[0036] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.50.2.12] \r\n"]
[319.312975, "o", "\u001b[36mINFO\u001b[0m[0036] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[319.446478, "o", "\u001b[36mINFO\u001b[0m[0036] Starting container [file-deployer] on host [10.50.2.12], try #1 \r\n"]
[319.797143, "o", "\u001b[36mINFO\u001b[0m[0037] Successfully started [file-deployer] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0037] Waiting for [file-deployer] container to exit on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0037] Waiting for [file-deployer] container to exit on host [10.50.2.12] \r\n"]
[319.798241, "o", "\u001b[36mINFO\u001b[0m[0037] Container [file-deployer] is still running on host [10.50.2.12]: stderr: [], stdout: [] \r\n"]
[320.798631, "o", "\u001b[36mINFO\u001b[0m[0038] Waiting for [file-deployer] container to exit on host [10.50.2.12] \r\n"]
[320.805265, "o", "\u001b[36mINFO\u001b[0m[0038] Removing container [file-deployer] on host [10.50.2.12], try #1 \r\n"]
[320.843205, "o", "\u001b[36mINFO\u001b[0m[0038] [remove/file-deployer] Successfully removed container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0038] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.50.2.10] \r\n"]
[320.852496, "o", "\u001b[36mINFO\u001b[0m[0038] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[321.038436, "o", "\u001b[36mINFO\u001b[0m[0038] Starting container [file-deployer] on host [10.50.2.10], try #1 \r\n"]
[321.364787, "o", "\u001b[36mINFO\u001b[0m[0038] Successfully started [file-deployer] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0038] Waiting for [file-deployer] container to exit on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0038] Waiting for [file-deployer] container to exit on host [10.50.2.10] \r\n"]
[321.365763, "o", "\u001b[36mINFO\u001b[0m[0038] Container [file-deployer] is still running on host [10.50.2.10]: stderr: [], stdout: [] \r\n"]
[322.366137, "o", "\u001b[36mINFO\u001b[0m[0039] Waiting for [file-deployer] container to exit on host [10.50.2.10] \r\n"]
[322.372513, "o", "\u001b[36mINFO\u001b[0m[0039] Removing container [file-deployer] on host [10.50.2.10], try #1 \r\n"]
[322.408864, "o", "\u001b[36mINFO\u001b[0m[0039] [remove/file-deployer] Successfully removed container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0039] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.50.2.11] \r\n"]
[322.415745, "o", "\u001b[36mINFO\u001b[0m[0039] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[322.599583, "o", "\u001b[36mINFO\u001b[0m[0039] Starting container [file-deployer] on host [10.50.2.11], try #1 \r\n"]
[322.921265, "o", "\u001b[36mINFO\u001b[0m[0040] Successfully started [file-deployer] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0040] Waiting for [file-deployer] container to exit on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0040] Waiting for [file-deployer] container to exit on host [10.50.2.11] \r\n"]
[322.922627, "o", "\u001b[36mINFO\u001b[0m[0040] Container [file-deployer] is still running on host [10.50.2.11]: stderr: [], stdout: [] \r\n"]
[323.92291, "o", "\u001b[36mINFO\u001b[0m[0041] Waiting for [file-deployer] container to exit on host [10.50.2.11] \r\n"]
[323.928891, "o", "\u001b[36mINFO\u001b[0m[0041] Removing container [file-deployer] on host [10.50.2.11], try #1 \r\n"]
[323.961628, "o", "\u001b[36mINFO\u001b[0m[0041] [remove/file-deployer] Successfully removed container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0041] [/etc/kubernetes/audit-policy.yaml] Successfully deployed audit policy file to Cluster control nodes \r\n\u001b[36mINFO\u001b[0m[0041] [reconcile] Reconciling cluster state        \r\n\u001b[36mINFO\u001b[0m[0041] [reconcile] This is newly generated cluster  \r\n\u001b[36mINFO\u001b[0m[0041] Pre-pulling kubernetes images                \r\n"]
[323.964606, "o", "\u001b[36mINFO\u001b[0m[0041] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.50.2.11], try #1 \r\n"]
[323.968383, "o", "\u001b[36mINFO\u001b[0m[0041] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.50.2.21], try #1 \r\n\u001b[36mINFO\u001b[0m[0041] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.50.2.10], try #1 \r\n\u001b[36mINFO\u001b[0m[0041] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.50.2.20], try #1 \r\n\u001b[36mINFO\u001b[0m[0041] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.50.2.22], try #1 \r\n\u001b[36mINFO\u001b[0m[0041] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] on host [10.50.2.12], try #1 \r\n"]
[370.347474, "o", "\u001b[36mINFO\u001b[0m[0087] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0087] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.10] \r\n"]
[371.135694, "o", "\u001b[36mINFO\u001b[0m[0088] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.12] \r\n"]
[371.146406, "o", "\u001b[36mINFO\u001b[0m[0088] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.22] \r\n"]
[371.25042, "o", "\u001b[36mINFO\u001b[0m[0088] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.20] \r\n"]
[371.790673, "o", "\u001b[36mINFO\u001b[0m[0089] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0089] Kubernetes images pulled successfully        \r\n"]
[371.801975, "o", "\u001b[36mINFO\u001b[0m[0089] [etcd] Building up etcd plane..              \r\n"]
[371.81764, "o", "\u001b[36mINFO\u001b[0m[0089] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[372.428215, "o", "\u001b[36mINFO\u001b[0m[0089] Starting container [etcd-fix-perm] on host [10.50.2.10], try #1 \r\n"]
[372.831726, "o", "\u001b[36mINFO\u001b[0m[0090] Successfully started [etcd-fix-perm] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0090] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0090] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.10] \r\n"]
[372.833842, "o", "\u001b[36mINFO\u001b[0m[0090] Container [etcd-fix-perm] is still running on host [10.50.2.10]: stderr: [], stdout: [] \r\n"]
[373.834131, "o", "\u001b[36mINFO\u001b[0m[0091] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.10] \r\n"]
[373.839659, "o", "\u001b[36mINFO\u001b[0m[0091] Removing container [etcd-fix-perm] on host [10.50.2.10], try #1 \r\n"]
[373.859349, "o", "\u001b[36mINFO\u001b[0m[0091] [remove/etcd-fix-perm] Successfully removed container on host [10.50.2.10] \r\n"]
[373.861275, "o", "\u001b[36mINFO\u001b[0m[0091] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] on host [10.50.2.10], try #1 \r\n"]
[375.367381, "o", "\u001b[36mINFO\u001b[0m[0092] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] exists on host [10.50.2.10] \r\n"]
[375.657084, "o", "\u001b[36mINFO\u001b[0m[0092] Starting container [etcd] on host [10.50.2.10], try #1 \r\n"]
[375.898599, "o", "\u001b[36mINFO\u001b[0m[0093] [etcd] Successfully started [etcd] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0093] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.50.2.10] \r\n"]
[375.903134, "o", "\u001b[36mINFO\u001b[0m[0093] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[376.136421, "o", "\u001b[36mINFO\u001b[0m[0093] Starting container [etcd-rolling-snapshots] on host [10.50.2.10], try #1 \r\n"]
[376.393211, "o", "\u001b[36mINFO\u001b[0m[0093] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.50.2.10] \r\n"]
[381.400771, "o", "\u001b[36mINFO\u001b[0m[0098] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[381.58838, "o", "\u001b[36mINFO\u001b[0m[0098] Starting container [rke-bundle-cert] on host [10.50.2.10], try #1 \r\n"]
[381.811618, "o", "\u001b[36mINFO\u001b[0m[0099] [certificates] Successfully started [rke-bundle-cert] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0099] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.10] \r\n"]
[381.812745, "o", "\u001b[36mINFO\u001b[0m[0099] Container [rke-bundle-cert] is still running on host [10.50.2.10]: stderr: [], stdout: [] \r\n"]
[382.813036, "o", "\u001b[36mINFO\u001b[0m[0100] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.10] \r\n"]
[382.815673, "o", "\u001b[36mINFO\u001b[0m[0100] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0100] Removing container [rke-bundle-cert] on host [10.50.2.10], try #1 \r\n"]
[382.850863, "o", "\u001b[36mINFO\u001b[0m[0100] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[383.258053, "o", "\u001b[36mINFO\u001b[0m[0100] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[383.498788, "o", "\u001b[36mINFO\u001b[0m[0100] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[383.499626, "o", "\u001b[36mINFO\u001b[0m[0100] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[383.714384, "o", "\u001b[36mINFO\u001b[0m[0100] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n"]
[383.720163, "o", "\u001b[36mINFO\u001b[0m[0100] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[384.180995, "o", "\u001b[36mINFO\u001b[0m[0101] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[384.478908, "o", "\u001b[36mINFO\u001b[0m[0101] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[384.479852, "o", "\u001b[36mINFO\u001b[0m[0101] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[384.733387, "o", "\u001b[36mINFO\u001b[0m[0102] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n"]
[384.742423, "o", "\u001b[36mINFO\u001b[0m[0102] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[385.382374, "o", "\u001b[36mINFO\u001b[0m[0102] Starting container [etcd-fix-perm] on host [10.50.2.11], try #1 \r\n"]
[385.799809, "o", "\u001b[36mINFO\u001b[0m[0103] Successfully started [etcd-fix-perm] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0103] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0103] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.11] \r\n"]
[385.801692, "o", "\u001b[36mINFO\u001b[0m[0103] Container [etcd-fix-perm] is still running on host [10.50.2.11]: stderr: [], stdout: [] \r\n"]
[386.802001, "o", "\u001b[36mINFO\u001b[0m[0104] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.11] \r\n"]
[386.806692, "o", "\u001b[36mINFO\u001b[0m[0104] Removing container [etcd-fix-perm] on host [10.50.2.11], try #1 \r\n"]
[386.823915, "o", "\u001b[36mINFO\u001b[0m[0104] [remove/etcd-fix-perm] Successfully removed container on host [10.50.2.11] \r\n"]
[386.825357, "o", "\u001b[36mINFO\u001b[0m[0104] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] on host [10.50.2.11], try #1 \r\n"]
[388.276359, "o", "\u001b[36mINFO\u001b[0m[0105] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] exists on host [10.50.2.11] \r\n"]
[388.919226, "o", "\u001b[36mINFO\u001b[0m[0106] Starting container [etcd] on host [10.50.2.11], try #1 \r\n"]
[389.10208, "o", "\u001b[36mINFO\u001b[0m[0106] [etcd] Successfully started [etcd] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0106] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.50.2.11] \r\n"]
[389.104766, "o", "\u001b[36mINFO\u001b[0m[0106] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[389.417775, "o", "\u001b[36mINFO\u001b[0m[0106] Starting container [etcd-rolling-snapshots] on host [10.50.2.11], try #1 \r\n"]
[389.624789, "o", "\u001b[36mINFO\u001b[0m[0106] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.50.2.11] \r\n"]
[394.629773, "o", "\u001b[36mINFO\u001b[0m[0111] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[394.823209, "o", "\u001b[36mINFO\u001b[0m[0112] Starting container [rke-bundle-cert] on host [10.50.2.11], try #1 \r\n"]
[395.046218, "o", "\u001b[36mINFO\u001b[0m[0112] [certificates] Successfully started [rke-bundle-cert] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0112] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.11] \r\n"]
[395.047553, "o", "\u001b[36mINFO\u001b[0m[0112] Container [rke-bundle-cert] is still running on host [10.50.2.11]: stderr: [], stdout: [] \r\n"]
[396.047785, "o", "\u001b[36mINFO\u001b[0m[0113] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.11] \r\n"]
[396.050494, "o", "\u001b[36mINFO\u001b[0m[0113] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0113] Removing container [rke-bundle-cert] on host [10.50.2.11], try #1 \r\n"]
[396.075592, "o", "\u001b[36mINFO\u001b[0m[0113] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[396.488296, "o", "\u001b[36mINFO\u001b[0m[0113] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[396.735307, "o", "\u001b[36mINFO\u001b[0m[0114] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[396.736043, "o", "\u001b[36mINFO\u001b[0m[0114] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[396.94056, "o", "\u001b[36mINFO\u001b[0m[0114] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[396.943494, "o", "\u001b[36mINFO\u001b[0m[0114] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[397.343628, "o", "\u001b[36mINFO\u001b[0m[0114] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[397.587183, "o", "\u001b[36mINFO\u001b[0m[0114] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[397.587923, "o", "\u001b[36mINFO\u001b[0m[0114] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[397.808121, "o", "\u001b[36mINFO\u001b[0m[0115] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[397.820117, "o", "\u001b[36mINFO\u001b[0m[0115] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[398.253468, "o", "\u001b[36mINFO\u001b[0m[0115] Starting container [etcd-fix-perm] on host [10.50.2.12], try #1 \r\n"]
[398.72456, "o", "\u001b[36mINFO\u001b[0m[0115] Successfully started [etcd-fix-perm] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0115] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0115] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.12] \r\n"]
[398.726704, "o", "\u001b[36mINFO\u001b[0m[0115] Container [etcd-fix-perm] is still running on host [10.50.2.12]: stderr: [], stdout: [] \r\n"]
[399.726851, "o", "\u001b[36mINFO\u001b[0m[0116] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.12] \r\n"]
[399.731173, "o", "\u001b[36mINFO\u001b[0m[0116] Removing container [etcd-fix-perm] on host [10.50.2.12], try #1 \r\n"]
[399.752379, "o", "\u001b[36mINFO\u001b[0m[0117] [remove/etcd-fix-perm] Successfully removed container on host [10.50.2.12] \r\n"]
[399.756409, "o", "\u001b[36mINFO\u001b[0m[0117] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] on host [10.50.2.12], try #1 \r\n"]
[401.11779, "o", "\u001b[36mINFO\u001b[0m[0118] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.3-rancher1] exists on host [10.50.2.12] \r\n"]
[402.092699, "o", "\u001b[36mINFO\u001b[0m[0119] Starting container [etcd] on host [10.50.2.12], try #1 \r\n"]
[402.278504, "o", "\u001b[36mINFO\u001b[0m[0119] [etcd] Successfully started [etcd] container on host [10.50.2.12] \r\n"]
[402.27855, "o", "\u001b[36mINFO\u001b[0m[0119] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.50.2.12] \r\n"]
[402.282904, "o", "\u001b[36mINFO\u001b[0m[0119] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[402.450218, "o", "\u001b[36mINFO\u001b[0m[0119] Starting container [etcd-rolling-snapshots] on host [10.50.2.12], try #1 \r\n"]
[402.636438, "o", "\u001b[36mINFO\u001b[0m[0119] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.50.2.12] \r\n"]
[407.642612, "o", "\u001b[36mINFO\u001b[0m[0124] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[407.832978, "o", "\u001b[36mINFO\u001b[0m[0125] Starting container [rke-bundle-cert] on host [10.50.2.12], try #1 \r\n"]
[408.059626, "o", "\u001b[36mINFO\u001b[0m[0125] [certificates] Successfully started [rke-bundle-cert] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0125] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.12] \r\n"]
[408.060734, "o", "\u001b[36mINFO\u001b[0m[0125] Container [rke-bundle-cert] is still running on host [10.50.2.12]: stderr: [], stdout: [] \r\n"]
[409.06117, "o", "\u001b[36mINFO\u001b[0m[0126] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.12] \r\n"]
[409.075917, "o", "\u001b[36mINFO\u001b[0m[0126] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0126] Removing container [rke-bundle-cert] on host [10.50.2.12], try #1 \r\n"]
[409.096678, "o", "\u001b[36mINFO\u001b[0m[0126] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[409.536161, "o", "\u001b[36mINFO\u001b[0m[0126] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[409.773272, "o", "\u001b[36mINFO\u001b[0m[0127] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[409.77404, "o", "\u001b[36mINFO\u001b[0m[0127] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[409.973008, "o", "\u001b[36mINFO\u001b[0m[0127] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n"]
[409.976217, "o", "\u001b[36mINFO\u001b[0m[0127] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[410.425122, "o", "\u001b[36mINFO\u001b[0m[0127] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[410.668977, "o", "\u001b[36mINFO\u001b[0m[0127] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[410.669732, "o", "\u001b[36mINFO\u001b[0m[0127] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[410.853578, "o", "\u001b[36mINFO\u001b[0m[0128] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0128] [etcd] Successfully started etcd plane.. Checking etcd cluster health \r\n"]
[411.459958, "o", "\u001b[36mINFO\u001b[0m[0128] [etcd] etcd host [10.50.2.10] reported healthy=true \r\n"]
[411.460702, "o", "\u001b[36mINFO\u001b[0m[0128] [controlplane] Building up Controller Plane.. \r\n"]
[411.460945, "o", "\u001b[36mINFO\u001b[0m[0128] Checking if container [service-sidekick] is running on host [10.50.2.12], try #1 \r\n\u001b[36mINFO\u001b[0m[0128] Checking if container [service-sidekick] is running on host [10.50.2.10], try #1 \r\n\u001b[36mINFO\u001b[0m[0128] Checking if container [service-sidekick] is running on host [10.50.2.11], try #1 \r\n"]
[411.465716, "o", "\u001b[36mINFO\u001b[0m[0128] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[411.466633, "o", "\u001b[36mINFO\u001b[0m[0128] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[411.467461, "o", "\u001b[36mINFO\u001b[0m[0128] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[412.296593, "o", "\u001b[36mINFO\u001b[0m[0129] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.12] \r\n"]
[412.29761, "o", "\u001b[36mINFO\u001b[0m[0129] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.11] \r\n"]
[412.299591, "o", "\u001b[36mINFO\u001b[0m[0129] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.10] \r\n"]
[412.348204, "o", "\u001b[36mINFO\u001b[0m[0129] Starting container [kube-apiserver] on host [10.50.2.12], try #1 \r\n"]
[412.365479, "o", "\u001b[36mINFO\u001b[0m[0129] Starting container [kube-apiserver] on host [10.50.2.10], try #1 \r\n"]
[412.369191, "o", "\u001b[36mINFO\u001b[0m[0129] Starting container [kube-apiserver] on host [10.50.2.11], try #1 \r\n"]
[412.498143, "o", "\u001b[36mINFO\u001b[0m[0129] [controlplane] Successfully started [kube-apiserver] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0129] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.50.2.12] \r\n"]
[412.571548, "o", "\u001b[36mINFO\u001b[0m[0129] [controlplane] Successfully started [kube-apiserver] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0129] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.50.2.10] \r\n"]
[412.639141, "o", "\u001b[36mINFO\u001b[0m[0129] [controlplane] Successfully started [kube-apiserver] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0129] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.50.2.11] \r\n"]
[421.711126, "o", "\u001b[36mINFO\u001b[0m[0138] [healthcheck] service [kube-apiserver] on host [10.50.2.11] is healthy \r\n"]
[421.726876, "o", "\u001b[36mINFO\u001b[0m[0138] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[421.900383, "o", "\u001b[36mINFO\u001b[0m[0139] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[421.90248, "o", "\u001b[36mINFO\u001b[0m[0139] [healthcheck] service [kube-apiserver] on host [10.50.2.12] is healthy \r\n"]
[421.905656, "o", "\u001b[36mINFO\u001b[0m[0139] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[422.003604, "o", "\u001b[36mINFO\u001b[0m[0139] [healthcheck] service [kube-apiserver] on host [10.50.2.10] is healthy \r\n"]
[422.006893, "o", "\u001b[36mINFO\u001b[0m[0139] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[422.060869, "o", "\u001b[36mINFO\u001b[0m[0139] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[422.404266, "o", "\u001b[36mINFO\u001b[0m[0139] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[422.425771, "o", "\u001b[36mINFO\u001b[0m[0139] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[422.426654, "o", "\u001b[36mINFO\u001b[0m[0139] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[422.428417, "o", "\u001b[36mINFO\u001b[0m[0139] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[422.429492, "o", "\u001b[36mINFO\u001b[0m[0139] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[422.653293, "o", "\u001b[36mINFO\u001b[0m[0139] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[422.654434, "o", "\u001b[36mINFO\u001b[0m[0139] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.11] \r\n"]
[422.65624, "o", "\u001b[36mINFO\u001b[0m[0139] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n"]
[422.657551, "o", "\u001b[36mINFO\u001b[0m[0139] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.12] \r\n"]
[422.69809, "o", "\u001b[36mINFO\u001b[0m[0139] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[422.699301, "o", "\u001b[36mINFO\u001b[0m[0139] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[422.70091, "o", "\u001b[36mINFO\u001b[0m[0139] Starting container [kube-controller-manager] on host [10.50.2.12], try #1 \r\n"]
[422.70753, "o", "\u001b[36mINFO\u001b[0m[0139] Starting container [kube-controller-manager] on host [10.50.2.11], try #1 \r\n"]
[422.896682, "o", "\u001b[36mINFO\u001b[0m[0140] [controlplane] Successfully started [kube-controller-manager] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0140] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.50.2.11] \r\n"]
[422.897355, "o", "\u001b[36mINFO\u001b[0m[0140] [controlplane] Successfully started [kube-controller-manager] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0140] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.50.2.12] \r\n"]
[422.95275, "o", "\u001b[36mINFO\u001b[0m[0140] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n"]
[422.954574, "o", "\u001b[36mINFO\u001b[0m[0140] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.10] \r\n"]
[423.038422, "o", "\u001b[36mINFO\u001b[0m[0140] Starting container [kube-controller-manager] on host [10.50.2.10], try #1 \r\n"]
[423.279571, "o", "\u001b[36mINFO\u001b[0m[0140] [controlplane] Successfully started [kube-controller-manager] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0140] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.50.2.10] \r\n"]
[429.05354, "o", "\u001b[36mINFO\u001b[0m[0146] [healthcheck] service [kube-controller-manager] on host [10.50.2.12] is healthy \r\n"]
[429.057084, "o", "\u001b[36mINFO\u001b[0m[0146] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[429.063428, "o", "\u001b[36mINFO\u001b[0m[0146] [healthcheck] service [kube-controller-manager] on host [10.50.2.11] is healthy \r\n"]
[429.067127, "o", "\u001b[36mINFO\u001b[0m[0146] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[429.237261, "o", "\u001b[36mINFO\u001b[0m[0146] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[429.237749, "o", "\u001b[36mINFO\u001b[0m[0146] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[429.435442, "o", "\u001b[36mINFO\u001b[0m[0146] [healthcheck] service [kube-controller-manager] on host [10.50.2.10] is healthy \r\n"]
[429.43841, "o", "\u001b[36mINFO\u001b[0m[0146] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[429.570317, "o", "\u001b[36mINFO\u001b[0m[0146] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[429.571094, "o", "\u001b[36mINFO\u001b[0m[0146] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[429.57145, "o", "\u001b[36mINFO\u001b[0m[0146] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[429.572316, "o", "\u001b[36mINFO\u001b[0m[0146] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[429.574344, "o", "\u001b[36mINFO\u001b[0m[0146] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[429.753624, "o", "\u001b[36mINFO\u001b[0m[0147] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n"]
[429.755881, "o", "\u001b[36mINFO\u001b[0m[0147] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.12] \r\n"]
[429.768738, "o", "\u001b[36mINFO\u001b[0m[0147] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[429.769917, "o", "\u001b[36mINFO\u001b[0m[0147] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.11] \r\n"]
[429.801744, "o", "\u001b[36mINFO\u001b[0m[0147] Starting container [kube-scheduler] on host [10.50.2.12], try #1 \r\n"]
[429.809567, "o", "\u001b[36mINFO\u001b[0m[0147] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[429.811135, "o", "\u001b[36mINFO\u001b[0m[0147] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[429.823641, "o", "\u001b[36mINFO\u001b[0m[0147] Starting container [kube-scheduler] on host [10.50.2.11], try #1 \r\n"]
[429.983285, "o", "\u001b[36mINFO\u001b[0m[0147] [controlplane] Successfully started [kube-scheduler] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0147] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.50.2.12] \r\n"]
[430.009256, "o", "\u001b[36mINFO\u001b[0m[0147] [controlplane] Successfully started [kube-scheduler] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0147] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.50.2.11] \r\n"]
[430.039282, "o", "\u001b[36mINFO\u001b[0m[0147] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n"]
[430.041005, "o", "\u001b[36mINFO\u001b[0m[0147] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.10] \r\n"]
[430.167846, "o", "\u001b[36mINFO\u001b[0m[0147] Starting container [kube-scheduler] on host [10.50.2.10], try #1 \r\n"]
[430.352752, "o", "\u001b[36mINFO\u001b[0m[0147] [controlplane] Successfully started [kube-scheduler] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0147] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.50.2.10] \r\n"]
[436.008828, "o", "\u001b[36mINFO\u001b[0m[0153] [healthcheck] service [kube-scheduler] on host [10.50.2.11] is healthy \r\n"]
[436.012105, "o", "\u001b[36mINFO\u001b[0m[0153] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[436.033762, "o", "\u001b[36mINFO\u001b[0m[0153] [healthcheck] service [kube-scheduler] on host [10.50.2.12] is healthy \r\n"]
[436.036643, "o", "\u001b[36mINFO\u001b[0m[0153] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[436.189065, "o", "\u001b[36mINFO\u001b[0m[0153] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[436.189199, "o", "\u001b[36mINFO\u001b[0m[0153] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[436.315811, "o", "\u001b[36mINFO\u001b[0m[0153] [healthcheck] service [kube-scheduler] on host [10.50.2.10] is healthy \r\n"]
[436.319324, "o", "\u001b[36mINFO\u001b[0m[0153] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[436.439669, "o", "\u001b[36mINFO\u001b[0m[0153] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[436.450387, "o", "\u001b[36mINFO\u001b[0m[0153] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[436.451305, "o", "\u001b[36mINFO\u001b[0m[0153] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[436.474154, "o", "\u001b[36mINFO\u001b[0m[0153] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[436.474881, "o", "\u001b[36mINFO\u001b[0m[0153] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[436.665114, "o", "\u001b[36mINFO\u001b[0m[0153] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n"]
[436.677255, "o", "\u001b[36mINFO\u001b[0m[0153] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[436.686592, "o", "\u001b[36mINFO\u001b[0m[0153] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[436.687202, "o", "\u001b[36mINFO\u001b[0m[0153] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[436.883275, "o", "\u001b[36mINFO\u001b[0m[0154] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0154] [controlplane] Successfully started Controller Plane.. \r\n"]
[436.884659, "o", "\u001b[36mINFO\u001b[0m[0154] [authz] Creating rke-job-deployer ServiceAccount \r\n"]
[436.913321, "o", "\u001b[36mINFO\u001b[0m[0154] [authz] rke-job-deployer ServiceAccount created successfully \r\n\u001b[36mINFO\u001b[0m[0154] [authz] Creating system:node ClusterRoleBinding \r\n"]
[436.936045, "o", "\u001b[36mINFO\u001b[0m[0154] [authz] system:node ClusterRoleBinding created successfully \r\n\u001b[36mINFO\u001b[0m[0154] [authz] Creating kube-apiserver proxy ClusterRole and ClusterRoleBinding \r\n"]
[436.94836, "o", "\u001b[36mINFO\u001b[0m[0154] [authz] kube-apiserver proxy ClusterRole and ClusterRoleBinding created successfully \r\n"]
[436.950231, "o", "\u001b[36mINFO\u001b[0m[0154] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/u1.rkestate] \r\n"]
[436.952552, "o", "\u001b[36mINFO\u001b[0m[0154] [state] Saving full cluster state to Kubernetes \r\n"]
[436.973704, "o", "\u001b[36mINFO\u001b[0m[0154] [state] Successfully Saved full cluster state to Kubernetes ConfigMap: full-cluster-state \r\n"]
[436.975882, "o", "\u001b[36mINFO\u001b[0m[0154] [worker] Building up Worker Plane..          \r\n"]
[436.976054, "o", "\u001b[36mINFO\u001b[0m[0154] Checking if container [service-sidekick] is running on host [10.50.2.10], try #1 \r\n\u001b[36mINFO\u001b[0m[0154] Checking if container [service-sidekick] is running on host [10.50.2.11], try #1 \r\n"]
[436.976257, "o", "\u001b[36mINFO\u001b[0m[0154] Checking if container [service-sidekick] is running on host [10.50.2.12], try #1 \r\n"]
[436.980843, "o", "\u001b[36mINFO\u001b[0m[0154] [sidekick] Sidekick container already created on host [10.50.2.10] \r\n"]
[436.980962, "o", "\u001b[36mINFO\u001b[0m[0154] [sidekick] Sidekick container already created on host [10.50.2.11] \r\n"]
[436.981086, "o", "\u001b[36mINFO\u001b[0m[0154] [sidekick] Sidekick container already created on host [10.50.2.12] \r\n"]
[436.981966, "o", "\u001b[36mINFO\u001b[0m[0154] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.10] \r\n"]
[436.982064, "o", "\u001b[36mINFO\u001b[0m[0154] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.11] \r\n"]
[436.982252, "o", "\u001b[36mINFO\u001b[0m[0154] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.12] \r\n"]
[437.0203, "o", "\u001b[36mINFO\u001b[0m[0154] Starting container [kubelet] on host [10.50.2.11], try #1 \r\n"]
[437.030382, "o", "\u001b[36mINFO\u001b[0m[0154] Starting container [kubelet] on host [10.50.2.12], try #1 \r\n"]
[437.036288, "o", "\u001b[36mINFO\u001b[0m[0154] Starting container [kubelet] on host [10.50.2.10], try #1 \r\n"]
[437.171661, "o", "\u001b[36mINFO\u001b[0m[0154] [worker] Successfully started [kubelet] container on host [10.50.2.12] \r\n"]
[437.171939, "o", "\u001b[36mINFO\u001b[0m[0154] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.12] \r\n"]
[437.178202, "o", "\u001b[36mINFO\u001b[0m[0154] [worker] Successfully started [kubelet] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0154] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.10] \r\n"]
[437.186061, "o", "\u001b[36mINFO\u001b[0m[0154] [worker] Successfully started [kubelet] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0154] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.11] \r\n"]
[437.680319, "o", "\u001b[36mINFO\u001b[0m[0154] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[437.698818, "o", "\u001b[36mINFO\u001b[0m[0154] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[437.705003, "o", "\u001b[36mINFO\u001b[0m[0154] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[437.992652, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [nginx-proxy] on host [10.50.2.21], try #1 \r\n"]
[438.206594, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [nginx-proxy] on host [10.50.2.22], try #1 \r\n"]
[438.211446, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [nginx-proxy] on host [10.50.2.20], try #1 \r\n"]
[438.2451, "o", "\u001b[36mINFO\u001b[0m[0155] [worker] Successfully started [nginx-proxy] container on host [10.50.2.21] \r\n"]
[438.250633, "o", "\u001b[36mINFO\u001b[0m[0155] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[438.409285, "o", "\u001b[36mINFO\u001b[0m[0155] [worker] Successfully started [nginx-proxy] container on host [10.50.2.22] \r\n"]
[438.411978, "o", "\u001b[36mINFO\u001b[0m[0155] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[438.414565, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[438.421191, "o", "\u001b[36mINFO\u001b[0m[0155] [worker] Successfully started [nginx-proxy] container on host [10.50.2.20] \r\n"]
[438.426901, "o", "\u001b[36mINFO\u001b[0m[0155] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[438.643386, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[438.649596, "o", "\u001b[36mINFO\u001b[0m[0155] Starting container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[438.715936, "o", "\u001b[36mINFO\u001b[0m[0155] [worker] Successfully started [rke-log-linker] container on host [10.50.2.21] \r\n"]
[438.717197, "o", "\u001b[36mINFO\u001b[0m[0155] Removing container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[439.029005, "o", "\u001b[36mINFO\u001b[0m[0156] [worker] Successfully started [rke-log-linker] container on host [10.50.2.22] \r\n"]
[439.030113, "o", "\u001b[36mINFO\u001b[0m[0156] [worker] Successfully started [rke-log-linker] container on host [10.50.2.20] \r\n"]
[439.030846, "o", "\u001b[36mINFO\u001b[0m[0156] Removing container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[439.032674, "o", "\u001b[36mINFO\u001b[0m[0156] Removing container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[439.075416, "o", "\u001b[36mINFO\u001b[0m[0156] [remove/rke-log-linker] Successfully removed container on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0156] Checking if container [service-sidekick] is running on host [10.50.2.21], try #1 \r\n"]
[439.08375, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[439.379832, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.21] \r\n"]
[439.466147, "o", "\u001b[36mINFO\u001b[0m[0156] Starting container [kubelet] on host [10.50.2.21], try #1 \r\n"]
[439.474667, "o", "\u001b[36mINFO\u001b[0m[0156] [remove/rke-log-linker] Successfully removed container on host [10.50.2.20] \r\n"]
[439.474796, "o", "\u001b[36mINFO\u001b[0m[0156] Checking if container [service-sidekick] is running on host [10.50.2.20], try #1 \r\n\u001b[36mINFO\u001b[0m[0156] [remove/rke-log-linker] Successfully removed container on host [10.50.2.22] \r\n\u001b[36mINFO\u001b[0m[0156] Checking if container [service-sidekick] is running on host [10.50.2.22], try #1 \r\n"]
[439.478005, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[439.480694, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[439.677988, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.20] \r\n"]
[439.681238, "o", "\u001b[36mINFO\u001b[0m[0156] [worker] Successfully started [kubelet] container on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0156] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.21] \r\n"]
[439.693337, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.22] \r\n"]
[439.739974, "o", "\u001b[36mINFO\u001b[0m[0157] Starting container [kubelet] on host [10.50.2.20], try #1 \r\n"]
[439.744779, "o", "\u001b[36mINFO\u001b[0m[0157] Starting container [kubelet] on host [10.50.2.22], try #1 \r\n"]
[439.857786, "o", "\u001b[36mINFO\u001b[0m[0157] [worker] Successfully started [kubelet] container on host [10.50.2.22] \r\n\u001b[36mINFO\u001b[0m[0157] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.22] \r\n"]
[439.873902, "o", "\u001b[36mINFO\u001b[0m[0157] [worker] Successfully started [kubelet] container on host [10.50.2.20] \r\n\u001b[36mINFO\u001b[0m[0157] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.20] \r\n"]
[448.737679, "o", "\u001b[36mINFO\u001b[0m[0166] [healthcheck] service [kubelet] on host [10.50.2.10] is healthy \r\n"]
[448.743467, "o", "\u001b[36mINFO\u001b[0m[0166] [healthcheck] service [kubelet] on host [10.50.2.11] is healthy \r\n"]
[448.746426, "o", "\u001b[36mINFO\u001b[0m[0166] [healthcheck] service [kubelet] on host [10.50.2.12] is healthy \r\n"]
[448.752596, "o", "\u001b[36mINFO\u001b[0m[0166] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[448.758509, "o", "\u001b[36mINFO\u001b[0m[0166] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[448.764576, "o", "\u001b[36mINFO\u001b[0m[0166] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[449.060464, "o", "\u001b[36mINFO\u001b[0m[0166] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[449.061772, "o", "\u001b[36mINFO\u001b[0m[0166] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[449.077399, "o", "\u001b[36mINFO\u001b[0m[0166] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[449.34648, "o", "\u001b[36mINFO\u001b[0m[0166] [worker] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[449.347484, "o", "\u001b[36mINFO\u001b[0m[0166] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[449.362794, "o", "\u001b[36mINFO\u001b[0m[0166] [worker] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[449.3639, "o", "\u001b[36mINFO\u001b[0m[0166] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[449.385734, "o", "\u001b[36mINFO\u001b[0m[0166] [worker] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[449.386524, "o", "\u001b[36mINFO\u001b[0m[0166] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[449.547666, "o", "\u001b[36mINFO\u001b[0m[0166] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n"]
[449.548829, "o", "\u001b[36mINFO\u001b[0m[0166] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.10] \r\n"]
[449.567466, "o", "\u001b[36mINFO\u001b[0m[0166] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n"]
[449.568804, "o", "\u001b[36mINFO\u001b[0m[0166] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.12] \r\n"]
[449.590843, "o", "\u001b[36mINFO\u001b[0m[0166] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[449.592348, "o", "\u001b[36mINFO\u001b[0m[0166] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.11] \r\n"]
[449.594266, "o", "\u001b[36mINFO\u001b[0m[0166] Starting container [kube-proxy] on host [10.50.2.10], try #1 \r\n"]
[449.618772, "o", "\u001b[36mINFO\u001b[0m[0166] Starting container [kube-proxy] on host [10.50.2.12], try #1 \r\n"]
[449.640422, "o", "\u001b[36mINFO\u001b[0m[0166] Starting container [kube-proxy] on host [10.50.2.11], try #1 \r\n"]
[449.724821, "o", "\u001b[36mINFO\u001b[0m[0166] [worker] Successfully started [kube-proxy] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0166] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.12] \r\n"]
[449.727307, "o", "\u001b[36mINFO\u001b[0m[0166] [worker] Successfully started [kube-proxy] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0166] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.10] \r\n"]
[449.850266, "o", "\u001b[36mINFO\u001b[0m[0167] [worker] Successfully started [kube-proxy] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0167] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.11] \r\n"]
[450.254663, "o", "\u001b[36mINFO\u001b[0m[0167] [healthcheck] service [kube-proxy] on host [10.50.2.10] is healthy \r\n"]
[450.257186, "o", "\u001b[36mINFO\u001b[0m[0167] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[450.384088, "o", "\u001b[36mINFO\u001b[0m[0167] [healthcheck] service [kube-proxy] on host [10.50.2.11] is healthy \r\n"]
[450.387725, "o", "\u001b[36mINFO\u001b[0m[0167] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[450.390739, "o", "\u001b[36mINFO\u001b[0m[0167] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[450.516624, "o", "\u001b[36mINFO\u001b[0m[0167] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[450.873364, "o", "\u001b[36mINFO\u001b[0m[0168] [worker] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[450.874198, "o", "\u001b[36mINFO\u001b[0m[0168] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[450.983892, "o", "\u001b[36mINFO\u001b[0m[0168] [worker] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[450.986387, "o", "\u001b[36mINFO\u001b[0m[0168] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[451.060665, "o", "\u001b[36mINFO\u001b[0m[0168] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n"]
[451.168356, "o", "\u001b[36mINFO\u001b[0m[0168] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[451.226661, "o", "\u001b[36mINFO\u001b[0m[0168] [healthcheck] service [kubelet] on host [10.50.2.21] is healthy \r\n"]
[451.22975, "o", "\u001b[36mINFO\u001b[0m[0168] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[451.340753, "o", "\u001b[36mINFO\u001b[0m[0168] [healthcheck] service [kubelet] on host [10.50.2.22] is healthy \r\n"]
[451.343269, "o", "\u001b[36mINFO\u001b[0m[0168] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[451.357594, "o", "\u001b[36mINFO\u001b[0m[0168] [healthcheck] service [kubelet] on host [10.50.2.20] is healthy \r\n"]
[451.359254, "o", "\u001b[36mINFO\u001b[0m[0168] Starting container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[451.360486, "o", "\u001b[36mINFO\u001b[0m[0168] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[451.527091, "o", "\u001b[36mINFO\u001b[0m[0168] Starting container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[451.527961, "o", "\u001b[36mINFO\u001b[0m[0168] Starting container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[451.6072, "o", "\u001b[36mINFO\u001b[0m[0168] [worker] Successfully started [rke-log-linker] container on host [10.50.2.21] \r\n"]
[451.608042, "o", "\u001b[36mINFO\u001b[0m[0168] Removing container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[451.750318, "o", "\u001b[36mINFO\u001b[0m[0169] [worker] Successfully started [rke-log-linker] container on host [10.50.2.22] \r\n"]
[451.753174, "o", "\u001b[36mINFO\u001b[0m[0169] Removing container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[451.765231, "o", "\u001b[36mINFO\u001b[0m[0169] [worker] Successfully started [rke-log-linker] container on host [10.50.2.20] \r\n"]
[451.765871, "o", "\u001b[36mINFO\u001b[0m[0169] Removing container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[451.788227, "o", "\u001b[36mINFO\u001b[0m[0169] [remove/rke-log-linker] Successfully removed container on host [10.50.2.21] \r\n"]
[451.789844, "o", "\u001b[36mINFO\u001b[0m[0169] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.21] \r\n"]
[451.830467, "o", "\u001b[36mINFO\u001b[0m[0169] Starting container [kube-proxy] on host [10.50.2.21], try #1 \r\n"]
[451.922053, "o", "\u001b[36mINFO\u001b[0m[0169] [worker] Successfully started [kube-proxy] container on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0169] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.21] \r\n"]
[451.960881, "o", "\u001b[36mINFO\u001b[0m[0169] [remove/rke-log-linker] Successfully removed container on host [10.50.2.22] \r\n"]
[451.962594, "o", "\u001b[36mINFO\u001b[0m[0169] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.22] \r\n"]
[451.963509, "o", "\u001b[36mINFO\u001b[0m[0169] [remove/rke-log-linker] Successfully removed container on host [10.50.2.20] \r\n"]
[451.964926, "o", "\u001b[36mINFO\u001b[0m[0169] Image [10.8.101.2:5000/rancher/hyperkube:v1.18.12-rancher1] exists on host [10.50.2.20] \r\n"]
[452.011055, "o", "\u001b[36mINFO\u001b[0m[0169] Starting container [kube-proxy] on host [10.50.2.22], try #1 \r\n"]
[452.021009, "o", "\u001b[36mINFO\u001b[0m[0169] Starting container [kube-proxy] on host [10.50.2.20], try #1 \r\n"]
[452.113151, "o", "\u001b[36mINFO\u001b[0m[0169] [worker] Successfully started [kube-proxy] container on host [10.50.2.22] \r\n\u001b[36mINFO\u001b[0m[0169] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.22] \r\n"]
[452.1633, "o", "\u001b[36mINFO\u001b[0m[0169] [worker] Successfully started [kube-proxy] container on host [10.50.2.20] \r\n\u001b[36mINFO\u001b[0m[0169] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.20] \r\n"]
[452.440771, "o", "\u001b[36mINFO\u001b[0m[0169] [healthcheck] service [kube-proxy] on host [10.50.2.21] is healthy \r\n"]
[452.44468, "o", "\u001b[36mINFO\u001b[0m[0169] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[452.578704, "o", "\u001b[36mINFO\u001b[0m[0169] Starting container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[452.64865, "o", "\u001b[36mINFO\u001b[0m[0169] [healthcheck] service [kube-proxy] on host [10.50.2.22] is healthy \r\n"]
[452.651315, "o", "\u001b[36mINFO\u001b[0m[0169] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[452.679634, "o", "\u001b[36mINFO\u001b[0m[0169] [healthcheck] service [kube-proxy] on host [10.50.2.20] is healthy \r\n"]
[452.682506, "o", "\u001b[36mINFO\u001b[0m[0169] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[452.838737, "o", "\u001b[36mINFO\u001b[0m[0170] Starting container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[452.841983, "o", "\u001b[36mINFO\u001b[0m[0170] Starting container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[453.063105, "o", "\u001b[36mINFO\u001b[0m[0170] [worker] Successfully started [rke-log-linker] container on host [10.50.2.21] \r\n"]
[453.064457, "o", "\u001b[36mINFO\u001b[0m[0170] Removing container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[453.257986, "o", "\u001b[36mINFO\u001b[0m[0170] [remove/rke-log-linker] Successfully removed container on host [10.50.2.21] \r\n"]
[453.334033, "o", "\u001b[36mINFO\u001b[0m[0170] [worker] Successfully started [rke-log-linker] container on host [10.50.2.22] \r\n"]
[453.335033, "o", "\u001b[36mINFO\u001b[0m[0170] Removing container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[453.343042, "o", "\u001b[36mINFO\u001b[0m[0170] [worker] Successfully started [rke-log-linker] container on host [10.50.2.20] \r\n"]
[453.344249, "o", "\u001b[36mINFO\u001b[0m[0170] Removing container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[453.604243, "o", "\u001b[36mINFO\u001b[0m[0170] [remove/rke-log-linker] Successfully removed container on host [10.50.2.22] \r\n"]
[453.60704, "o", "\u001b[36mINFO\u001b[0m[0170] [remove/rke-log-linker] Successfully removed container on host [10.50.2.20] \r\n"]
[455.731624, "o", "\u001b[36mINFO\u001b[0m[0173] [healthcheck] service [kube-proxy] on host [10.50.2.12] is healthy \r\n"]
[455.735546, "o", "\u001b[36mINFO\u001b[0m[0173] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[455.916, "o", "\u001b[36mINFO\u001b[0m[0173] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[456.335253, "o", "\u001b[36mINFO\u001b[0m[0173] [worker] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[456.337948, "o", "\u001b[36mINFO\u001b[0m[0173] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[456.540596, "o", "\u001b[36mINFO\u001b[0m[0173] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0173] [worker] Successfully started Worker Plane.. \r\n"]
[456.545309, "o", "\u001b[36mINFO\u001b[0m[0173] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0173] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[456.546449, "o", "\u001b[36mINFO\u001b[0m[0173] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[456.547526, "o", "\u001b[36mINFO\u001b[0m[0173] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[456.547759, "o", "\u001b[36mINFO\u001b[0m[0173] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[456.551175, "o", "\u001b[36mINFO\u001b[0m[0173] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[456.953849, "o", "\u001b[36mINFO\u001b[0m[0174] Starting container [rke-log-cleaner] on host [10.50.2.21], try #1 \r\n"]
[456.954916, "o", "\u001b[36mINFO\u001b[0m[0174] Starting container [rke-log-cleaner] on host [10.50.2.20], try #1 \r\n"]
[456.960608, "o", "\u001b[36mINFO\u001b[0m[0174] Starting container [rke-log-cleaner] on host [10.50.2.22], try #1 \r\n\u001b[36mINFO\u001b[0m[0174] Starting container [rke-log-cleaner] on host [10.50.2.10], try #1 \r\n"]
[456.961845, "o", "\u001b[36mINFO\u001b[0m[0174] Starting container [rke-log-cleaner] on host [10.50.2.12], try #1 \r\n"]
[456.964249, "o", "\u001b[36mINFO\u001b[0m[0174] Starting container [rke-log-cleaner] on host [10.50.2.11], try #1 \r\n"]
[457.505893, "o", "\u001b[36mINFO\u001b[0m[0174] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.22] \r\n"]
[457.508743, "o", "\u001b[36mINFO\u001b[0m[0174] Removing container [rke-log-cleaner] on host [10.50.2.22], try #1 \r\n"]
[457.516182, "o", "\u001b[36mINFO\u001b[0m[0174] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.20] \r\n"]
[457.518201, "o", "\u001b[36mINFO\u001b[0m[0174] Removing container [rke-log-cleaner] on host [10.50.2.20], try #1 \r\n"]
[457.52861, "o", "\u001b[36mINFO\u001b[0m[0174] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.10] \r\n"]
[457.530482, "o", "\u001b[36mINFO\u001b[0m[0174] Removing container [rke-log-cleaner] on host [10.50.2.10], try #1 \r\n"]
[457.536036, "o", "\u001b[36mINFO\u001b[0m[0174] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.11] \r\n"]
[457.537441, "o", "\u001b[36mINFO\u001b[0m[0174] Removing container [rke-log-cleaner] on host [10.50.2.11], try #1 \r\n"]
[457.538185, "o", "\u001b[36mINFO\u001b[0m[0174] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.21] \r\n"]
[457.541159, "o", "\u001b[36mINFO\u001b[0m[0174] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.12] \r\n"]
[457.542728, "o", "\u001b[36mINFO\u001b[0m[0174] Removing container [rke-log-cleaner] on host [10.50.2.21], try #1 \r\n"]
[457.544703, "o", "\u001b[36mINFO\u001b[0m[0174] Removing container [rke-log-cleaner] on host [10.50.2.12], try #1 \r\n"]
[457.724808, "o", "\u001b[36mINFO\u001b[0m[0174] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.20] \r\n"]
[457.735629, "o", "\u001b[36mINFO\u001b[0m[0175] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.22] \r\n"]
[457.742785, "o", "\u001b[36mINFO\u001b[0m[0175] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.10] \r\n"]
[457.743632, "o", "\u001b[36mINFO\u001b[0m[0175] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.11] \r\n"]
[457.748827, "o", "\u001b[36mINFO\u001b[0m[0175] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.12] \r\n"]
[457.751918, "o", "\u001b[36mINFO\u001b[0m[0175] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0175] [sync] Syncing nodes Labels and Taints       \r\n"]
[458.174975, "o", "\u001b[36mINFO\u001b[0m[0175] [sync] Successfully synced nodes Labels and Taints \r\n"]
[458.175334, "o", "\u001b[36mINFO\u001b[0m[0175] [network] Setting up network plugin: calico  \r\n"]
[458.17732, "o", "\u001b[36mINFO\u001b[0m[0175] [addons] Saving ConfigMap for addon rke-network-plugin to Kubernetes \r\n"]
[458.22493, "o", "\u001b[36mINFO\u001b[0m[0175] [addons] Successfully saved ConfigMap for addon rke-network-plugin to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0175] [addons] Executing deploy job rke-network-plugin \r\n"]
[468.329462, "o", "\u001b[36mINFO\u001b[0m[0185] [addons] Setting up coredns                  \r\n"]
[468.329858, "o", "\u001b[36mINFO\u001b[0m[0185] [addons] Saving ConfigMap for addon rke-coredns-addon to Kubernetes \r\n"]
[468.353152, "o", "\u001b[36mINFO\u001b[0m[0185] [addons] Successfully saved ConfigMap for addon rke-coredns-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0185] [addons] Executing deploy job rke-coredns-addon \r\n"]
[473.375947, "o", "\u001b[36mINFO\u001b[0m[0190] [addons] CoreDNS deployed successfully       \r\n\u001b[36mINFO\u001b[0m[0190] [dns] DNS provider coredns deployed successfully \r\n"]
[473.378508, "o", "\u001b[36mINFO\u001b[0m[0190] [addons] Setting up Metrics Server           \r\n"]
[473.379555, "o", "\u001b[36mINFO\u001b[0m[0190] [addons] Saving ConfigMap for addon rke-metrics-addon to Kubernetes \r\n"]
[473.388243, "o", "\u001b[36mINFO\u001b[0m[0190] [addons] Successfully saved ConfigMap for addon rke-metrics-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0190] [addons] Executing deploy job rke-metrics-addon \r\n"]
[478.462957, "o", "\u001b[36mINFO\u001b[0m[0195] [addons] Metrics Server deployed successfully \r\n\u001b[36mINFO\u001b[0m[0195] [ingress] Setting up nginx ingress controller \r\n\u001b[36mINFO\u001b[0m[0195] [addons] Saving ConfigMap for addon rke-ingress-controller to Kubernetes \r\n"]
[478.521227, "o", "\u001b[36mINFO\u001b[0m[0195] [addons] Successfully saved ConfigMap for addon rke-ingress-controller to Kubernetes \r\n"]
[478.521641, "o", "\u001b[36mINFO\u001b[0m[0195] [addons] Executing deploy job rke-ingress-controller \r\n"]
[483.578688, "o", "\u001b[36mINFO\u001b[0m[0200] [ingress] ingress controller nginx deployed successfully \r\n\u001b[36mINFO\u001b[0m[0200] [addons] Setting up user addons              \r\n"]
[483.589402, "o", "\u001b[36mINFO\u001b[0m[0200] [addons] no user addons defined              \r\n\u001b[36mINFO\u001b[0m[0200] Finished building Kubernetes cluster successfully \r\n"]
[483.606903, "o", "\r\nreal\t3m21.134s\r\nuser\t0m3.167s\r\nsys\t0m0.564s\r\n"]
[514.187871, "o", "NAME              STATUS   ROLES               AGE   VERSION\r\nnode/10.50.2.10   Ready    controlplane,etcd   68s   v1.18.12\r\nnode/10.50.2.11   Ready    controlplane,etcd   68s   v1.18.12\r\nnode/10.50.2.12   Ready    controlplane,etcd   68s   v1.18.12\r\nnode/10.50.2.20   Ready    worker              67s   v1.18.12\r\nnode/10.50.2.21   Ready    worker              67s   v1.18.12\r\nnode/10.50.2.22   Ready    worker              67s   v1.18.12\r\n"]
[514.201036, "o", "\r\nNAMESPACE       NAME                                           READY   STATUS      RESTARTS   AGE\r\ningress-nginx   pod/default-http-backend-5b564dd459-stjgz      1/1     Running     0          32s\r\ningress-nginx   pod/nginx-ingress-controller-blz4j             1/1     Running     0          32s\r\ningress-nginx   pod/nginx-ingress-controller-cj8nd             1/1     Running     0          "]
[514.201086, "o", "32s\r\ningress-nginx   pod/nginx-ingress-controller-fqjdh             1/1     Running     0          32s\r\nkube-system     pod/calico-kube-controllers-6c6fc476f6-82wzb   1/1     Running     0          48s\r\nkube-system     pod/calico-node-7dz2m                "]
[514.2011, "o", "          1/1     Running     0        "]
[514.201306, "o", "  47s\r\nkube-system     pod/calico-node-cm5rc                          0/1     Running     0          47s\r\nkube-system     pod/calico-node-dmwbz                          0/1     Running     0          47s\r\nkube-system     pod/calico-node-kf6zb                          0/1     Running     0          47s\r\nkube-system     pod/calico-node-nczbb                          0/1     Running     0          47s\r\nkube-system     pod/calico-node-zjtmj                          1/1     Running     0          47s\r\nkube-system     pod/coredns-5dd4dfcb45-lvlc4                   1/1     Running     0          10s\r\nkube-system     pod/coredns-5dd4dfcb45-sj44t                   1/1     Running     0          44s\r\nkube-system     pod/coredns-autoscaler-557f965569-4sbdd        1/1     Running     0"]
[514.201342, "o", "          43s\r\nkube-system     pod/metrics-server-77956db857-pdklq            1/1     Running     0          38s\r\nkube-system     pod/rke-coredns-addon-deploy-job-2h25s         "]
[514.201356, "o", "0/1     Completed   0          46s\r\nkube-system"]
[514.201836, "o", "     pod/rke-ingress-controller-deploy-job-9882t    0/1     Completed   0          36s\r\nkube-system     pod/rke-metrics-addon-deploy-job-l47wt         0/1     Completed   0          41s\r\nkube-system     pod/rke-network-plugin-deploy-job-xzrwp        0/1     Completed   0          56s\r\n"]
[514.404732, "o", "cd /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/ && tee /home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/u1.yml </home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/u1-v1.19.4-rancher1-1.yml \\\r\n            && ./files/rke up --config=/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/u1.yml\r\n"]
[514.490629, "o", "---\r\ncluster_name: u1\r\nkubernetes_version: v1.19.4-rancher1-1\r\n\r\nssh_agent_auth: true\r\n\r\nnodes:\r\n  - &m1\r\n    address: 10.50.2.10\r\n    user: ubuntu\r\n    role: [etcd, controlplane]\r\n  - <<: *m1\r\n    address: 10.50.2.11\r\n  - <<: *m1\r\n    address: 10.50.2.12\r\n\r\n  - &n1\r\n    address: 10.50.2.20\r\n    user: ubuntu\r\n    role: [worker]\r\n  - <<: *n1\r\n    address: 10.50.2.21\r\n  - <<: *n1\r\n    address: 10.50.2.22\r\n\r\nnetwork:\r\n  plugin: calico\r\n\r\nprivate_registries:\r\n  - url: 10.8.101.2:5000\r\n    is_default: true\r\n"]
[514.511257, "o", "\u001b[36mINFO\u001b[0m[0000] Running RKE version: v1.2.3                  \r\n"]
[514.512028, "o", "\u001b[36mINFO\u001b[0m[0000] Initiating Kubernetes cluster                \r\n"]
[514.535396, "o", "\u001b[36mINFO\u001b[0m[0000] [certificates] GenerateServingCertificate is disabled, checking if there are unused kubelet certificates \r\n\u001b[36mINFO\u001b[0m[0000] [certificates] Generating admin certificates and kubeconfig \r\n"]
[514.538589, "o", "\u001b[36mINFO\u001b[0m[0000] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/u1.rkestate] \r\n"]
[514.541517, "o", "\u001b[36mINFO\u001b[0m[0000] Building Kubernetes cluster                  \r\n"]
[514.54162, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.10]  \r\n\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.20]  \r\n\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.21]  \r\n"]
[514.541713, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.22]  \r\n\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.11]  \r\n"]
[514.541823, "o", "\u001b[36mINFO\u001b[0m[0000] [dialer] Setup tunnel for host [10.50.2.12]  \r\n"]
[515.201984, "o", "\u001b[36mINFO\u001b[0m[0000] [network] No hosts added existing cluster, skipping port check \r\n\u001b[36mINFO\u001b[0m[0000] [certificates] Deploying kubernetes certificates to Cluster nodes \r\n"]
[515.202127, "o", "\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.50.2.20], try #1 \r\n"]
[515.202227, "o", "\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.50.2.21], try #1 \r\n\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.50.2.22], try #1 \r\n"]
[515.202412, "o", "\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.50.2.12], try #1 \r\n\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.50.2.11], try #1 \r\n"]
[515.202493, "o", "\u001b[36mINFO\u001b[0m[0000] Checking if container [cert-deployer] is running on host [10.50.2.10], try #1 \r\n"]
[515.210811, "o", "\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[515.213259, "o", "\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[515.217241, "o", "\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[515.223519, "o", "\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[515.233511, "o", "\u001b[36mINFO\u001b[0m[0000] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[515.599757, "o", "\u001b[36mINFO\u001b[0m[0001] Starting container [cert-deployer] on host [10.50.2.21], try #1 \r\n"]
[515.601889, "o", "\u001b[36mINFO\u001b[0m[0001] Starting container [cert-deployer] on host [10.50.2.20], try #1 \r\n"]
[515.604227, "o", "\u001b[36mINFO\u001b[0m[0001] Starting container [cert-deployer] on host [10.50.2.22], try #1 \r\n"]
[516.067406, "o", "\u001b[36mINFO\u001b[0m[0001] Starting container [cert-deployer] on host [10.50.2.11], try #1 \r\n"]
[516.079789, "o", "\u001b[36mINFO\u001b[0m[0001] Starting container [cert-deployer] on host [10.50.2.10], try #1 \r\n"]
[516.10095, "o", "\u001b[36mINFO\u001b[0m[0001] Starting container [cert-deployer] on host [10.50.2.12], try #1 \r\n"]
[516.622158, "o", "\u001b[36mINFO\u001b[0m[0002] Checking if container [cert-deployer] is running on host [10.50.2.22], try #1 \r\n"]
[516.644288, "o", "\u001b[36mINFO\u001b[0m[0002] Checking if container [cert-deployer] is running on host [10.50.2.20], try #1 \r\n"]
[516.651619, "o", "\u001b[36mINFO\u001b[0m[0002] Checking if container [cert-deployer] is running on host [10.50.2.11], try #1 \r\n"]
[516.658228, "o", "\u001b[36mINFO\u001b[0m[0002] Checking if container [cert-deployer] is running on host [10.50.2.12], try #1 \r\n"]
[516.666134, "o", "\u001b[36mINFO\u001b[0m[0002] Checking if container [cert-deployer] is running on host [10.50.2.21], try #1 \r\n"]
[516.687263, "o", "\u001b[36mINFO\u001b[0m[0002] Checking if container [cert-deployer] is running on host [10.50.2.10], try #1 \r\n"]
[521.627301, "o", "\u001b[36mINFO\u001b[0m[0007] Checking if container [cert-deployer] is running on host [10.50.2.22], try #1 \r\n"]
[521.63678, "o", "\u001b[36mINFO\u001b[0m[0007] Removing container [cert-deployer] on host [10.50.2.22], try #1 \r\n"]
[521.655963, "o", "\u001b[36mINFO\u001b[0m[0007] Checking if container [cert-deployer] is running on host [10.50.2.20], try #1 \r\n"]
[521.668344, "o", "\u001b[36mINFO\u001b[0m[0007] Checking if container [cert-deployer] is running on host [10.50.2.11], try #1 \r\n"]
[521.669858, "o", "\u001b[36mINFO\u001b[0m[0007] Checking if container [cert-deployer] is running on host [10.50.2.12], try #1 \r\n"]
[521.672535, "o", "\u001b[36mINFO\u001b[0m[0007] Checking if container [cert-deployer] is running on host [10.50.2.21], try #1 \r\n\u001b[36mINFO\u001b[0m[0007] Removing container [cert-deployer] on host [10.50.2.20], try #1 \r\n"]
[521.678233, "o", "\u001b[36mINFO\u001b[0m[0007] Removing container [cert-deployer] on host [10.50.2.12], try #1 \r\n"]
[521.680322, "o", "\u001b[36mINFO\u001b[0m[0007] Removing container [cert-deployer] on host [10.50.2.11], try #1 \r\n"]
[521.684041, "o", "\u001b[36mINFO\u001b[0m[0007] Removing container [cert-deployer] on host [10.50.2.21], try #1 \r\n"]
[521.702673, "o", "\u001b[36mINFO\u001b[0m[0007] Checking if container [cert-deployer] is running on host [10.50.2.10], try #1 \r\n"]
[521.705725, "o", "\u001b[36mINFO\u001b[0m[0007] Removing container [cert-deployer] on host [10.50.2.10], try #1 \r\n"]
[521.716188, "o", "\u001b[36mINFO\u001b[0m[0007] [reconcile] Rebuilding and updating local kube config \r\n"]
[521.716311, "o", "\u001b[36mINFO\u001b[0m[0007] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_u1.yml] \r\n"]
[521.723045, "o", "\u001b[36mINFO\u001b[0m[0007] [reconcile] host [10.50.2.10] is active master on the cluster \r\n\u001b[36mINFO\u001b[0m[0007] [certificates] Successfully deployed kubernetes certificates to Cluster nodes \r\n"]
[521.723131, "o", "\u001b[36mINFO\u001b[0m[0007] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.50.2.10] \r\n"]
[521.724955, "o", "\u001b[36mINFO\u001b[0m[0007] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[521.849606, "o", "\u001b[36mINFO\u001b[0m[0007] Starting container [file-deployer] on host [10.50.2.10], try #1 \r\n"]
[522.3858, "o", "\u001b[36mINFO\u001b[0m[0007] Successfully started [file-deployer] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0007] Waiting for [file-deployer] container to exit on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0007] Waiting for [file-deployer] container to exit on host [10.50.2.10] \r\n"]
[522.387106, "o", "\u001b[36mINFO\u001b[0m[0007] Container [file-deployer] is still running on host [10.50.2.10]: stderr: [], stdout: [] \r\n"]
[523.387241, "o", "\u001b[36mINFO\u001b[0m[0008] Waiting for [file-deployer] container to exit on host [10.50.2.10] \r\n"]
[523.389421, "o", "\u001b[36mINFO\u001b[0m[0008] Removing container [file-deployer] on host [10.50.2.10], try #1 \r\n"]
[523.398021, "o", "\u001b[36mINFO\u001b[0m[0008] [remove/file-deployer] Successfully removed container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0008] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.50.2.11] \r\n"]
[523.399995, "o", "\u001b[36mINFO\u001b[0m[0008] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[523.535897, "o", "\u001b[36mINFO\u001b[0m[0009] Starting container [file-deployer] on host [10.50.2.11], try #1 \r\n"]
[524.099604, "o", "\u001b[36mINFO\u001b[0m[0009] Successfully started [file-deployer] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0009] Waiting for [file-deployer] container to exit on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0009] Waiting for [file-deployer] container to exit on host [10.50.2.11] \r\n"]
[524.100996, "o", "\u001b[36mINFO\u001b[0m[0009] Container [file-deployer] is still running on host [10.50.2.11]: stderr: [], stdout: [] \r\n"]
[525.101521, "o", "\u001b[36mINFO\u001b[0m[0010] Waiting for [file-deployer] container to exit on host [10.50.2.11] \r\n"]
[525.108495, "o", "\u001b[36mINFO\u001b[0m[0010] Removing container [file-deployer] on host [10.50.2.11], try #1 \r\n"]
[525.123094, "o", "\u001b[36mINFO\u001b[0m[0010] [remove/file-deployer] Successfully removed container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0010] [file-deploy] Deploying file [/etc/kubernetes/audit-policy.yaml] to node [10.50.2.12] \r\n"]
[525.12623, "o", "\u001b[36mINFO\u001b[0m[0010] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[525.274068, "o", "\u001b[36mINFO\u001b[0m[0010] Starting container [file-deployer] on host [10.50.2.12], try #1 \r\n"]
[525.785714, "o", "\u001b[36mINFO\u001b[0m[0011] Successfully started [file-deployer] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0011] Waiting for [file-deployer] container to exit on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0011] Waiting for [file-deployer] container to exit on host [10.50.2.12] \r\n"]
[525.787172, "o", "\u001b[36mINFO\u001b[0m[0011] Container [file-deployer] is still running on host [10.50.2.12]: stderr: [], stdout: [] \r\n"]
[526.787281, "o", "\u001b[36mINFO\u001b[0m[0012] Waiting for [file-deployer] container to exit on host [10.50.2.12] \r\n"]
[526.788991, "o", "\u001b[36mINFO\u001b[0m[0012] Removing container [file-deployer] on host [10.50.2.12], try #1 \r\n"]
[526.797172, "o", "\u001b[36mINFO\u001b[0m[0012] [remove/file-deployer] Successfully removed container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0012] [/etc/kubernetes/audit-policy.yaml] Successfully deployed audit policy file to Cluster control nodes \r\n\u001b[36mINFO\u001b[0m[0012] [reconcile] Reconciling cluster state        \r\n"]
[526.797887, "o", "\u001b[36mINFO\u001b[0m[0012] [reconcile] Check etcd hosts to be deleted   \r\n"]
[526.798343, "o", "\u001b[36mINFO\u001b[0m[0012] [reconcile] Check etcd hosts to be added     \r\n"]
[526.798715, "o", "\u001b[36mINFO\u001b[0m[0012] [reconcile] Rebuilding and updating local kube config \r\n"]
[526.798883, "o", "\u001b[36mINFO\u001b[0m[0012] Successfully Deployed local admin kubeconfig at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/kube_config_u1.yml] \r\n"]
[526.800162, "o", "\u001b[36mINFO\u001b[0m[0012] [reconcile] host [10.50.2.10] is active master on the cluster \r\n\u001b[36mINFO\u001b[0m[0012] [reconcile] Reconciled cluster state successfully \r\n\u001b[36mINFO\u001b[0m[0012] max_unavailable_worker got rounded down to 0, resetting to 1 \r\n\u001b[36mINFO\u001b[0m[0012] Setting maxUnavailable for worker nodes to: 1 \r\n\u001b[36mINFO\u001b[0m[0012] Setting maxUnavailable for controlplane nodes to: 1 \r\n"]
[526.80019, "o", "\u001b[36mINFO\u001b[0m[0012] Pre-pulling kubernetes images                \r\n"]
[526.801429, "o", "\u001b[36mINFO\u001b[0m[0012] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.50.2.22], try #1 \r\n"]
[526.801588, "o", "\u001b[36mINFO\u001b[0m[0012] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.50.2.20], try #1 \r\n"]
[526.801672, "o", "\u001b[36mINFO\u001b[0m[0012] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.50.2.11], try #1 \r\n\u001b[36mINFO\u001b[0m[0012] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.50.2.21], try #1 \r\n\u001b[36mINFO\u001b[0m[0012] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.50.2.10], try #1 \r\n"]
[526.801958, "o", "\u001b[36mINFO\u001b[0m[0012] Pulling image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] on host [10.50.2.12], try #1 \r\n"]
[568.063407, "o", "\u001b[36mINFO\u001b[0m[0053] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.22] \r\n"]
[568.068682, "o", "\u001b[36mINFO\u001b[0m[0053] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.21] \r\n"]
[568.072085, "o", "\u001b[36mINFO\u001b[0m[0053] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.20] \r\n"]
[575.065513, "o", "\u001b[36mINFO\u001b[0m[0060] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.10] \r\n"]
[576.220579, "o", "\u001b[36mINFO\u001b[0m[0061] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.12] \r\n"]
[576.709442, "o", "\u001b[36mINFO\u001b[0m[0062] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0062] Kubernetes images pulled successfully        \r\n"]
[576.71191, "o", "\u001b[36mINFO\u001b[0m[0062] [etcd] Building up etcd plane..              \r\n"]
[576.732985, "o", "\u001b[36mINFO\u001b[0m[0062] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[577.144386, "o", "\u001b[36mINFO\u001b[0m[0062] Starting container [etcd-fix-perm] on host [10.50.2.10], try #1 \r\n"]
[577.718338, "o", "\u001b[36mINFO\u001b[0m[0063] Successfully started [etcd-fix-perm] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0063] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0063] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.10] \r\n"]
[577.721319, "o", "\u001b[36mINFO\u001b[0m[0063] Container [etcd-fix-perm] is still running on host [10.50.2.10]: stderr: [], stdout: [] \r\n"]
[578.721571, "o", "\u001b[36mINFO\u001b[0m[0064] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.10] \r\n"]
[578.726969, "o", "\u001b[36mINFO\u001b[0m[0064] Removing container [etcd-fix-perm] on host [10.50.2.10], try #1 \r\n"]
[578.751304, "o", "\u001b[36mINFO\u001b[0m[0064] [remove/etcd-fix-perm] Successfully removed container on host [10.50.2.10] \r\n"]
[578.758666, "o", "\u001b[36mINFO\u001b[0m[0064] Checking if container [etcd] is running on host [10.50.2.10], try #1 \r\n"]
[578.763917, "o", "\u001b[36mINFO\u001b[0m[0064] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] on host [10.50.2.10], try #1 \r\n"]
[579.545401, "o", "\u001b[36mINFO\u001b[0m[0065] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] exists on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0065] Checking if container [old-etcd] is running on host [10.50.2.10], try #1 \r\n"]
[579.552233, "o", "\u001b[36mINFO\u001b[0m[0065] Stopping container [etcd] on host [10.50.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[581.720916, "o", "\u001b[36mINFO\u001b[0m[0067] Waiting for [etcd] container to exit on host [10.50.2.10] \r\n"]
[581.722437, "o", "\u001b[36mINFO\u001b[0m[0067] Renaming container [etcd] to [old-etcd] on host [10.50.2.10], try #1 \r\n"]
[581.799009, "o", "\u001b[36mINFO\u001b[0m[0067] Starting container [etcd] on host [10.50.2.10], try #1 \r\n"]
[582.065183, "o", "\u001b[36mINFO\u001b[0m[0067] [etcd] Successfully updated [etcd] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0067] Removing container [old-etcd] on host [10.50.2.10], try #1 \r\n"]
[582.079611, "o", "\u001b[36mINFO\u001b[0m[0067] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.50.2.10] \r\n"]
[582.084478, "o", "\u001b[36mINFO\u001b[0m[0067] Removing container [etcd-rolling-snapshots] on host [10.50.2.10], try #1 \r\n"]
[582.25579, "o", "\u001b[36mINFO\u001b[0m[0067] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.50.2.10] \r\n"]
[582.258011, "o", "\u001b[36mINFO\u001b[0m[0067] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[582.39961, "o", "\u001b[36mINFO\u001b[0m[0067] Starting container [etcd-rolling-snapshots] on host [10.50.2.10], try #1 \r\n"]
[582.630641, "o", "\u001b[36mINFO\u001b[0m[0068] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.50.2.10] \r\n"]
[587.635407, "o", "\u001b[36mINFO\u001b[0m[0073] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[587.794976, "o", "\u001b[36mINFO\u001b[0m[0073] Starting container [rke-bundle-cert] on host [10.50.2.10], try #1 \r\n"]
[588.207734, "o", "\u001b[36mINFO\u001b[0m[0073] [certificates] Successfully started [rke-bundle-cert] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0073] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.10] \r\n"]
[588.212463, "o", "\u001b[36mINFO\u001b[0m[0073] Container [rke-bundle-cert] is still running on host [10.50.2.10]: stderr: [], stdout: [] \r\n"]
[589.212775, "o", "\u001b[36mINFO\u001b[0m[0074] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.10] \r\n"]
[589.214136, "o", "\u001b[36mINFO\u001b[0m[0074] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0074] Removing container [rke-bundle-cert] on host [10.50.2.10], try #1 \r\n"]
[589.22886, "o", "\u001b[36mINFO\u001b[0m[0074] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[589.363362, "o", "\u001b[36mINFO\u001b[0m[0074] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[589.824804, "o", "\u001b[36mINFO\u001b[0m[0075] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[589.82605, "o", "\u001b[36mINFO\u001b[0m[0075] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[590.014862, "o", "\u001b[36mINFO\u001b[0m[0075] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n"]
[590.017907, "o", "\u001b[36mINFO\u001b[0m[0075] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[590.134033, "o", "\u001b[36mINFO\u001b[0m[0075] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[590.59393, "o", "\u001b[36mINFO\u001b[0m[0076] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[590.596531, "o", "\u001b[36mINFO\u001b[0m[0076] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[590.785836, "o", "\u001b[36mINFO\u001b[0m[0076] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n"]
[590.801629, "o", "\u001b[36mINFO\u001b[0m[0076] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[591.017774, "o", "\u001b[36mINFO\u001b[0m[0076] Starting container [etcd-fix-perm] on host [10.50.2.11], try #1 \r\n"]
[591.659506, "o", "\u001b[36mINFO\u001b[0m[0077] Successfully started [etcd-fix-perm] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0077] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0077] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.11] \r\n"]
[591.666138, "o", "\u001b[36mINFO\u001b[0m[0077] Container [etcd-fix-perm] is still running on host [10.50.2.11]: stderr: [], stdout: [] \r\n"]
[592.666441, "o", "\u001b[36mINFO\u001b[0m[0078] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.11] \r\n"]
[592.67012, "o", "\u001b[36mINFO\u001b[0m[0078] Removing container [etcd-fix-perm] on host [10.50.2.11], try #1 \r\n"]
[592.699435, "o", "\u001b[36mINFO\u001b[0m[0078] [remove/etcd-fix-perm] Successfully removed container on host [10.50.2.11] \r\n"]
[592.706177, "o", "\u001b[36mINFO\u001b[0m[0078] Checking if container [etcd] is running on host [10.50.2.11], try #1 \r\n"]
[592.713068, "o", "\u001b[36mINFO\u001b[0m[0078] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] on host [10.50.2.11], try #1 \r\n"]
[593.508439, "o", "\u001b[36mINFO\u001b[0m[0079] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] exists on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0079] Checking if container [old-etcd] is running on host [10.50.2.11], try #1 \r\n"]
[593.513535, "o", "\u001b[36mINFO\u001b[0m[0079] Stopping container [etcd] on host [10.50.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[595.333148, "o", "\u001b[36mINFO\u001b[0m[0080] Waiting for [etcd] container to exit on host [10.50.2.11] \r\n"]
[595.333945, "o", "\u001b[36mINFO\u001b[0m[0080] Renaming container [etcd] to [old-etcd] on host [10.50.2.11], try #1 \r\n"]
[595.379241, "o", "\u001b[36mINFO\u001b[0m[0080] Starting container [etcd] on host [10.50.2.11], try #1 \r\n"]
[595.553559, "o", "\u001b[36mINFO\u001b[0m[0081] [etcd] Successfully updated [etcd] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0081] Removing container [old-etcd] on host [10.50.2.11], try #1 \r\n"]
[595.567586, "o", "\u001b[36mINFO\u001b[0m[0081] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.50.2.11] \r\n"]
[595.570302, "o", "\u001b[36mINFO\u001b[0m[0081] Removing container [etcd-rolling-snapshots] on host [10.50.2.11], try #1 \r\n"]
[595.717611, "o", "\u001b[36mINFO\u001b[0m[0081] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.50.2.11] \r\n"]
[595.719202, "o", "\u001b[36mINFO\u001b[0m[0081] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[595.905314, "o", "\u001b[36mINFO\u001b[0m[0081] Starting container [etcd-rolling-snapshots] on host [10.50.2.11], try #1 \r\n"]
[596.119739, "o", "\u001b[36mINFO\u001b[0m[0081] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.50.2.11] \r\n"]
[601.132849, "o", "\u001b[36mINFO\u001b[0m[0086] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[601.28193, "o", "\u001b[36mINFO\u001b[0m[0086] Starting container [rke-bundle-cert] on host [10.50.2.11], try #1 \r\n"]
[601.736303, "o", "\u001b[36mINFO\u001b[0m[0087] [certificates] Successfully started [rke-bundle-cert] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0087] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.11] \r\n"]
[601.738764, "o", "\u001b[36mINFO\u001b[0m[0087] Container [rke-bundle-cert] is still running on host [10.50.2.11]: stderr: [], stdout: [] \r\n"]
[602.738983, "o", "\u001b[36mINFO\u001b[0m[0088] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.11] \r\n"]
[602.740842, "o", "\u001b[36mINFO\u001b[0m[0088] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0088] Removing container [rke-bundle-cert] on host [10.50.2.11], try #1 \r\n"]
[602.770211, "o", "\u001b[36mINFO\u001b[0m[0088] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[602.907492, "o", "\u001b[36mINFO\u001b[0m[0088] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[603.351539, "o", "\u001b[36mINFO\u001b[0m[0088] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[603.353077, "o", "\u001b[36mINFO\u001b[0m[0088] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[603.571594, "o", "\u001b[36mINFO\u001b[0m[0089] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[603.574239, "o", "\u001b[36mINFO\u001b[0m[0089] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[603.691887, "o", "\u001b[36mINFO\u001b[0m[0089] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[604.120047, "o", "\u001b[36mINFO\u001b[0m[0089] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[604.120704, "o", "\u001b[36mINFO\u001b[0m[0089] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[604.334094, "o", "\u001b[36mINFO\u001b[0m[0089] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[604.344358, "o", "\u001b[36mINFO\u001b[0m[0089] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[604.559194, "o", "\u001b[36mINFO\u001b[0m[0090] Starting container [etcd-fix-perm] on host [10.50.2.12], try #1 \r\n"]
[605.171425, "o", "\u001b[36mINFO\u001b[0m[0090] Successfully started [etcd-fix-perm] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0090] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0090] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.12] \r\n"]
[605.172791, "o", "\u001b[36mINFO\u001b[0m[0090] Container [etcd-fix-perm] is still running on host [10.50.2.12]: stderr: [], stdout: [] \r\n"]
[606.172939, "o", "\u001b[36mINFO\u001b[0m[0091] Waiting for [etcd-fix-perm] container to exit on host [10.50.2.12] \r\n"]
[606.175255, "o", "\u001b[36mINFO\u001b[0m[0091] Removing container [etcd-fix-perm] on host [10.50.2.12], try #1 \r\n"]
[606.184141, "o", "\u001b[36mINFO\u001b[0m[0091] [remove/etcd-fix-perm] Successfully removed container on host [10.50.2.12] \r\n"]
[606.189329, "o", "\u001b[36mINFO\u001b[0m[0091] Checking if container [etcd] is running on host [10.50.2.12], try #1 \r\n"]
[606.192909, "o", "\u001b[36mINFO\u001b[0m[0091] Pulling image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] on host [10.50.2.12], try #1 \r\n"]
[607.028379, "o", "\u001b[36mINFO\u001b[0m[0092] Image [10.8.101.2:5000/rancher/coreos-etcd:v3.4.13-rancher1] exists on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0092] Checking if container [old-etcd] is running on host [10.50.2.12], try #1 \r\n"]
[607.032246, "o", "\u001b[36mINFO\u001b[0m[0092] Stopping container [etcd] on host [10.50.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[608.864347, "o", "\u001b[36mINFO\u001b[0m[0094] Waiting for [etcd] container to exit on host [10.50.2.12] \r\n"]
[608.865086, "o", "\u001b[36mINFO\u001b[0m[0094] Renaming container [etcd] to [old-etcd] on host [10.50.2.12], try #1 \r\n"]
[608.920709, "o", "\u001b[36mINFO\u001b[0m[0094] Starting container [etcd] on host [10.50.2.12], try #1 \r\n"]
[609.091295, "o", "\u001b[36mINFO\u001b[0m[0094] [etcd] Successfully updated [etcd] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0094] Removing container [old-etcd] on host [10.50.2.12], try #1 \r\n"]
[609.100532, "o", "\u001b[36mINFO\u001b[0m[0094] [etcd] Running rolling snapshot container [etcd-snapshot-once] on host [10.50.2.12] \r\n"]
[609.105834, "o", "\u001b[36mINFO\u001b[0m[0094] Removing container [etcd-rolling-snapshots] on host [10.50.2.12], try #1 \r\n"]
[609.272372, "o", "\u001b[36mINFO\u001b[0m[0094] [remove/etcd-rolling-snapshots] Successfully removed container on host [10.50.2.12] \r\n"]
[609.274015, "o", "\u001b[36mINFO\u001b[0m[0094] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[609.407232, "o", "\u001b[36mINFO\u001b[0m[0094] Starting container [etcd-rolling-snapshots] on host [10.50.2.12], try #1 \r\n"]
[609.609959, "o", "\u001b[36mINFO\u001b[0m[0095] [etcd] Successfully started [etcd-rolling-snapshots] container on host [10.50.2.12] \r\n"]
[614.613999, "o", "\u001b[36mINFO\u001b[0m[0100] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[614.768439, "o", "\u001b[36mINFO\u001b[0m[0100] Starting container [rke-bundle-cert] on host [10.50.2.12], try #1 \r\n"]
[615.245399, "o", "\u001b[36mINFO\u001b[0m[0100] [certificates] Successfully started [rke-bundle-cert] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0100] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.12] \r\n"]
[615.247461, "o", "\u001b[36mINFO\u001b[0m[0100] Container [rke-bundle-cert] is still running on host [10.50.2.12]: stderr: [], stdout: [] \r\n"]
[616.247537, "o", "\u001b[36mINFO\u001b[0m[0101] Waiting for [rke-bundle-cert] container to exit on host [10.50.2.12] \r\n"]
[616.248373, "o", "\u001b[36mINFO\u001b[0m[0101] [certificates] successfully saved certificate bundle [/opt/rke/etcd-snapshots//pki.bundle.tar.gz] on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0101] Removing container [rke-bundle-cert] on host [10.50.2.12], try #1 \r\n"]
[616.25871, "o", "\u001b[36mINFO\u001b[0m[0101] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[616.395099, "o", "\u001b[36mINFO\u001b[0m[0101] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[616.827227, "o", "\u001b[36mINFO\u001b[0m[0102] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[616.828215, "o", "\u001b[36mINFO\u001b[0m[0102] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[617.017939, "o", "\u001b[36mINFO\u001b[0m[0102] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n"]
[617.020611, "o", "\u001b[36mINFO\u001b[0m[0102] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[617.141156, "o", "\u001b[36mINFO\u001b[0m[0102] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[617.616101, "o", "\u001b[36mINFO\u001b[0m[0103] [etcd] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[617.617523, "o", "\u001b[36mINFO\u001b[0m[0103] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[617.831345, "o", "\u001b[36mINFO\u001b[0m[0103] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0103] [etcd] Successfully started etcd plane.. Checking etcd cluster health \r\n"]
[618.41564, "o", "\u001b[36mINFO\u001b[0m[0103] [etcd] etcd host [10.50.2.10] reported healthy=true \r\n"]
[618.416096, "o", "\u001b[36mINFO\u001b[0m[0103] [controlplane] Now checking status of node 10.50.2.10, try #1 \r\n"]
[618.424917, "o", "\u001b[36mINFO\u001b[0m[0103] [controlplane] Now checking status of node 10.50.2.11, try #1 \r\n"]
[618.429696, "o", "\u001b[36mINFO\u001b[0m[0103] [controlplane] Now checking status of node 10.50.2.12, try #1 \r\n"]
[618.433829, "o", "\u001b[36mINFO\u001b[0m[0103] [controlplane] Processing controlplane hosts for upgrade 1 at a time \r\n\u001b[36mINFO\u001b[0m[0103] Processing controlplane host 10.50.2.10      \r\n"]
[618.433869, "o", "\u001b[36mINFO\u001b[0m[0103] [controlplane] Now checking status of node 10.50.2.10, try #1 \r\n"]
[618.440897, "o", "\u001b[36mINFO\u001b[0m[0103] [controlplane] Getting list of nodes for upgrade \r\n"]
[618.478771, "o", "\u001b[36mINFO\u001b[0m[0103] Upgrading controlplane components for control host 10.50.2.10 \r\n\u001b[36mINFO\u001b[0m[0103] Checking if container [service-sidekick] is running on host [10.50.2.10], try #1 \r\n"]
[618.486432, "o", "\u001b[36mINFO\u001b[0m[0103] [sidekick] Sidekick container already created on host [10.50.2.10] \r\n"]
[618.489444, "o", "\u001b[36mINFO\u001b[0m[0103] Checking if container [kube-apiserver] is running on host [10.50.2.10], try #1 \r\n"]
[618.494501, "o", "\u001b[36mINFO\u001b[0m[0104] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0104] Checking if container [old-kube-apiserver] is running on host [10.50.2.10], try #1 \r\n"]
[618.502801, "o", "\u001b[36mINFO\u001b[0m[0104] Stopping container [kube-apiserver] on host [10.50.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[618.742562, "o", "\u001b[36mINFO\u001b[0m[0104] Waiting for [kube-apiserver] container to exit on host [10.50.2.10] \r\n"]
[618.7435, "o", "\u001b[36mINFO\u001b[0m[0104] Renaming container [kube-apiserver] to [old-kube-apiserver] on host [10.50.2.10], try #1 \r\n"]
[618.800246, "o", "\u001b[36mINFO\u001b[0m[0104] Starting container [kube-apiserver] on host [10.50.2.10], try #1 \r\n"]
[618.996354, "o", "\u001b[36mINFO\u001b[0m[0104] [controlplane] Successfully updated [kube-apiserver] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0104] Removing container [old-kube-apiserver] on host [10.50.2.10], try #1 \r\n"]
[619.008001, "o", "\u001b[36mINFO\u001b[0m[0104] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.50.2.10] \r\n"]
[627.457242, "o", "\u001b[36mINFO\u001b[0m[0112] [healthcheck] service [kube-apiserver] on host [10.50.2.10] is healthy \r\n"]
[627.460871, "o", "\u001b[36mINFO\u001b[0m[0112] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[627.612761, "o", "\u001b[36mINFO\u001b[0m[0113] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[628.081209, "o", "\u001b[36mINFO\u001b[0m[0113] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[628.082711, "o", "\u001b[36mINFO\u001b[0m[0113] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[628.317176, "o", "\u001b[36mINFO\u001b[0m[0113] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n"]
[628.321924, "o", "\u001b[36mINFO\u001b[0m[0113] Checking if container [kube-controller-manager] is running on host [10.50.2.10], try #1 \r\n"]
[628.32726, "o", "\u001b[36mINFO\u001b[0m[0113] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0113] Checking if container [old-kube-controller-manager] is running on host [10.50.2.10], try #1 \r\n"]
[628.339863, "o", "\u001b[36mINFO\u001b[0m[0113] Stopping container [kube-controller-manager] on host [10.50.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[628.498244, "o", "\u001b[36mINFO\u001b[0m[0114] Waiting for [kube-controller-manager] container to exit on host [10.50.2.10] \r\n"]
[628.498957, "o", "\u001b[36mINFO\u001b[0m[0114] Renaming container [kube-controller-manager] to [old-kube-controller-manager] on host [10.50.2.10], try #1 \r\n"]
[628.552459, "o", "\u001b[36mINFO\u001b[0m[0114] Starting container [kube-controller-manager] on host [10.50.2.10], try #1 \r\n"]
[628.741795, "o", "\u001b[36mINFO\u001b[0m[0114] [controlplane] Successfully updated [kube-controller-manager] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0114] Removing container [old-kube-controller-manager] on host [10.50.2.10], try #1 \r\n"]
[628.750572, "o", "\u001b[36mINFO\u001b[0m[0114] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.50.2.10] \r\n"]
[634.771989, "o", "\u001b[36mINFO\u001b[0m[0120] [healthcheck] service [kube-controller-manager] on host [10.50.2.10] is healthy \r\n"]
[634.783881, "o", "\u001b[36mINFO\u001b[0m[0120] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[634.94176, "o", "\u001b[36mINFO\u001b[0m[0120] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[635.385838, "o", "\u001b[36mINFO\u001b[0m[0120] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[635.388261, "o", "\u001b[36mINFO\u001b[0m[0120] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[635.582838, "o", "\u001b[36mINFO\u001b[0m[0121] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n"]
[635.586949, "o", "\u001b[36mINFO\u001b[0m[0121] Checking if container [kube-scheduler] is running on host [10.50.2.10], try #1 \r\n"]
[635.592403, "o", "\u001b[36mINFO\u001b[0m[0121] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0121] Checking if container [old-kube-scheduler] is running on host [10.50.2.10], try #1 \r\n"]
[635.598608, "o", "\u001b[36mINFO\u001b[0m[0121] Stopping container [kube-scheduler] on host [10.50.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[635.71072, "o", "\u001b[36mINFO\u001b[0m[0121] Waiting for [kube-scheduler] container to exit on host [10.50.2.10] \r\n"]
[635.711363, "o", "\u001b[36mINFO\u001b[0m[0121] Renaming container [kube-scheduler] to [old-kube-scheduler] on host [10.50.2.10], try #1 \r\n"]
[635.766866, "o", "\u001b[36mINFO\u001b[0m[0121] Starting container [kube-scheduler] on host [10.50.2.10], try #1 \r\n"]
[635.95668, "o", "\u001b[36mINFO\u001b[0m[0121] [controlplane] Successfully updated [kube-scheduler] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0121] Removing container [old-kube-scheduler] on host [10.50.2.10], try #1 \r\n"]
[635.969067, "o", "\u001b[36mINFO\u001b[0m[0121] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.50.2.10] \r\n"]
[636.703892, "o", "\u001b[36mINFO\u001b[0m[0122] [healthcheck] service [kube-scheduler] on host [10.50.2.10] is healthy \r\n"]
[636.706386, "o", "\u001b[36mINFO\u001b[0m[0122] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[636.832781, "o", "\u001b[36mINFO\u001b[0m[0122] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[637.257783, "o", "\u001b[36mINFO\u001b[0m[0122] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[637.260252, "o", "\u001b[36mINFO\u001b[0m[0122] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[637.461967, "o", "\u001b[36mINFO\u001b[0m[0122] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0122] Upgrading workerplane components for control host 10.50.2.10 \r\n\u001b[36mINFO\u001b[0m[0122] Checking if container [service-sidekick] is running on host [10.50.2.10], try #1 \r\n"]
[637.469009, "o", "\u001b[36mINFO\u001b[0m[0122] [sidekick] Sidekick container already created on host [10.50.2.10] \r\n"]
[637.471185, "o", "\u001b[36mINFO\u001b[0m[0122] Checking if container [kubelet] is running on host [10.50.2.10], try #1 \r\n"]
[637.474895, "o", "\u001b[36mINFO\u001b[0m[0122] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0122] Checking if container [old-kubelet] is running on host [10.50.2.10], try #1 \r\n"]
[637.479229, "o", "\u001b[36mINFO\u001b[0m[0122] Stopping container [kubelet] on host [10.50.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[637.65863, "o", "\u001b[36mINFO\u001b[0m[0123] Waiting for [kubelet] container to exit on host [10.50.2.10] \r\n"]
[637.659251, "o", "\u001b[36mINFO\u001b[0m[0123] Renaming container [kubelet] to [old-kubelet] on host [10.50.2.10], try #1 \r\n"]
[637.710303, "o", "\u001b[36mINFO\u001b[0m[0123] Starting container [kubelet] on host [10.50.2.10], try #1 \r\n"]
[637.814127, "o", "\u001b[36mINFO\u001b[0m[0123] [worker] Successfully updated [kubelet] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0123] Removing container [old-kubelet] on host [10.50.2.10], try #1 \r\n"]
[637.830343, "o", "\u001b[36mINFO\u001b[0m[0123] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.10] \r\n"]
[649.262753, "o", "\u001b[36mINFO\u001b[0m[0134] [healthcheck] service [kubelet] on host [10.50.2.10] is healthy \r\n"]
[649.276629, "o", "\u001b[36mINFO\u001b[0m[0134] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[649.434306, "o", "\u001b[36mINFO\u001b[0m[0134] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[649.92059, "o", "\u001b[36mINFO\u001b[0m[0135] [worker] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[649.921649, "o", "\u001b[36mINFO\u001b[0m[0135] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[650.110987, "o", "\u001b[36mINFO\u001b[0m[0135] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n"]
[650.113953, "o", "\u001b[36mINFO\u001b[0m[0135] Checking if container [kube-proxy] is running on host [10.50.2.10], try #1 \r\n"]
[650.133172, "o", "\u001b[36mINFO\u001b[0m[0135] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0135] Checking if container [old-kube-proxy] is running on host [10.50.2.10], try #1 \r\n"]
[650.141576, "o", "\u001b[36mINFO\u001b[0m[0135] Stopping container [kube-proxy] on host [10.50.2.10] with stopTimeoutDuration [5s], try #1 \r\n"]
[650.286165, "o", "\u001b[36mINFO\u001b[0m[0135] Waiting for [kube-proxy] container to exit on host [10.50.2.10] \r\n"]
[650.286904, "o", "\u001b[36mINFO\u001b[0m[0135] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.50.2.10], try #1 \r\n"]
[650.341504, "o", "\u001b[36mINFO\u001b[0m[0135] Starting container [kube-proxy] on host [10.50.2.10], try #1 \r\n"]
[650.4478, "o", "\u001b[36mINFO\u001b[0m[0135] [worker] Successfully updated [kube-proxy] container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0135] Removing container [old-kube-proxy] on host [10.50.2.10], try #1 \r\n"]
[650.456438, "o", "\u001b[36mINFO\u001b[0m[0135] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.10] \r\n"]
[650.961401, "o", "\u001b[36mINFO\u001b[0m[0136] [healthcheck] service [kube-proxy] on host [10.50.2.10] is healthy \r\n"]
[650.974273, "o", "\u001b[36mINFO\u001b[0m[0136] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[651.148161, "o", "\u001b[36mINFO\u001b[0m[0136] Starting container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[651.599331, "o", "\u001b[36mINFO\u001b[0m[0137] [worker] Successfully started [rke-log-linker] container on host [10.50.2.10] \r\n"]
[651.600171, "o", "\u001b[36mINFO\u001b[0m[0137] Removing container [rke-log-linker] on host [10.50.2.10], try #1 \r\n"]
[651.806008, "o", "\u001b[36mINFO\u001b[0m[0137] [remove/rke-log-linker] Successfully removed container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0137] [controlplane] Now checking status of node 10.50.2.10, try #1 \r\n"]
[651.843622, "o", "\u001b[36mINFO\u001b[0m[0137] Processing controlplane host 10.50.2.11      \r\n\u001b[36mINFO\u001b[0m[0137] [controlplane] Now checking status of node 10.50.2.11, try #1 \r\n"]
[651.857723, "o", "\u001b[36mINFO\u001b[0m[0137] [controlplane] Getting list of nodes for upgrade \r\n"]
[651.888957, "o", "\u001b[36mINFO\u001b[0m[0137] Upgrading controlplane components for control host 10.50.2.11 \r\n\u001b[36mINFO\u001b[0m[0137] Checking if container [service-sidekick] is running on host [10.50.2.11], try #1 \r\n"]
[651.898099, "o", "\u001b[36mINFO\u001b[0m[0137] [sidekick] Sidekick container already created on host [10.50.2.11] \r\n"]
[651.904273, "o", "\u001b[36mINFO\u001b[0m[0137] Checking if container [kube-apiserver] is running on host [10.50.2.11], try #1 \r\n"]
[651.908705, "o", "\u001b[36mINFO\u001b[0m[0137] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0137] Checking if container [old-kube-apiserver] is running on host [10.50.2.11], try #1 \r\n"]
[651.914067, "o", "\u001b[36mINFO\u001b[0m[0137] Stopping container [kube-apiserver] on host [10.50.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[652.177061, "o", "\u001b[36mINFO\u001b[0m[0137] Waiting for [kube-apiserver] container to exit on host [10.50.2.11] \r\n"]
[652.177848, "o", "\u001b[36mINFO\u001b[0m[0137] Renaming container [kube-apiserver] to [old-kube-apiserver] on host [10.50.2.11], try #1 \r\n"]
[652.232559, "o", "\u001b[36mINFO\u001b[0m[0137] Starting container [kube-apiserver] on host [10.50.2.11], try #1 \r\n"]
[652.413116, "o", "\u001b[36mINFO\u001b[0m[0137] [controlplane] Successfully updated [kube-apiserver] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0137] Removing container [old-kube-apiserver] on host [10.50.2.11], try #1 \r\n"]
[652.426731, "o", "\u001b[36mINFO\u001b[0m[0137] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.50.2.11] \r\n"]
[661.211623, "o", "\u001b[36mINFO\u001b[0m[0146] [healthcheck] service [kube-apiserver] on host [10.50.2.11] is healthy \r\n"]
[661.216755, "o", "\u001b[36mINFO\u001b[0m[0146] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[661.393645, "o", "\u001b[36mINFO\u001b[0m[0146] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[661.843067, "o", "\u001b[36mINFO\u001b[0m[0147] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[661.846331, "o", "\u001b[36mINFO\u001b[0m[0147] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[662.124161, "o", "\u001b[36mINFO\u001b[0m[0147] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[662.127192, "o", "\u001b[36mINFO\u001b[0m[0147] Checking if container [kube-controller-manager] is running on host [10.50.2.11], try #1 \r\n"]
[662.130882, "o", "\u001b[36mINFO\u001b[0m[0147] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0147] Checking if container [old-kube-controller-manager] is running on host [10.50.2.11], try #1 \r\n"]
[662.136159, "o", "\u001b[36mINFO\u001b[0m[0147] Stopping container [kube-controller-manager] on host [10.50.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[662.309747, "o", "\u001b[36mINFO\u001b[0m[0147] Waiting for [kube-controller-manager] container to exit on host [10.50.2.11] \r\n"]
[662.310402, "o", "\u001b[36mINFO\u001b[0m[0147] Renaming container [kube-controller-manager] to [old-kube-controller-manager] on host [10.50.2.11], try #1 \r\n"]
[662.359445, "o", "\u001b[36mINFO\u001b[0m[0147] Starting container [kube-controller-manager] on host [10.50.2.11], try #1 \r\n"]
[662.555351, "o", "\u001b[36mINFO\u001b[0m[0148] [controlplane] Successfully updated [kube-controller-manager] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0148] Removing container [old-kube-controller-manager] on host [10.50.2.11], try #1 \r\n"]
[662.567748, "o", "\u001b[36mINFO\u001b[0m[0148] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.50.2.11] \r\n"]
[663.272403, "o", "\u001b[36mINFO\u001b[0m[0148] [healthcheck] service [kube-controller-manager] on host [10.50.2.11] is healthy \r\n"]
[663.275652, "o", "\u001b[36mINFO\u001b[0m[0148] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[663.476217, "o", "\u001b[36mINFO\u001b[0m[0148] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[663.987754, "o", "\u001b[36mINFO\u001b[0m[0149] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[663.989969, "o", "\u001b[36mINFO\u001b[0m[0149] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[664.234653, "o", "\u001b[36mINFO\u001b[0m[0149] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[664.241005, "o", "\u001b[36mINFO\u001b[0m[0149] Checking if container [kube-scheduler] is running on host [10.50.2.11], try #1 \r\n"]
[664.249781, "o", "\u001b[36mINFO\u001b[0m[0149] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0149] Checking if container [old-kube-scheduler] is running on host [10.50.2.11], try #1 \r\n"]
[664.254347, "o", "\u001b[36mINFO\u001b[0m[0149] Stopping container [kube-scheduler] on host [10.50.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[664.413425, "o", "\u001b[36mINFO\u001b[0m[0149] Waiting for [kube-scheduler] container to exit on host [10.50.2.11] \r\n"]
[664.414187, "o", "\u001b[36mINFO\u001b[0m[0149] Renaming container [kube-scheduler] to [old-kube-scheduler] on host [10.50.2.11], try #1 \r\n"]
[664.476507, "o", "\u001b[36mINFO\u001b[0m[0149] Starting container [kube-scheduler] on host [10.50.2.11], try #1 \r\n"]
[664.659287, "o", "\u001b[36mINFO\u001b[0m[0150] [controlplane] Successfully updated [kube-scheduler] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0150] Removing container [old-kube-scheduler] on host [10.50.2.11], try #1 \r\n"]
[664.670859, "o", "\u001b[36mINFO\u001b[0m[0150] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.50.2.11] \r\n"]
[670.685757, "o", "\u001b[36mINFO\u001b[0m[0156] [healthcheck] service [kube-scheduler] on host [10.50.2.11] is healthy \r\n"]
[670.688418, "o", "\u001b[36mINFO\u001b[0m[0156] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[670.915705, "o", "\u001b[36mINFO\u001b[0m[0156] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[671.372036, "o", "\u001b[36mINFO\u001b[0m[0156] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[671.373608, "o", "\u001b[36mINFO\u001b[0m[0156] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[671.566095, "o", "\u001b[36mINFO\u001b[0m[0157] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0157] Upgrading workerplane components for control host 10.50.2.11 \r\n\u001b[36mINFO\u001b[0m[0157] Checking if container [service-sidekick] is running on host [10.50.2.11], try #1 \r\n"]
[671.57226, "o", "\u001b[36mINFO\u001b[0m[0157] [sidekick] Sidekick container already created on host [10.50.2.11] \r\n"]
[671.575179, "o", "\u001b[36mINFO\u001b[0m[0157] Checking if container [kubelet] is running on host [10.50.2.11], try #1 \r\n"]
[671.580984, "o", "\u001b[36mINFO\u001b[0m[0157] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0157] Checking if container [old-kubelet] is running on host [10.50.2.11], try #1 \r\n"]
[671.585306, "o", "\u001b[36mINFO\u001b[0m[0157] Stopping container [kubelet] on host [10.50.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[671.779115, "o", "\u001b[36mINFO\u001b[0m[0157] Waiting for [kubelet] container to exit on host [10.50.2.11] \r\n"]
[671.779934, "o", "\u001b[36mINFO\u001b[0m[0157] Renaming container [kubelet] to [old-kubelet] on host [10.50.2.11], try #1 \r\n"]
[671.821483, "o", "\u001b[36mINFO\u001b[0m[0157] Starting container [kubelet] on host [10.50.2.11], try #1 \r\n"]
[671.918293, "o", "\u001b[36mINFO\u001b[0m[0157] [worker] Successfully updated [kubelet] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0157] Removing container [old-kubelet] on host [10.50.2.11], try #1 \r\n"]
[671.932353, "o", "\u001b[36mINFO\u001b[0m[0157] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.11] \r\n"]
[683.350765, "o", "\u001b[36mINFO\u001b[0m[0168] [healthcheck] service [kubelet] on host [10.50.2.11] is healthy \r\n"]
[683.353792, "o", "\u001b[36mINFO\u001b[0m[0168] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[683.49225, "o", "\u001b[36mINFO\u001b[0m[0168] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[683.948252, "o", "\u001b[36mINFO\u001b[0m[0169] [worker] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[683.949158, "o", "\u001b[36mINFO\u001b[0m[0169] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[684.140831, "o", "\u001b[36mINFO\u001b[0m[0169] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n"]
[684.143171, "o", "\u001b[36mINFO\u001b[0m[0169] Checking if container [kube-proxy] is running on host [10.50.2.11], try #1 \r\n"]
[684.148148, "o", "\u001b[36mINFO\u001b[0m[0169] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0169] Checking if container [old-kube-proxy] is running on host [10.50.2.11], try #1 \r\n"]
[684.155453, "o", "\u001b[36mINFO\u001b[0m[0169] Stopping container [kube-proxy] on host [10.50.2.11] with stopTimeoutDuration [5s], try #1 \r\n"]
[684.311369, "o", "\u001b[36mINFO\u001b[0m[0169] Waiting for [kube-proxy] container to exit on host [10.50.2.11] \r\n"]
[684.315023, "o", "\u001b[36mINFO\u001b[0m[0169] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.50.2.11], try #1 \r\n"]
[684.384332, "o", "\u001b[36mINFO\u001b[0m[0169] Starting container [kube-proxy] on host [10.50.2.11], try #1 \r\n"]
[684.516456, "o", "\u001b[36mINFO\u001b[0m[0170] [worker] Successfully updated [kube-proxy] container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0170] Removing container [old-kube-proxy] on host [10.50.2.11], try #1 \r\n"]
[684.524443, "o", "\u001b[36mINFO\u001b[0m[0170] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.11] \r\n"]
[685.017174, "o", "\u001b[36mINFO\u001b[0m[0170] [healthcheck] service [kube-proxy] on host [10.50.2.11] is healthy \r\n"]
[685.02918, "o", "\u001b[36mINFO\u001b[0m[0170] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[685.183428, "o", "\u001b[36mINFO\u001b[0m[0170] Starting container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[685.630162, "o", "\u001b[36mINFO\u001b[0m[0171] [worker] Successfully started [rke-log-linker] container on host [10.50.2.11] \r\n"]
[685.631024, "o", "\u001b[36mINFO\u001b[0m[0171] Removing container [rke-log-linker] on host [10.50.2.11], try #1 \r\n"]
[685.820106, "o", "\u001b[36mINFO\u001b[0m[0171] [remove/rke-log-linker] Successfully removed container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0171] [controlplane] Now checking status of node 10.50.2.11, try #1 \r\n"]
[685.846088, "o", "\u001b[36mINFO\u001b[0m[0171] Processing controlplane host 10.50.2.12      \r\n\u001b[36mINFO\u001b[0m[0171] [controlplane] Now checking status of node 10.50.2.12, try #1 \r\n"]
[685.855071, "o", "\u001b[36mINFO\u001b[0m[0171] [controlplane] Getting list of nodes for upgrade \r\n"]
[686.451699, "o", "\u001b[36mINFO\u001b[0m[0171] Upgrading controlplane components for control host 10.50.2.12 \r\n\u001b[36mINFO\u001b[0m[0171] Checking if container [service-sidekick] is running on host [10.50.2.12], try #1 \r\n"]
[686.459069, "o", "\u001b[36mINFO\u001b[0m[0171] [sidekick] Sidekick container already created on host [10.50.2.12] \r\n"]
[686.462049, "o", "\u001b[36mINFO\u001b[0m[0171] Checking if container [kube-apiserver] is running on host [10.50.2.12], try #1 \r\n"]
[686.465571, "o", "\u001b[36mINFO\u001b[0m[0171] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0171] Checking if container [old-kube-apiserver] is running on host [10.50.2.12], try #1 \r\n"]
[686.472245, "o", "\u001b[36mINFO\u001b[0m[0171] Stopping container [kube-apiserver] on host [10.50.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[686.696439, "o", "\u001b[36mINFO\u001b[0m[0172] Waiting for [kube-apiserver] container to exit on host [10.50.2.12] \r\n"]
[686.697195, "o", "\u001b[36mINFO\u001b[0m[0172] Renaming container [kube-apiserver] to [old-kube-apiserver] on host [10.50.2.12], try #1 \r\n"]
[686.758762, "o", "\u001b[36mINFO\u001b[0m[0172] Starting container [kube-apiserver] on host [10.50.2.12], try #1 \r\n"]
[686.941603, "o", "\u001b[36mINFO\u001b[0m[0172] [controlplane] Successfully updated [kube-apiserver] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0172] Removing container [old-kube-apiserver] on host [10.50.2.12], try #1 \r\n"]
[686.952334, "o", "\u001b[36mINFO\u001b[0m[0172] [healthcheck] Start Healthcheck on service [kube-apiserver] on host [10.50.2.12] \r\n"]
[695.791329, "o", "\u001b[36mINFO\u001b[0m[0181] [healthcheck] service [kube-apiserver] on host [10.50.2.12] is healthy \r\n"]
[695.807143, "o", "\u001b[36mINFO\u001b[0m[0181] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[695.972686, "o", "\u001b[36mINFO\u001b[0m[0181] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[696.419195, "o", "\u001b[36mINFO\u001b[0m[0181] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[696.420333, "o", "\u001b[36mINFO\u001b[0m[0181] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[696.649808, "o", "\u001b[36mINFO\u001b[0m[0182] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n"]
[696.653502, "o", "\u001b[36mINFO\u001b[0m[0182] Checking if container [kube-controller-manager] is running on host [10.50.2.12], try #1 \r\n"]
[696.656904, "o", "\u001b[36mINFO\u001b[0m[0182] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0182] Checking if container [old-kube-controller-manager] is running on host [10.50.2.12], try #1 \r\n"]
[696.661055, "o", "\u001b[36mINFO\u001b[0m[0182] Stopping container [kube-controller-manager] on host [10.50.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[696.784926, "o", "\u001b[36mINFO\u001b[0m[0182] Waiting for [kube-controller-manager] container to exit on host [10.50.2.12] \r\n"]
[696.785602, "o", "\u001b[36mINFO\u001b[0m[0182] Renaming container [kube-controller-manager] to [old-kube-controller-manager] on host [10.50.2.12], try #1 \r\n"]
[696.854468, "o", "\u001b[36mINFO\u001b[0m[0182] Starting container [kube-controller-manager] on host [10.50.2.12], try #1 \r\n"]
[697.016398, "o", "\u001b[36mINFO\u001b[0m[0182] [controlplane] Successfully updated [kube-controller-manager] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0182] Removing container [old-kube-controller-manager] on host [10.50.2.12], try #1 \r\n"]
[697.027836, "o", "\u001b[36mINFO\u001b[0m[0182] [healthcheck] Start Healthcheck on service [kube-controller-manager] on host [10.50.2.12] \r\n"]
[703.038628, "o", "\u001b[36mINFO\u001b[0m[0188] [healthcheck] service [kube-controller-manager] on host [10.50.2.12] is healthy \r\n"]
[703.041021, "o", "\u001b[36mINFO\u001b[0m[0188] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[705.271967, "o", "\u001b[36mINFO\u001b[0m[0190] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[705.796561, "o", "\u001b[36mINFO\u001b[0m[0191] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[705.797377, "o", "\u001b[36mINFO\u001b[0m[0191] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[706.159465, "o", "\u001b[36mINFO\u001b[0m[0191] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n"]
[706.174265, "o", "\u001b[36mINFO\u001b[0m[0191] Checking if container [kube-scheduler] is running on host [10.50.2.12], try #1 \r\n"]
[706.178476, "o", "\u001b[36mINFO\u001b[0m[0191] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0191] Checking if container [old-kube-scheduler] is running on host [10.50.2.12], try #1 \r\n"]
[706.183956, "o", "\u001b[36mINFO\u001b[0m[0191] Stopping container [kube-scheduler] on host [10.50.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[706.431918, "o", "\u001b[36mINFO\u001b[0m[0191] Waiting for [kube-scheduler] container to exit on host [10.50.2.12] \r\n"]
[706.43576, "o", "\u001b[36mINFO\u001b[0m[0191] Renaming container [kube-scheduler] to [old-kube-scheduler] on host [10.50.2.12], try #1 \r\n"]
[706.597372, "o", "\u001b[36mINFO\u001b[0m[0192] Starting container [kube-scheduler] on host [10.50.2.12], try #1 \r\n"]
[706.843266, "o", "\u001b[36mINFO\u001b[0m[0192] [controlplane] Successfully updated [kube-scheduler] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0192] Removing container [old-kube-scheduler] on host [10.50.2.12], try #1 \r\n"]
[706.905725, "o", "\u001b[36mINFO\u001b[0m[0192] [healthcheck] Start Healthcheck on service [kube-scheduler] on host [10.50.2.12] \r\n"]
[707.568607, "o", "\u001b[36mINFO\u001b[0m[0193] [healthcheck] service [kube-scheduler] on host [10.50.2.12] is healthy \r\n"]
[707.571005, "o", "\u001b[36mINFO\u001b[0m[0193] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[710.462824, "o", "\u001b[36mINFO\u001b[0m[0195] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[711.003355, "o", "\u001b[36mINFO\u001b[0m[0196] [controlplane] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[711.004498, "o", "\u001b[36mINFO\u001b[0m[0196] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[711.515133, "o", "\u001b[36mINFO\u001b[0m[0197] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0197] Upgrading workerplane components for control host 10.50.2.12 \r\n\u001b[36mINFO\u001b[0m[0197] Checking if container [service-sidekick] is running on host [10.50.2.12], try #1 \r\n"]
[711.521125, "o", "\u001b[36mINFO\u001b[0m[0197] [sidekick] Sidekick container already created on host [10.50.2.12] \r\n"]
[711.524121, "o", "\u001b[36mINFO\u001b[0m[0197] Checking if container [kubelet] is running on host [10.50.2.12], try #1 \r\n"]
[711.528981, "o", "\u001b[36mINFO\u001b[0m[0197] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0197] Checking if container [old-kubelet] is running on host [10.50.2.12], try #1 \r\n"]
[711.534606, "o", "\u001b[36mINFO\u001b[0m[0197] Stopping container [kubelet] on host [10.50.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[711.795864, "o", "\u001b[36mINFO\u001b[0m[0197] Waiting for [kubelet] container to exit on host [10.50.2.12] \r\n"]
[711.800609, "o", "\u001b[36mINFO\u001b[0m[0197] Renaming container [kubelet] to [old-kubelet] on host [10.50.2.12], try #1 \r\n"]
[711.974549, "o", "\u001b[36mINFO\u001b[0m[0197] Starting container [kubelet] on host [10.50.2.12], try #1 \r\n"]
[712.177197, "o", "\u001b[36mINFO\u001b[0m[0197] [worker] Successfully updated [kubelet] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0197] Removing container [old-kubelet] on host [10.50.2.12], try #1 \r\n"]
[712.250837, "o", "\u001b[36mINFO\u001b[0m[0197] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.12] \r\n"]
[723.73772, "o", "\u001b[36mINFO\u001b[0m[0209] [healthcheck] service [kubelet] on host [10.50.2.12] is healthy \r\n"]
[723.740372, "o", "\u001b[36mINFO\u001b[0m[0209] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[726.529113, "o", "\u001b[36mINFO\u001b[0m[0212] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[727.144124, "o", "\u001b[36mINFO\u001b[0m[0212] [worker] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[727.146469, "o", "\u001b[36mINFO\u001b[0m[0212] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[727.489887, "o", "\u001b[36mINFO\u001b[0m[0212] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n"]
[727.493311, "o", "\u001b[36mINFO\u001b[0m[0212] Checking if container [kube-proxy] is running on host [10.50.2.12], try #1 \r\n"]
[727.501422, "o", "\u001b[36mINFO\u001b[0m[0213] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0213] Checking if container [old-kube-proxy] is running on host [10.50.2.12], try #1 \r\n"]
[727.508411, "o", "\u001b[36mINFO\u001b[0m[0213] Stopping container [kube-proxy] on host [10.50.2.12] with stopTimeoutDuration [5s], try #1 \r\n"]
[727.735869, "o", "\u001b[36mINFO\u001b[0m[0213] Waiting for [kube-proxy] container to exit on host [10.50.2.12] \r\n"]
[727.739308, "o", "\u001b[36mINFO\u001b[0m[0213] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.50.2.12], try #1 \r\n"]
[727.877461, "o", "\u001b[36mINFO\u001b[0m[0213] Starting container [kube-proxy] on host [10.50.2.12], try #1 \r\n"]
[728.052572, "o", "\u001b[36mINFO\u001b[0m[0213] [worker] Successfully updated [kube-proxy] container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0213] Removing container [old-kube-proxy] on host [10.50.2.12], try #1 \r\n"]
[728.105685, "o", "\u001b[36mINFO\u001b[0m[0213] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.12] \r\n"]
[728.580676, "o", "\u001b[36mINFO\u001b[0m[0214] [healthcheck] service [kube-proxy] on host [10.50.2.12] is healthy \r\n"]
[728.58343, "o", "\u001b[36mINFO\u001b[0m[0214] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[731.412346, "o", "\u001b[36mINFO\u001b[0m[0216] Starting container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[731.956615, "o", "\u001b[36mINFO\u001b[0m[0217] [worker] Successfully started [rke-log-linker] container on host [10.50.2.12] \r\n"]
[731.958251, "o", "\u001b[36mINFO\u001b[0m[0217] Removing container [rke-log-linker] on host [10.50.2.12], try #1 \r\n"]
[732.274005, "o", "\u001b[36mINFO\u001b[0m[0217] [remove/rke-log-linker] Successfully removed container on host [10.50.2.12] \r\n\u001b[36mINFO\u001b[0m[0217] [controlplane] Now checking status of node 10.50.2.12, try #1 \r\n"]
[732.333223, "o", "\u001b[36mINFO\u001b[0m[0217] [controlplane] Successfully upgraded Controller Plane.. \r\n"]
[732.334453, "o", "\u001b[36mINFO\u001b[0m[0217] [authz] Creating rke-job-deployer ServiceAccount \r\n"]
[732.375629, "o", "\u001b[36mINFO\u001b[0m[0217] [authz] rke-job-deployer ServiceAccount created successfully \r\n\u001b[36mINFO\u001b[0m[0217] [authz] Creating system:node ClusterRoleBinding \r\n"]
[732.384313, "o", "\u001b[36mINFO\u001b[0m[0217] [authz] system:node ClusterRoleBinding created successfully \r\n\u001b[36mINFO\u001b[0m[0217] [authz] Creating kube-apiserver proxy ClusterRole and ClusterRoleBinding \r\n"]
[732.419783, "o", "\u001b[36mINFO\u001b[0m[0217] [authz] kube-apiserver proxy ClusterRole and ClusterRoleBinding created successfully \r\n"]
[732.421382, "o", "\u001b[36mINFO\u001b[0m[0217] Successfully Deployed state file at [/home/asd/_git/RESEARCH/k8s-engines-on-prem/rke/u1.rkestate] \r\n"]
[732.421944, "o", "\u001b[36mINFO\u001b[0m[0217] [state] Saving full cluster state to Kubernetes \r\n"]
[732.518312, "o", "\u001b[36mINFO\u001b[0m[0218] [state] Successfully Saved full cluster state to Kubernetes ConfigMap: full-cluster-state \r\n"]
[732.521572, "o", "\u001b[36mINFO\u001b[0m[0218] [worker] Now checking status of node 10.50.2.20, try #1 \r\n"]
[732.528265, "o", "\u001b[36mINFO\u001b[0m[0218] [worker] Now checking status of node 10.50.2.21, try #1 \r\n"]
[732.534267, "o", "\u001b[36mINFO\u001b[0m[0218] [worker] Now checking status of node 10.50.2.22, try #1 \r\n"]
[732.539495, "o", "\u001b[36mINFO\u001b[0m[0218] [worker] Upgrading Worker Plane..            \r\n"]
[732.569707, "o", "\u001b[36mINFO\u001b[0m[0218] Now checking and upgrading worker components on nodes with only worker role 1 at a time \r\n\u001b[36mINFO\u001b[0m[0218] [workerplane] Processing host 10.50.2.20     \r\n"]
[732.569865, "o", "\u001b[36mINFO\u001b[0m[0218] [worker] Now checking status of node 10.50.2.20, try #1 \r\n"]
[732.579078, "o", "\u001b[36mINFO\u001b[0m[0218] [worker] Getting list of nodes for upgrade   \r\n"]
[733.362014, "o", "\u001b[36mINFO\u001b[0m[0218] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[735.959421, "o", "\u001b[36mINFO\u001b[0m[0221] Starting container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[736.466974, "o", "\u001b[36mINFO\u001b[0m[0221] [worker] Successfully started [rke-log-linker] container on host [10.50.2.20] \r\n"]
[736.471008, "o", "\u001b[36mINFO\u001b[0m[0221] Removing container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[736.828604, "o", "\u001b[36mINFO\u001b[0m[0222] [remove/rke-log-linker] Successfully removed container on host [10.50.2.20] \r\n\u001b[36mINFO\u001b[0m[0222] Checking if container [service-sidekick] is running on host [10.50.2.20], try #1 \r\n"]
[736.839272, "o", "\u001b[36mINFO\u001b[0m[0222] [sidekick] Sidekick container already created on host [10.50.2.20] \r\n"]
[736.842846, "o", "\u001b[36mINFO\u001b[0m[0222] Checking if container [kubelet] is running on host [10.50.2.20], try #1 \r\n"]
[736.848337, "o", "\u001b[36mINFO\u001b[0m[0222] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.20] \r\n\u001b[36mINFO\u001b[0m[0222] Checking if container [old-kubelet] is running on host [10.50.2.20], try #1 \r\n"]
[736.853154, "o", "\u001b[36mINFO\u001b[0m[0222] Stopping container [kubelet] on host [10.50.2.20] with stopTimeoutDuration [5s], try #1 \r\n"]
[737.119468, "o", "\u001b[36mINFO\u001b[0m[0222] Waiting for [kubelet] container to exit on host [10.50.2.20] \r\n"]
[737.120622, "o", "\u001b[36mINFO\u001b[0m[0222] Renaming container [kubelet] to [old-kubelet] on host [10.50.2.20], try #1 \r\n"]
[737.266684, "o", "\u001b[36mINFO\u001b[0m[0222] Starting container [kubelet] on host [10.50.2.20], try #1 \r\n"]
[737.451952, "o", "\u001b[36mINFO\u001b[0m[0222] [worker] Successfully updated [kubelet] container on host [10.50.2.20] \r\n\u001b[36mINFO\u001b[0m[0222] Removing container [old-kubelet] on host [10.50.2.20], try #1 \r\n"]
[737.507266, "o", "\u001b[36mINFO\u001b[0m[0223] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.20] \r\n"]
[749.039638, "o", "\u001b[36mINFO\u001b[0m[0234] [healthcheck] service [kubelet] on host [10.50.2.20] is healthy \r\n"]
[749.05276, "o", "\u001b[36mINFO\u001b[0m[0234] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[751.848172, "o", "\u001b[36mINFO\u001b[0m[0237] Starting container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[752.502494, "o", "\u001b[36mINFO\u001b[0m[0238] [worker] Successfully started [rke-log-linker] container on host [10.50.2.20] \r\n"]
[752.50369, "o", "\u001b[36mINFO\u001b[0m[0238] Removing container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[753.12288, "o", "\u001b[36mINFO\u001b[0m[0238] [remove/rke-log-linker] Successfully removed container on host [10.50.2.20] \r\n"]
[753.133681, "o", "\u001b[36mINFO\u001b[0m[0238] Checking if container [kube-proxy] is running on host [10.50.2.20], try #1 \r\n"]
[753.161584, "o", "\u001b[36mINFO\u001b[0m[0238] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.20] \r\n\u001b[36mINFO\u001b[0m[0238] Checking if container [old-kube-proxy] is running on host [10.50.2.20], try #1 \r\n"]
[753.203211, "o", "\u001b[36mINFO\u001b[0m[0238] Stopping container [kube-proxy] on host [10.50.2.20] with stopTimeoutDuration [5s], try #1 "]
[753.205059, "o", "\r\n"]
[753.43741, "o", "\u001b[36mINFO\u001b[0m[0238] Waiting for [kube-proxy] container to exit on host [10.50.2.20] \r\n"]
[753.440678, "o", "\u001b[36mINFO\u001b[0m[0238] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.50.2.20], try #1 \r\n"]
[753.59865, "o", "\u001b[36mINFO\u001b[0m[0239] Starting container [kube-proxy] on host [10.50.2.20], try #1 \r\n"]
[753.813842, "o", "\u001b[36mINFO\u001b[0m[0239] [worker] Successfully updated [kube-proxy] container on host [10.50.2.20] \r\n\u001b[36mINFO\u001b[0m[0239] Removing container [old-kube-proxy] on host [10.50.2.20], try #1 \r\n"]
[753.884163, "o", "\u001b[36mINFO\u001b[0m[0239] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.20] \r\n"]
[754.37177, "o", "\u001b[36mINFO\u001b[0m[0239] [healthcheck] service [kube-proxy] on host [10.50.2.20] is healthy \r\n"]
[754.374706, "o", "\u001b[36mINFO\u001b[0m[0239] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[758.499725, "o", "\u001b[36mINFO\u001b[0m[0244] Starting container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[759.006485, "o", "\u001b[36mINFO\u001b[0m[0244] [worker] Successfully started [rke-log-linker] container on host [10.50.2.20] \r\n"]
[759.009061, "o", "\u001b[36mINFO\u001b[0m[0244] Removing container [rke-log-linker] on host [10.50.2.20], try #1 \r\n"]
[759.341864, "o", "\u001b[36mINFO\u001b[0m[0244] [remove/rke-log-linker] Successfully removed container on host [10.50.2.20] \r\n\u001b[36mINFO\u001b[0m[0244] [worker] Now checking status of node 10.50.2.20, try #1 \r\n"]
[759.379388, "o", "\u001b[36mINFO\u001b[0m[0244] [workerplane] Processing host 10.50.2.21     \r\n\u001b[36mINFO\u001b[0m[0244] [worker] Now checking status of node 10.50.2.21, try #1 \r\n"]
[759.395828, "o", "\u001b[36mINFO\u001b[0m[0244] [worker] Getting list of nodes for upgrade   \r\n"]
[760.260764, "o", "\u001b[36mINFO\u001b[0m[0245] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[762.251627, "o", "\u001b[36mINFO\u001b[0m[0247] Starting container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[762.782423, "o", "\u001b[36mINFO\u001b[0m[0248] [worker] Successfully started [rke-log-linker] container on host [10.50.2.21] \r\n"]
[762.783641, "o", "\u001b[36mINFO\u001b[0m[0248] Removing container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[763.153059, "o", "\u001b[36mINFO\u001b[0m[0248] [remove/rke-log-linker] Successfully removed container on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0248] Checking if container [service-sidekick] is running on host [10.50.2.21], try #1 \r\n"]
[763.163666, "o", "\u001b[36mINFO\u001b[0m[0248] [sidekick] Sidekick container already created on host [10.50.2.21] \r\n"]
[763.166967, "o", "\u001b[36mINFO\u001b[0m[0248] Checking if container [kubelet] is running on host [10.50.2.21], try #1 \r\n"]
[763.171871, "o", "\u001b[36mINFO\u001b[0m[0248] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0248] Checking if container [old-kubelet] is running on host [10.50.2.21], try #1 \r\n"]
[763.178262, "o", "\u001b[36mINFO\u001b[0m[0248] Stopping container [kubelet] on host [10.50.2.21] with stopTimeoutDuration [5s], try #1 \r\n"]
[763.427951, "o", "\u001b[36mINFO\u001b[0m[0248] Waiting for [kubelet] container to exit on host [10.50.2.21] \r\n"]
[763.429534, "o", "\u001b[36mINFO\u001b[0m[0248] Renaming container [kubelet] to [old-kubelet] on host [10.50.2.21], try #1 \r\n"]
[763.585059, "o", "\u001b[36mINFO\u001b[0m[0249] Starting container [kubelet] on host [10.50.2.21], try #1 \r\n"]
[763.753149, "o", "\u001b[36mINFO\u001b[0m[0249] [worker] Successfully updated [kubelet] container on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0249] Removing container [old-kubelet] on host [10.50.2.21], try #1 \r\n"]
[763.846828, "o", "\u001b[36mINFO\u001b[0m[0249] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.21] \r\n"]
[775.399807, "o", "\u001b[36mINFO\u001b[0m[0260] [healthcheck] service [kubelet] on host [10.50.2.21] is healthy \r\n"]
[775.403292, "o", "\u001b[36mINFO\u001b[0m[0260] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[777.816244, "o", "\u001b[36mINFO\u001b[0m[0263] Starting container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[778.453533, "o", "\u001b[36mINFO\u001b[0m[0263] [worker] Successfully started [rke-log-linker] container on host [10.50.2.21] \r\n"]
[778.456254, "o", "\u001b[36mINFO\u001b[0m[0263] Removing container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[778.724952, "o", "\u001b[36mINFO\u001b[0m[0264] [remove/rke-log-linker] Successfully removed container on host [10.50.2.21] \r\n"]
[778.740661, "o", "\u001b[36mINFO\u001b[0m[0264] Checking if container [kube-proxy] is running on host [10.50.2.21], try #1 \r\n"]
[778.744024, "o", "\u001b[36mINFO\u001b[0m[0264] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0264] Checking if container [old-kube-proxy] is running on host [10.50.2.21], try #1 \r\n"]
[778.747856, "o", "\u001b[36mINFO\u001b[0m[0264] Stopping container [kube-proxy] on host [10.50.2.21] with stopTimeoutDuration [5s], try #1 \r\n"]
[778.919278, "o", "\u001b[36mINFO\u001b[0m[0264] Waiting for [kube-proxy] container to exit on host [10.50.2.21] \r\n"]
[778.921129, "o", "\u001b[36mINFO\u001b[0m[0264] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.50.2.21], try #1 \r\n"]
[779.073509, "o", "\u001b[36mINFO\u001b[0m[0264] Starting container [kube-proxy] on host [10.50.2.21], try #1 \r\n"]
[779.264688, "o", "\u001b[36mINFO\u001b[0m[0264] [worker] Successfully updated [kube-proxy] container on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0264] Removing container [old-kube-proxy] on host [10.50.2.21], try #1 \r\n"]
[779.335025, "o", "\u001b[36mINFO\u001b[0m[0264] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.21] \r\n"]
[779.798801, "o", "\u001b[36mINFO\u001b[0m[0265] [healthcheck] service [kube-proxy] on host [10.50.2.21] is healthy \r\n"]
[779.802145, "o", "\u001b[36mINFO\u001b[0m[0265] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[781.638347, "o", "\u001b[36mINFO\u001b[0m[0267] Starting container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[782.133361, "o", "\u001b[36mINFO\u001b[0m[0267] [worker] Successfully started [rke-log-linker] container on host [10.50.2.21] \r\n"]
[782.134401, "o", "\u001b[36mINFO\u001b[0m[0267] Removing container [rke-log-linker] on host [10.50.2.21], try #1 \r\n"]
[782.41576, "o", "\u001b[36mINFO\u001b[0m[0267] [remove/rke-log-linker] Successfully removed container on host [10.50.2.21] \r\n\u001b[36mINFO\u001b[0m[0267] [worker] Now checking status of node 10.50.2.21, try #1 \r\n"]
[782.472838, "o", "\u001b[36mINFO\u001b[0m[0267] [workerplane] Processing host 10.50.2.22     \r\n\u001b[36mINFO\u001b[0m[0267] [worker] Now checking status of node 10.50.2.22, try #1 \r\n"]
[782.490254, "o", "\u001b[36mINFO\u001b[0m[0267] [worker] Getting list of nodes for upgrade   \r\n"]
[783.082114, "o", "\u001b[36mINFO\u001b[0m[0268] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[784.679275, "o", "\u001b[36mINFO\u001b[0m[0270] Starting container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[785.177639, "o", "\u001b[36mINFO\u001b[0m[0270] [worker] Successfully started [rke-log-linker] container on host [10.50.2.22] \r\n"]
[785.178405, "o", "\u001b[36mINFO\u001b[0m[0270] Removing container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[785.462857, "o", "\u001b[36mINFO\u001b[0m[0270] [remove/rke-log-linker] Successfully removed container on host [10.50.2.22] \r\n\u001b[36mINFO\u001b[0m[0270] Checking if container [service-sidekick] is running on host [10.50.2.22], try #1 \r\n"]
[785.478157, "o", "\u001b[36mINFO\u001b[0m[0270] [sidekick] Sidekick container already created on host [10.50.2.22] \r\n"]
[785.483531, "o", "\u001b[36mINFO\u001b[0m[0270] Checking if container [kubelet] is running on host [10.50.2.22], try #1 \r\n"]
[785.493803, "o", "\u001b[36mINFO\u001b[0m[0270] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.22] \r\n\u001b[36mINFO\u001b[0m[0270] Checking if container [old-kubelet] is running on host [10.50.2.22], try #1 \r\n"]
[785.503999, "o", "\u001b[36mINFO\u001b[0m[0271] Stopping container [kubelet] on host [10.50.2.22] with stopTimeoutDuration [5s], try #1 \r\n"]
[785.718984, "o", "\u001b[36mINFO\u001b[0m[0271] Waiting for [kubelet] container to exit on host [10.50.2.22] \r\n"]
[785.719663, "o", "\u001b[36mINFO\u001b[0m[0271] Renaming container [kubelet] to [old-kubelet] on host [10.50.2.22], try #1 \r\n"]
[785.820955, "o", "\u001b[36mINFO\u001b[0m[0271] Starting container [kubelet] on host [10.50.2.22], try #1 \r\n"]
[786.026842, "o", "\u001b[36mINFO\u001b[0m[0271] [worker] Successfully updated [kubelet] container on host [10.50.2.22] \r\n\u001b[36mINFO\u001b[0m[0271] Removing container [old-kubelet] on host [10.50.2.22], try #1 \r\n"]
[786.064787, "o", "\u001b[36mINFO\u001b[0m[0271] [healthcheck] Start Healthcheck on service [kubelet] on host [10.50.2.22] \r\n"]
[797.587587, "o", "\u001b[36mINFO\u001b[0m[0283] [healthcheck] service [kubelet] on host [10.50.2.22] is healthy \r\n"]
[797.590194, "o", "\u001b[36mINFO\u001b[0m[0283] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[798.870139, "o", "\u001b[36mINFO\u001b[0m[0284] Starting container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[799.338545, "o", "\u001b[36mINFO\u001b[0m[0284] [worker] Successfully started [rke-log-linker] container on host [10.50.2.22] \r\n"]
[799.339099, "o", "\u001b[36mINFO\u001b[0m[0284] Removing container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[799.597027, "o", "\u001b[36mINFO\u001b[0m[0285] [remove/rke-log-linker] Successfully removed container on host [10.50.2.22] \r\n"]
[799.599575, "o", "\u001b[36mINFO\u001b[0m[0285] Checking if container [kube-proxy] is running on host [10.50.2.22], try #1 \r\n"]
[799.603852, "o", "\u001b[36mINFO\u001b[0m[0285] Image [10.8.101.2:5000/rancher/hyperkube:v1.19.4-rancher1] exists on host [10.50.2.22] \r\n\u001b[36mINFO\u001b[0m[0285] Checking if container [old-kube-proxy] is running on host [10.50.2.22], try #1 \r\n"]
[799.607641, "o", "\u001b[36mINFO\u001b[0m[0285] Stopping container [kube-proxy] on host [10.50.2.22] with stopTimeoutDuration [5s], try #1 \r\n"]
[799.783357, "o", "\u001b[36mINFO\u001b[0m[0285] Waiting for [kube-proxy] container to exit on host [10.50.2.22] \r\n"]
[799.78687, "o", "\u001b[36mINFO\u001b[0m[0285] Renaming container [kube-proxy] to [old-kube-proxy] on host [10.50.2.22], try #1 \r\n"]
[799.905731, "o", "\u001b[36mINFO\u001b[0m[0285] Starting container [kube-proxy] on host [10.50.2.22], try #1 \r\n"]
[800.061651, "o", "\u001b[36mINFO\u001b[0m[0285] [worker] Successfully updated [kube-proxy] container on host [10.50.2.22] \r\n\u001b[36mINFO\u001b[0m[0285] Removing container [old-kube-proxy] on host [10.50.2.22], try #1 \r\n"]
[800.109584, "o", "\u001b[36mINFO\u001b[0m[0285] [healthcheck] Start Healthcheck on service [kube-proxy] on host [10.50.2.22] \r\n"]
[800.565792, "o", "\u001b[36mINFO\u001b[0m[0286] [healthcheck] service [kube-proxy] on host [10.50.2.22] is healthy \r\n"]
[800.568902, "o", "\u001b[36mINFO\u001b[0m[0286] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[801.791086, "o", "\u001b[36mINFO\u001b[0m[0287] Starting container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[802.235647, "o", "\u001b[36mINFO\u001b[0m[0287] [worker] Successfully started [rke-log-linker] container on host [10.50.2.22] \r\n"]
[802.236819, "o", "\u001b[36mINFO\u001b[0m[0287] Removing container [rke-log-linker] on host [10.50.2.22], try #1 \r\n"]
[802.514515, "o", "\u001b[36mINFO\u001b[0m[0288] [remove/rke-log-linker] Successfully removed container on host [10.50.2.22] \r\n\u001b[36mINFO\u001b[0m[0288] [worker] Now checking status of node 10.50.2.22, try #1 \r\n"]
[802.541277, "o", "\u001b[36mINFO\u001b[0m[0288] [worker] Successfully upgraded Worker Plane.. \r\n"]
[802.545098, "o", "\u001b[36mINFO\u001b[0m[0288] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.22] \r\n"]
[802.545995, "o", "\u001b[36mINFO\u001b[0m[0288] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.21] \r\n"]
[802.551299, "o", "\u001b[36mINFO\u001b[0m[0288] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.20] \r\n"]
[803.136235, "o", "\u001b[36mINFO\u001b[0m[0288] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.10] \r\n"]
[803.148978, "o", "\u001b[36mINFO\u001b[0m[0288] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.12] \r\n"]
[803.149099, "o", "\u001b[36mINFO\u001b[0m[0288] Image [10.8.101.2:5000/rancher/rke-tools:v0.1.66] exists on host [10.50.2.11] \r\n"]
[806.08409, "o", "\u001b[36mINFO\u001b[0m[0291] Starting container [rke-log-cleaner] on host [10.50.2.20], try #1 \r\n"]
[806.112981, "o", "\u001b[36mINFO\u001b[0m[0291] Starting container [rke-log-cleaner] on host [10.50.2.22], try #1 \r\n"]
[806.132179, "o", "\u001b[36mINFO\u001b[0m[0291] Starting container [rke-log-cleaner] on host [10.50.2.21], try #1 \r\n"]
[809.028537, "o", "\u001b[36mINFO\u001b[0m[0294] Starting container [rke-log-cleaner] on host [10.50.2.12], try #1 \r\n"]
[809.031153, "o", "\u001b[36mINFO\u001b[0m[0294] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.20] \r\n"]
[809.032463, "o", "\u001b[36mINFO\u001b[0m[0294] Removing container [rke-log-cleaner] on host [10.50.2.20], try #1 \r\n"]
[809.079927, "o", "\u001b[36mINFO\u001b[0m[0294] Starting container [rke-log-cleaner] on host [10.50.2.10], try #1 \r\n"]
[809.105553, "o", "\u001b[36mINFO\u001b[0m[0294] Starting container [rke-log-cleaner] on host [10.50.2.11], try #1 \r\n"]
[809.415113, "o", "\u001b[36mINFO\u001b[0m[0294] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.20] \r\n"]
[809.475848, "o", "\u001b[36mINFO\u001b[0m[0294] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.21] \r\n"]
[809.478147, "o", "\u001b[36mINFO\u001b[0m[0294] Removing container [rke-log-cleaner] on host [10.50.2.21], try #1 \r\n"]
[809.529283, "o", "\u001b[36mINFO\u001b[0m[0295] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.22] \r\n"]
[809.661441, "o", "\u001b[36mINFO\u001b[0m[0295] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.12] \r\n"]
[809.662954, "o", "\u001b[36mINFO\u001b[0m[0295] Removing container [rke-log-cleaner] on host [10.50.2.12], try #1 \r\n"]
[809.663538, "o", "\u001b[36mINFO\u001b[0m[0295] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.10] \r\n"]
[809.667156, "o", "\u001b[36mINFO\u001b[0m[0295] Removing container [rke-log-cleaner] on host [10.50.2.10], try #1 \r\n"]
[809.711423, "o", "\u001b[36mINFO\u001b[0m[0295] [cleanup] Successfully started [rke-log-cleaner] container on host [10.50.2.11] \r\n"]
[809.714713, "o", "\u001b[36mINFO\u001b[0m[0295] Removing container [rke-log-cleaner] on host [10.50.2.11], try #1 \r\n"]
[809.791662, "o", "\u001b[36mINFO\u001b[0m[0295] Removing container [rke-log-cleaner] on host [10.50.2.22], try #1 \r\n"]
[809.815547, "o", "\u001b[36mINFO\u001b[0m[0295] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.21] \r\n"]
[809.821865, "o", "\u001b[36mINFO\u001b[0m[0295] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.22] \r\n"]
[810.078697, "o", "\u001b[36mINFO\u001b[0m[0295] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.10] \r\n\u001b[36mINFO\u001b[0m[0295] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.12] \r\n"]
[810.103908, "o", "\u001b[36mINFO\u001b[0m[0295] [remove/rke-log-cleaner] Successfully removed container on host [10.50.2.11] \r\n\u001b[36mINFO\u001b[0m[0295] [sync] Syncing nodes Labels and Taints       \r\n"]
[810.116324, "o", "\u001b[36mINFO\u001b[0m[0295] [sync] Successfully synced nodes Labels and Taints \r\n"]
[810.11643, "o", "\u001b[36mINFO\u001b[0m[0295] [network] Setting up network plugin: calico  \r\n"]
[810.116927, "o", "\u001b[36mINFO\u001b[0m[0295] [addons] Saving ConfigMap for addon rke-network-plugin to Kubernetes \r\n"]
[810.188905, "o", "\u001b[36mINFO\u001b[0m[0295] [addons] Successfully saved ConfigMap for addon rke-network-plugin to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0295] [addons] Executing deploy job rke-network-plugin \r\n"]
[825.619098, "o", "\u001b[36mINFO\u001b[0m[0311] [addons] Setting up coredns                  \r\n"]
[825.619494, "o", "\u001b[36mINFO\u001b[0m[0311] [addons] Saving ConfigMap for addon rke-coredns-addon to Kubernetes \r\n"]
[825.790262, "o", "\u001b[36mINFO\u001b[0m[0311] [addons] Successfully saved ConfigMap for addon rke-coredns-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0311] [addons] Executing deploy job rke-coredns-addon \r\n"]
[836.000503, "o", "\u001b[36mINFO\u001b[0m[0321] [addons] CoreDNS deployed successfully       \r\n\u001b[36mINFO\u001b[0m[0321] [dns] DNS provider coredns deployed successfully \r\n"]
[836.003361, "o", "\u001b[36mINFO\u001b[0m[0321] [addons] Setting up Metrics Server           \r\n"]
[836.004549, "o", "\u001b[36mINFO\u001b[0m[0321] [addons] Saving ConfigMap for addon rke-metrics-addon to Kubernetes \r\n"]
[836.010185, "o", "\u001b[36mINFO\u001b[0m[0321] [addons] Successfully saved ConfigMap for addon rke-metrics-addon to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0321] [addons] Executing deploy job rke-metrics-addon \r\n"]
[836.027305, "o", "\u001b[36mINFO\u001b[0m[0321] [addons] Metrics Server deployed successfully \r\n\u001b[36mINFO\u001b[0m[0321] [ingress] Setting up nginx ingress controller \r\n"]
[836.027716, "o", "\u001b[36mINFO\u001b[0m[0321] [addons] Saving ConfigMap for addon rke-ingress-controller to Kubernetes \r\n"]
[836.031003, "o", "\u001b[36mINFO\u001b[0m[0321] [addons] Successfully saved ConfigMap for addon rke-ingress-controller to Kubernetes \r\n\u001b[36mINFO\u001b[0m[0321] [addons] Executing deploy job rke-ingress-controller \r\n"]
[836.044734, "o", "\u001b[36mINFO\u001b[0m[0321] [ingress] ingress controller nginx deployed successfully \r\n\u001b[36mINFO\u001b[0m[0321] [addons] Setting up user addons              \r\n"]
[836.05072, "o", "\u001b[36mINFO\u001b[0m[0321] [addons] no user addons defined              \r\n\u001b[36mINFO\u001b[0m[0321] Finished building Kubernetes cluster successfully \r\n"]
[836.053757, "o", "\r\nreal\t5m21.850s\r\nuser\t0m1.383s\r\nsys\t0m0.541s\r\n"]
[866.627919, "o", "NAME              STATUS   ROLES               AGE    VERSION\r\nnode/10.50.2.10   Ready    controlplane,etcd   7m1s   v1.19.4\r\nnode/10.50.2.11   Ready    controlplane,etcd   7m1s   v1.19.4\r\nnode/10.50.2.12   Ready    controlplane,etcd   7m1s   v1.19.4\r\nnode/10.50.2.20   Ready    worker              7m     v1.19.4\r\nnode/10.50.2.21   Ready    worker              7m     v1.19.4\r\nnode/10.50.2.22   Ready    worker"]
[866.628079, "o", "              7m     v1.19.4\r\n"]
[866.639451, "o", "\r\nNAMESPACE       NAME                                           READY   STATUS              RESTARTS   AGE\r\ningress-nginx   pod/default-http-backend-5b564dd459-stjgz      1/1     Running             0          6m25s\r\ningress-nginx   pod/nginx-ingress-controller-blz4j             1/1     Running             0          6m25s\r\ningress-nginx   pod/nginx-ingress-controller-cj8nd             1/1     Running             0          6m25s\r\ningress-nginx   pod/nginx-ingress-controller-fqjdh             1/1     Running"]
[866.639646, "o", "             0          6m25s\r\nkube-system     pod/calico-kube-controllers-7dcd869879-pq85l   0/1     ContainerCreating   0          42s\r\nkube-system     pod/calico-node-78wz2                          1/1     Running             0          44s\r\nkube-system     pod/calico-node-7dz2m                          1/1     Running             0          6m40s\r\nkube-system     pod/calico-node-cm5rc                          1/1     Running             0          6m40s\r\nkube-system     pod/calico-node-kf6zb                          1/1     Running             0          6m40s\r\nkube-system"]
[866.639724, "o", "     pod/calico-node-npvfs                          0/1     Running             0          24s\r\nkube-system     pod/calico-node-zjtmj                          1/1     Running             0          6m40s"]
[866.639862, "o", "\r\nkube-system     pod/coredns-5d44d4c69-4g2wn                    1/1     Running             0          33s\r\nkube-system     pod/coredns-5d44d4c69-ll64w                    0/1     ContainerCreating   0          34s\r\nkube-system     pod/coredns-autoscaler-557f965569-4sbdd        1/1     Running             0          6m36s"]
[866.639942, "o", "\r\nkube-system     pod/coredns-autoscaler-8df879c6f-qvztz         0/1     ContainerCreating   0          34s\r\nkube-system     pod/metrics-server-77956db857-pdklq            1/1     Running             0          6m31s\r\nkube-system     "]
[866.640019, "o", "pod/rke-coredns-addon-deploy-job-gjhhg         0/1     Completed           0          36s\r\nkube-system     pod/rke-ingress-controller-deploy-job-9882t    0/1     Completed           0          6m29s\r\nkube-system     pod/rke-metrics-addon-deploy-job-l47wt        "]
[866.640089, "o", " 0/1     Completed           0          6m34s\r\nkube-system     pod/rke-network-plugin-deploy-job-6vbwj        0/1     Completed           0          52s\r\n"]
